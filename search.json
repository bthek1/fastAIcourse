[
  {
    "objectID": "ddpm_v2.html",
    "href": "ddpm_v2.html",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "",
    "text": "from torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nmpl.rcParams['image.cmap'] = 'gray_r'\nlogging.disable(logging.WARNING)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#imports",
    "href": "ddpm_v2.html#imports",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "",
    "text": "from torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nmpl.rcParams['image.cmap'] = 'gray_r'\nlogging.disable(logging.WARNING)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#load-the-dataset",
    "href": "ddpm_v2.html#load-the-dataset",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Load the dataset",
    "text": "Load the dataset\n\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2)) for o in b[xl]]\n\nbs = 128\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\n\nshow_images(xb[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\nbetamin,betamax,n_steps = 0.0001,0.02,1000\nbeta = torch.linspace(betamin, betamax, n_steps)\nalpha = 1.-beta\nalphabar = alpha.cumprod(dim=0)\nsigma = beta.sqrt()\n\n\nplt.plot(beta);\n\n\n\n\n\n\n\n\n\nplt.plot(sigma);\n\n\n\n\n\n\n\n\n\nplt.plot(alphabar);\n\n\n\n\n\n\n\n\n\n\nExported source\ndef noisify(x0, ᾱ):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    ε = torch.randn(x0.shape, device=device)\n    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n    return (xt, t.to(device)), ε\n\n\n\n(xt,t),ε = noisify(xb[:25],alphabar)\nt\n\ntensor([ 26, 335, 620, 924, 950, 113, 378,  14, 210, 954, 231, 572, 315, 295, 567, 706, 749, 876,  73, 111, 899, 213, 541, 769, 287])\n\n\n\ntitles = fc.map_ex(t, '{}')\nshow_images(xt, imsize=1.5, titles=titles)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#training",
    "href": "ddpm_v2.html#training",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Training",
    "text": "Training\n\n\nExported source\nfrom diffusers import UNet2DModel\n\n\n\n\nExported source\n@torch.no_grad()\ndef sample(model, sz, alpha, alphabar, sigma, n_steps):\n    device = next(model.parameters()).device\n    x_t = torch.randn(sz, device=device)\n    preds = []\n    for t in reversed(range(n_steps)):\n        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n        z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(device)\n        ᾱ_t1 = alphabar[t-1]  if t &gt; 0 else torch.tensor(1)\n        b̄_t = 1 - alphabar[t]\n        b̄_t1 = 1 - ᾱ_t1\n        x_0_hat = ((x_t - b̄_t.sqrt() * learn.model((x_t, t_batch)))/alphabar[t].sqrt()).clamp(-1,1)\n        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n        preds.append(x_t.cpu())\n    return preds\n\n\n\n\nExported source\nclass DDPMCB(Callback):\n    order = DeviceCB.order+1\n    def __init__(self, n_steps, beta_min, beta_max):\n        super().__init__()\n        fc.store_attr()\n        self.beta = torch.linspace(self.beta_min, self.beta_max, self.n_steps)\n        self.α = 1. - self.beta \n        self.ᾱ = torch.cumprod(self.α, dim=0)\n        self.σ = self.beta.sqrt()\n    \n    def before_batch(self, learn): learn.batch = noisify(learn.batch[0], self.ᾱ)\n    def sample(self, model, sz): return sample(model, sz, self.α, self.ᾱ, self.σ, self.n_steps)\n\n\n\n\nExported source\nclass UNet(UNet2DModel):\n    def forward(self, x): return super().forward(*x).sample\n\n\n\nddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\n\n\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(16, 32, 64, 64), norm_num_groups=8)\n\nlearn = TrainLearner(model, dls, nn.MSELoss())\nlearn.fit(train=False, cbs=[ddpm_cb,SingleBatchCB()])\n(xt,t),ε = learn.batch\nshow_images(xt[:25], titles=fc.map_ex(t[:25], '{}'), imsize=1.5)\n\n\n\n\n\n\n\n\n\nlr = 5e-3\nepochs = 3\n\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [ddpm_cb, DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n\n\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(16, 32, 64, 128), norm_num_groups=8)\n\n\n\nExported source\ndef init_ddpm(model):\n    for o in model.down_blocks:\n        for p in o.resnets:\n            p.conv2.weight.data.zero_()\n            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n\n    for o in model.up_blocks:\n        for p in o.resnets: p.conv2.weight.data.zero_()\n\n    model.conv_out.weight.data.zero_()\n\n\n\ninit_ddpm(model)\n\n\nopt_func = partial(optim.Adam, eps=1e-5)\n\n\nlearn = TrainLearner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.131\n0\ntrain\n\n\n0.024\n0\neval\n\n\n0.022\n1\ntrain\n\n\n0.022\n1\neval\n\n\n0.019\n2\ntrain\n\n\n0.020\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmdl_path = Path('models')\n\n\ntorch.save(learn.model, mdl_path/'fashion_ddpm2.pkl')\n\n\nlearn.model = torch.load(mdl_path/'fashion_ddpm2.pkl')",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#sampling",
    "href": "ddpm_v2.html#sampling",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Sampling",
    "text": "Sampling\n\nsamples = ddpm_cb.sample(learn.model, (16, 1, 32, 32))\n\n\nshow_images(samples[-1], figsize=(5,5))",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#mixed-precision",
    "href": "ddpm_v2.html#mixed-precision",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Mixed Precision",
    "text": "Mixed Precision\n\nbs = 512\n\n\nnext(iter(DataLoader(tds['train'], batch_size=2)))\n\n{'image': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n         [[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n 'label': tensor([9, 0])}\n\n\n\n\nExported source\ndef collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\ndef dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)\n\n\n\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\n\nExported source\nclass MixedPrecision(TrainCB):\n    order = DeviceCB.order+10\n    \n    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()\n\n    def before_batch(self, learn):\n        self.autocast = torch.autocast(\"cuda\", dtype=torch.float16)\n        self.autocast.__enter__()\n\n    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n        \n    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n\n    def step(self, learn):\n        self.scaler.step(learn.opt)\n        self.scaler.update()\n\n\n\nlr = 1e-2\nepochs = 3\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(16, 32, 64, 128), norm_num_groups=8)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\nnan\n0\ntrain\n\n\nnan\n0\neval\n\n\nnan\n1\ntrain\n\n\nnan\n1\neval\n\n\nnan\n2\ntrain\n\n\nnan\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsamples = sample(learn.model, (32, 1, 32, 32), alpha, alphabar, sigma, n_steps)\n\n\nshow_images(samples[-1][:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/fashion_ddpm_mp.pkl')",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#accelerate",
    "href": "ddpm_v2.html#accelerate",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Accelerate",
    "text": "Accelerate\npip install accelerate before running this section.\n\n\nExported source\nfrom accelerate import Accelerator\n\n\n\n\nExported source\nclass AccelerateCB(TrainCB):\n    order = DeviceCB.order+10\n    def __init__(self, n_inp=1, mixed_precision=\"fp16\"):\n        super().__init__(n_inp=n_inp)\n        self.acc = Accelerator(mixed_precision=mixed_precision)\n        \n    def before_fit(self, learn):\n        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(\n            learn.model, learn.opt, learn.dls.train, learn.dls.valid)\n\n    def backward(self, learn): self.acc.backward(learn.loss)\n\n\n\n\nExported source\ndef noisify(x0, ᾱ):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    ε = torch.randn(x0.shape, device=device)\n    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n    return xt, t.to(device), ε\n\n\n\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\n\nExported source\nclass DDPMCB2(Callback):\n    def after_predict(self, learn): learn.preds = learn.preds.sample\n\n\n\nmodel = UNet2DModel(in_channels=1, out_channels=1, block_out_channels=(16, 32, 64, 128), norm_num_groups=8)\ninit_ddpm(model)\ncbs = [DDPMCB2(), DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), AccelerateCB(n_inp=2)]\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.201\n0\ntrain\n\n\n0.031\n0\neval\n\n\n0.025\n1\ntrain\n\n\n0.022\n1\neval\n\n\n0.022\n2\ntrain\n\n\n0.021\n2\neval",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v2.html#a-sneaky-trick",
    "href": "ddpm_v2.html#a-sneaky-trick",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "A sneaky trick",
    "text": "A sneaky trick\n\n\nExported source\nclass MultDL:\n    def __init__(self, dl, mult=2): self.dl,self.mult = dl,mult\n    def __len__(self): return len(self.dl)*self.mult\n    def __iter__(self):\n        for o in self.dl:\n            for i in range(self.mult): yield o\n\n\n\ndls.train = MultDL(dls.train)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "naturallanguage.html",
    "href": "naturallanguage.html",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.0.4\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ntorch.cuda.device_count()\n\n1\n\n\n\ntorch.cuda.get_device_capability()\n\n(7, 5)\n\n\n\ntorch.cuda.current_device()\n\n0\n\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\") # clearing cache \nlibc.malloc_trim(0)\n\n1\n\n\n\nimport gc\nimport torch\ntorch.cuda.empty_cache()\ngc.collect()\n\n11\n\n\n\nfor name in dir():\n    if not name.startswith('_'):\n        del globals()[name]\n\nin the book, doing NLP using RNNs (recurrent neural networks).\nWe are using transformers, using Hugging Face Transformers.\n\nWikitext - Language Model\nIMDb - Language Model - use wikitest as pretraining\nIMDb - Classifier - use IMDb as pretraining\n\n!pip install kaggle",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#initial-checks",
    "href": "naturallanguage.html#initial-checks",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.0.4\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ntorch.cuda.device_count()\n\n1\n\n\n\ntorch.cuda.get_device_capability()\n\n(7, 5)\n\n\n\ntorch.cuda.current_device()\n\n0\n\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\") # clearing cache \nlibc.malloc_trim(0)\n\n1\n\n\n\nimport gc\nimport torch\ntorch.cuda.empty_cache()\ngc.collect()\n\n11\n\n\n\nfor name in dir():\n    if not name.startswith('_'):\n        del globals()[name]\n\nin the book, doing NLP using RNNs (recurrent neural networks).\nWe are using transformers, using Hugging Face Transformers.\n\nWikitext - Language Model\nIMDb - Language Model - use wikitest as pretraining\nIMDb - Classifier - use IMDb as pretraining\n\n!pip install kaggle",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#kaggle-setup",
    "href": "naturallanguage.html#kaggle-setup",
    "title": "NLP - Natural Language Processing",
    "section": "Kaggle setup",
    "text": "Kaggle setup\n\nimport os\n\n# for working with paths in Python, I recommend using `pathlib.Path`\nfrom pathlib import Path\n\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\ncreds = '{\"username\":\"bensonthekkel\",\"key\":\"5d0c64462ee63521393fead641685ce8\"}'\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\n\n\nname = 'us-patent-phrase-to-phrase-matching'\ndataPath = Path(f'./Data/{name}')\n\nkaggle_competition_download(name)\n\nfile exists",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#need-libraries-for-data-science",
    "href": "naturallanguage.html#need-libraries-for-data-science",
    "title": "NLP - Natural Language Processing",
    "section": "Need libraries for data science",
    "text": "Need libraries for data science\n\nNumpy\nMatplotlib\npandas\npytorch\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv(dataPath/'train.csv')\ndf\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\nscore\n\n\n\n\n0\n37d61fd2272659b1\nabatement\nabatement of pollution\nA47\n0.50\n\n\n1\n7b9652b17b68b7a4\nabatement\nact of abating\nA47\n0.75\n\n\n2\n36d72442aefd8232\nabatement\nactive catalyst\nA47\n0.25\n\n\n3\n5296b0c19e1ce60e\nabatement\neliminating process\nA47\n0.50\n\n\n4\n54c1e3b9184cb5b6\nabatement\nforest region\nA47\n0.00\n\n\n...\n...\n...\n...\n...\n...\n\n\n36468\n8e1386cbefd7f245\nwood article\nwooden article\nB44\n1.00\n\n\n36469\n42d9e032d1cd3242\nwood article\nwooden box\nB44\n0.50\n\n\n36470\n208654ccb9e14fa3\nwood article\nwooden handle\nB44\n0.50\n\n\n36471\n756ec035e694722b\nwood article\nwooden material\nB44\n0.75\n\n\n36472\n8d135da0b55b8c88\nwood article\nwooden substrate\nB44\n0.50\n\n\n\n\n36473 rows × 5 columns\n\n\n\n\ndf.describe(include='object')\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\ncount\n36473\n36473\n36473\n36473\n\n\nunique\n36473\n733\n29340\n106\n\n\ntop\n37d61fd2272659b1\ncomponent composite coating\ncomposition\nH01\n\n\nfreq\n1\n152\n24\n2186\n\n\n\n\n\n\n\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\ndf.input.head()\n\n0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\nName: input, dtype: object",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#tokenization",
    "href": "naturallanguage.html#tokenization",
    "title": "NLP - Natural Language Processing",
    "section": "Tokenization",
    "text": "Tokenization\n\nfrom datasets import Dataset,DatasetDict\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\n\n\nds = Dataset.from_pandas(df)\nds\n\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n    num_rows: 36473\n})\n\n\n\nmodel_nm = 'microsoft/deberta-v3-small'\ntokz = AutoTokenizer.from_pretrained(model_nm)\ntokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n['▁G',\n \"'\",\n 'day',\n '▁folks',\n ',',\n '▁I',\n \"'\",\n 'm',\n '▁Jeremy',\n '▁from',\n '▁fast',\n '.',\n 'ai',\n '!']\n\n\n\ntokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n\n['▁A',\n '▁platypus',\n '▁is',\n '▁an',\n '▁or',\n 'ni',\n 'tho',\n 'rhynch',\n 'us',\n '▁an',\n 'at',\n 'inus',\n '.']\n\n\n\ndef tok_func(x): return tokz(x[\"input\"])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\n\nrow = tok_ds[0]\nrow['input'], row['input_ids']\n\n('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n [1,\n  54453,\n  435,\n  294,\n  336,\n  5753,\n  346,\n  54453,\n  445,\n  294,\n  47284,\n  265,\n  6435,\n  346,\n  23702,\n  435,\n  294,\n  47284,\n  2])\n\n\n\ntokz.vocab['▁of']\n\n265\n\n\n\ntok_ds = tok_ds.rename_columns({'score':'labels'})\n\n\neval_df = pd.read_csv(dataPath/'test.csv')\neval_df.describe()\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\ncount\n36\n36\n36\n36\n\n\nunique\n36\n34\n36\n29\n\n\ntop\n4112d61851461f60\nel display\ninorganic photoconductor drum\nG02\n\n\nfreq\n1\n2\n1\n3\n\n\n\n\n\n\n\n\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27354\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9119\n    })\n})\n\n\n\neval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\neval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#understanding-overfitting",
    "href": "naturallanguage.html#understanding-overfitting",
    "title": "NLP - Natural Language Processing",
    "section": "Understanding Overfitting",
    "text": "Understanding Overfitting\n\nfrom fastAIcourse.neuralnet import *\n\n\ndef f(x): return -3*x**2 + 2*x + 20\n\n\nplot_function(f)\n\n\n\n\n\n\n\n\n\nfrom numpy.random import normal,seed,uniform\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(42)\n\n\ndef noise(x, scale): return normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\n\nx = np.linspace(-2, 2, num=20)[:,None]\ny = add_noise(f(x), 0.2, 1.3)\nplt.scatter(x,y);\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef plot_poly(degree):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x, y)\n    plt.scatter(x,y)\n    plot_function(model.predict)\n\n\nplot_poly(1)\n\n\n\n\n\n\n\n\n\nplot_poly(10)\n\n\n\n\n\n\n\n\n\nplot_poly(2)\nplot_function(f, color='b')",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#validation-set",
    "href": "naturallanguage.html#validation-set",
    "title": "NLP - Natural Language Processing",
    "section": "Validation Set",
    "text": "Validation Set\nFastAI always uses a validation set. HUgging face is also good\n\nCreating a good validation set\n\nbe careful of randow data split (sin functions)\nbe careful of cross-validation\nbe careful of overfitting in the validation set\n\n\n\nTest set\n\nbasically another validation set\nex. trying 180 models will give you a good accuracy for some validation set by chance or coincidence\nHence, test set for the real test\n\nMetrics - accruacy - error - pearson correlation coefficient\nmetrics is not same as loss function - loss function used for calculating gradient descent - if using accuracy, function is too bumpy - we want a function that is nice and smooth like average absolute error, mean absolute error\nloss function - average absolute error - mean absolute error\nIn real life, trial and error",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#pearson-correlation-coefficient",
    "href": "naturallanguage.html#pearson-correlation-coefficient",
    "title": "NLP - Natural Language Processing",
    "section": "Pearson Correlation Coefficient",
    "text": "Pearson Correlation Coefficient\n\nfrom sklearn.datasets import fetch_california_housing\n\n\nhousing = fetch_california_housing(as_frame=True)\nhousing = housing['data'].join(housing['target']).sample(1000, random_state=52)\nhousing.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n7506\n3.0550\n37.0\n5.152778\n1.048611\n729.0\n5.062500\n33.92\n-118.28\n1.054\n\n\n4720\n3.0862\n35.0\n4.697897\n1.055449\n1159.0\n2.216061\n34.05\n-118.37\n3.453\n\n\n12888\n2.5556\n24.0\n4.864905\n1.129222\n1631.0\n2.395007\n38.66\n-121.35\n1.057\n\n\n13344\n3.0057\n32.0\n4.212687\n0.936567\n1378.0\n5.141791\n34.05\n-117.64\n0.969\n\n\n7173\n1.9083\n42.0\n3.888554\n1.039157\n1535.0\n4.623494\n34.05\n-118.19\n1.192\n\n\n\n\n\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\nnp.corrcoef(housing, rowvar=False)\n\narray([[ 1.  , -0.12,  0.43, -0.08,  0.01, -0.07, -0.12,  0.04,  0.68],\n       [-0.12,  1.  , -0.17, -0.06, -0.31,  0.  ,  0.03, -0.13,  0.12],\n       [ 0.43, -0.17,  1.  ,  0.76, -0.09, -0.07,  0.12, -0.03,  0.21],\n       [-0.08, -0.06,  0.76,  1.  , -0.08, -0.07,  0.09,  0.  , -0.04],\n       [ 0.01, -0.31, -0.09, -0.08,  1.  ,  0.16, -0.15,  0.13,  0.  ],\n       [-0.07,  0.  , -0.07, -0.07,  0.16,  1.  , -0.16,  0.17, -0.27],\n       [-0.12,  0.03,  0.12,  0.09, -0.15, -0.16,  1.  , -0.93, -0.16],\n       [ 0.04, -0.13, -0.03,  0.  ,  0.13,  0.17, -0.93,  1.  , -0.03],\n       [ 0.68,  0.12,  0.21, -0.04,  0.  , -0.27, -0.16, -0.03,  1.  ]])\n\n\n\nnp.corrcoef(housing.MedInc, housing.MedHouseVal)\n\narray([[1.  , 0.68],\n       [0.68, 1.  ]])\n\n\n\ndef corr(x,y): return np.corrcoef(x,y)[0][1]\n\ndef show_corr(df, a, b):\n    x,y = df[a],df[b]\n    plt.scatter(x,y, alpha=0.5, s=4)\n    plt.title(f'{a} vs {b}; r: {corr(x, y):.2f}')\n\n\ncorr(housing.MedInc, housing.MedHouseVal)\nshow_corr(housing, 'MedInc', 'MedHouseVal')\n\n\n\n\n\n\n\n\n\nshow_corr(housing, 'MedInc', 'AveRooms')\n\n\n\n\n\n\n\n\n\nsubset = housing[housing.AveRooms&lt;15]\nshow_corr(subset, 'MedInc', 'AveRooms')\n\n\n\n\n\n\n\n\n\nshow_corr(subset, 'MedHouseVal', 'AveRooms')\n\n\n\n\n\n\n\n\n\nshow_corr(subset, 'HouseAge', 'AveRooms')\n\n\n\n\n\n\n\n\n\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "naturallanguage.html#training-our-model",
    "href": "naturallanguage.html#training-our-model",
    "title": "NLP - Natural Language Processing",
    "section": "Training our Model",
    "text": "Training our Model\nTo train a model in Transformers we’ll need this:\n\nfrom transformers import TrainingArguments,Trainer\n\ncomet_ml is installed but `COMET_API_KEY` is not set.\n\n\nWe pick a batch size that fits our GPU, and small number of epochs so we can run experiments quickly:\n\nbs = 128\nepochs = 5\n\nThe most important hyperparameter is the learning rate. fastai provides a learning rate finder to help you figure this out, but Transformers doesn’t, so you’ll just have to use trial and error. The idea is to find the largest value you can, but which doesn’t result in training failing.\n\nlr = 8e-5\n\nTransformers uses the TrainingArguments class to set up arguments. Don’t worry too much about the values we’re using here – they should generally work fine in most cases. It’s just the 3 parameters above that you may need to change for different models.\n\nargs = TrainingArguments('outputs', learning_rate=lr,\n                         warmup_ratio=0.1,\n                         lr_scheduler_type='cosine',\n                         fp16=True,\n                         evaluation_strategy=\"epoch\",\n                         per_device_train_batch_size=bs,\n                         per_device_eval_batch_size=bs*2,\n                         num_train_epochs=epochs,\n                         weight_decay=0.01,\n                         report_to='none'\n                        )\n\nWe can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai):\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm,\n                                                           num_labels=1\n                                                          )\ntrainer = Trainer(model,\n                  args,\n                  train_dataset=dds['train'],\n                  eval_dataset=dds['test'],\n                  tokenizer=tokz,\n                  compute_metrics=corr_d\n                 )\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ntrainer.train();\n\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [1070/1070 18:00, Epoch 5/5]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.026321\n0.802121\n\n\n2\nNo log\n0.021985\n0.819910\n\n\n3\n0.030600\n0.021005\n0.832954\n\n\n4\n0.030600\n0.022370\n0.836500\n\n\n5\n0.012300\n0.022081\n0.836393\n\n\n\n\n\n\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds\n\n\n\n\narray([[ 0.47],\n       [ 0.57],\n       [ 0.55],\n       [ 0.36],\n       [-0.05],\n       [ 0.53],\n       [ 0.53],\n       [-0.03],\n       [ 0.32],\n       [ 1.12],\n       [ 0.22],\n       [ 0.24],\n       [ 0.77],\n       [ 0.96],\n       [ 0.76],\n       [ 0.39],\n       [ 0.33],\n       [-0.01],\n       [ 0.65],\n       [ 0.4 ],\n       [ 0.48],\n       [ 0.26],\n       [ 0.17],\n       [ 0.26],\n       [ 0.56],\n       [-0.03],\n       [-0.04],\n       [-0.04],\n       [-0.05],\n       [ 0.57],\n       [ 0.32],\n       [-0.03],\n       [ 0.7 ],\n       [ 0.49],\n       [ 0.43],\n       [ 0.22]])\n\n\n\npreds = np.clip(preds, 0, 1)\n\n\npreds\n\narray([[0.47],\n       [0.57],\n       [0.55],\n       [0.36],\n       [0.  ],\n       [0.53],\n       [0.53],\n       [0.  ],\n       [0.32],\n       [1.  ],\n       [0.22],\n       [0.24],\n       [0.77],\n       [0.96],\n       [0.76],\n       [0.39],\n       [0.33],\n       [0.  ],\n       [0.65],\n       [0.4 ],\n       [0.48],\n       [0.26],\n       [0.17],\n       [0.26],\n       [0.56],\n       [0.  ],\n       [0.  ],\n       [0.  ],\n       [0.  ],\n       [0.57],\n       [0.32],\n       [0.  ],\n       [0.7 ],\n       [0.49],\n       [0.43],\n       [0.22]])\n\n\n\nimport datasets\n\n\nsubmission = datasets.Dataset.from_dict({\n    'id': eval_ds['id'],\n    'score': preds\n})\n\nsubmission.to_csv('Data/submission.csv', index=False)\n\n\n\n\n853",
    "crumbs": [
      "Blog",
      "NLP - Natural Language Processing"
    ]
  },
  {
    "objectID": "diffusion_unet.html",
    "href": "diffusion_unet.html",
    "title": "Diffusion unet",
    "section": "",
    "text": "import os\n# os.environ['CUDA_VISIBLE_DEVICES']='1'\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.training import *\nfrom fastprogress import progress_bar\nfrom diffusers import UNet2DModel, DDIMPipeline, DDPMPipeline, DDIMScheduler, DDPMScheduler\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nn_steps = 1000\nbs = 512\ndsd = load_dataset(name)\nsig_data = 0.66\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\ndef scalings(sig):\n    totvar = sig**2+sig_data**2\n    # c_skip,c_out,c_in\n    return sig_data**2/totvar,sig*sig_data/totvar.sqrt(),1/totvar.sqrt()\n\ndef noisify(x0):\n    device = x0.device\n    sig = (torch.randn([len(x0)])*1.2-1.2).exp().to(x0).reshape(-1,1,1,1)\n    noise = torch.randn_like(x0, device=device)\n    c_skip,c_out,c_in = scalings(sig)\n    noised_input = x0 + noise*sig\n    target = (x0-c_skip*noised_input)/c_out\n    return (noised_input*c_in,sig.squeeze()),target\n\ndef collate_ddpm(b): return noisify(default_collate(b)[xl])\ndef dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=0)\ntds = dsd.with_transform(transformi)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion_unet.html#train",
    "href": "diffusion_unet.html#train",
    "title": "Diffusion unet",
    "section": "Train",
    "text": "Train\nBased on Diffusers\n\ndef unet_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n    layers = nn.Sequential()\n    if norm: layers.append(norm(ni))\n    if act : layers.append(act())\n    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n    return layers\n\n\nclass UnetResBlock(nn.Module):\n    def __init__(self, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d):\n        super().__init__()\n        if nf is None: nf = ni\n        self.convs = nn.Sequential(unet_conv(ni, nf, ks, act=act, norm=norm),\n                                   unet_conv(nf, nf, ks, act=act, norm=norm))\n        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n\n    def forward(self, x): return self.convs(x) + self.idconv(x)\n\n\nclass A:\n    def __call__(self):\n        super().__call__()\n        print('a')\n\nclass B:\n    def __call__(self): print('b')\n\nclass C(A,B): pass\n\n\nC()()\n\nb\na\n\n\n\nclass SaveModule:\n    def forward(self, x, *args, **kwargs):\n        self.saved = super().forward(x, *args, **kwargs)\n        return self.saved\n\nclass SavedResBlock(SaveModule, UnetResBlock): pass\nclass SavedConv(SaveModule, nn.Conv2d): pass\n\n\ndef down_block(ni, nf, add_down=True, num_layers=1):\n    res = nn.Sequential(*[SavedResBlock(ni=ni if i==0 else nf, nf=nf)\n                         for i in range(num_layers)])\n    if add_down: res.append(SavedConv(nf, nf, 3, stride=2, padding=1))\n    return res\n\n\ndef upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))\n\n\nclass UpBlock(nn.Module):\n    def __init__(self, ni, prev_nf, nf, add_up=True, num_layers=2):\n        super().__init__()\n        self.resnets = nn.ModuleList(\n            [UnetResBlock((prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf)\n            for i in range(num_layers)])\n        self.up = upsample(nf) if add_up else nn.Identity()\n\n    def forward(self, x, ups):\n        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1))\n        return self.up(x)\n\n\nclass UNet2DModel(nn.Module):\n    def __init__( self, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n        super().__init__()\n        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n        nf = nfs[0]\n        self.downs = nn.Sequential()\n        for i in range(len(nfs)):\n            ni = nf\n            nf = nfs[i]\n            self.downs.append(down_block(ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n        self.mid_block = UnetResBlock(nfs[-1])\n\n        rev_nfs = list(reversed(nfs))\n        nf = rev_nfs[0]\n        self.ups = nn.ModuleList()\n        for i in range(len(nfs)):\n            prev_nf = nf\n            nf = rev_nfs[i]\n            ni = rev_nfs[min(i+1, len(nfs)-1)]\n            self.ups.append(UpBlock(ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n        self.conv_out = unet_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d)\n\n    def forward(self, inp):\n        x = self.conv_in(inp[0])\n        saved = [x]\n        x = self.downs(x)\n        saved += [p.saved for o in self.downs for p in o]\n        x = self.mid_block(x)\n        for block in self.ups: x = block(x, saved)\n        return self.conv_out(x)\n\n\nmodel = UNet2DModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n\n\nlr = 3e-3\nepochs = 25\nopt_func = partial(optim.Adam, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion_unet.html#timesteps",
    "href": "diffusion_unet.html#timesteps",
    "title": "Diffusion unet",
    "section": "Timesteps",
    "text": "Timesteps\n\nemb_dim = 16\ntsteps = torch.linspace(-10,10,100)\nmax_period = 10000\n\n\nmath.log(10000)\n\n9.210340371976184\n\n\n\nexponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n\n\nplt.plot(exponent);\n\n\n\n\n\n\n\n\n\nemb = tsteps[:,None].float() * exponent.exp()[None,:]\nemb.shape\n\ntorch.Size([100, 8])\n\n\n\nplt.plot(emb[0])\nplt.plot(emb[10])\nplt.plot(emb[20])\nplt.plot(emb[50])\nplt.plot(emb[-1]);\n\n\n\n\n\n\n\n\n\nemb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\nemb.shape\n\ntorch.Size([100, 16])\n\n\n\nplt.plot(emb[:,0])\nplt.plot(emb[:,1])\nplt.plot(emb[:,2])\nplt.plot(emb[:,3])\nplt.plot(emb[:,4]);\n\n\n\n\n\n\n\n\n\nplt.plot(emb[:,8])\nplt.plot(emb[:,9])\nplt.plot(emb[:,10]);\n\n\n\n\n\n\n\n\n\nshow_image(emb.T, figsize=(7,7));\n\n\n\n\n\n\n\n\n\ndef timestep_embedding(tsteps, emb_dim, max_period= 10000):\n    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n    emb = tsteps[:,None].float() * exponent.exp()[None,:]\n    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n    return F.pad(emb, (0,1,0,0)) if emb_dim%2==1 else emb\n\n\nshow_image(timestep_embedding(tsteps, 32, max_period=1000).T, figsize=(7,7));\n\n\n\n\n\n\n\n\n\nshow_image(timestep_embedding(tsteps, 32, max_period=10).T, figsize=(7,7));",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion_unet.html#timestep-model",
    "href": "diffusion_unet.html#timestep-model",
    "title": "Diffusion unet",
    "section": "Timestep model",
    "text": "Timestep model\n\nfrom functools import wraps\n\n\ndef lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n    layers = nn.Sequential()\n    if norm: layers.append(norm(ni))\n    if act : layers.append(act())\n    layers.append(nn.Linear(ni, nf, bias=bias))\n    return layers\n\n\nclass EmbResBlock(nn.Module):\n    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d):\n        super().__init__()\n        if nf is None: nf = ni\n        self.emb_proj = nn.Linear(n_emb, nf*2)\n        self.conv1 = unet_conv(ni, nf, ks, act=act, norm=norm) #, bias=not norm)\n        self.conv2 = unet_conv(nf, nf, ks, act=act, norm=norm)\n        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n\n    def forward(self, x, t):\n        inp = x\n        x = self.conv1(x)\n        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n        scale,shift = torch.chunk(emb, 2, dim=1)\n        x = x*(1+scale) + shift\n        x = self.conv2(x)\n        return x + self.idconv(inp)\n\n\ndef saved(m, blk):\n    m_ = m.forward\n\n    @wraps(m.forward)\n    def _f(*args, **kwargs):\n        res = m_(*args, **kwargs)\n        blk.saved.append(res)\n        return res\n\n    m.forward = _f\n    return m\n\n\nclass DownBlock(nn.Module):\n    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1):\n        super().__init__()\n        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf), self)\n                                      for i in range(num_layers)])\n        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n\n    def forward(self, x, t):\n        self.saved = []\n        for resnet in self.resnets: x = resnet(x, t)\n        x = self.down(x)\n        return x\n\n\nclass UpBlock(nn.Module):\n    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2):\n        super().__init__()\n        self.resnets = nn.ModuleList(\n            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf)\n            for i in range(num_layers)])\n        self.up = upsample(nf) if add_up else nn.Identity()\n\n    def forward(self, x, t, ups):\n        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n        return self.up(x)\n\n\nclass EmbUNetModel(nn.Module):\n    def __init__( self, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n        super().__init__()\n        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n        self.n_temb = nf = nfs[0]\n        n_emb = nf*4\n        # TODO: remove act func from 1st MLP layer\n        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n                                     lin(n_emb, n_emb))\n        self.downs = nn.ModuleList()\n        for i in range(len(nfs)):\n            ni = nf\n            nf = nfs[i]\n            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n\n        rev_nfs = list(reversed(nfs))\n        nf = rev_nfs[0]\n        self.ups = nn.ModuleList()\n        for i in range(len(nfs)):\n            prev_nf = nf\n            nf = rev_nfs[i]\n            ni = rev_nfs[min(i+1, len(nfs)-1)]\n            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n        self.conv_out = unet_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n\n    def forward(self, inp):\n        x,t = inp\n        temb = timestep_embedding(t, self.n_temb)\n        emb = self.emb_mlp(temb)\n        x = self.conv_in(x)\n        saved = [x]\n        for block in self.downs: x = block(x, emb)\n        saved += [p for o in self.downs for p in o.saved]\n        x = self.mid_block(x, emb)\n        for block in self.ups: x = block(x, emb, saved)\n        return self.conv_out(x)\n\n\nmodel = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n\n\nlr = 1e-2\nepochs = 25\nopt_func = partial(optim.Adam, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\nmodel = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.409\n0\ntrain\n\n\n0.304\n0\neval\n\n\n0.221\n1\ntrain\n\n\n0.338\n1\neval\n\n\n0.193\n2\ntrain\n\n\n0.215\n2\neval\n\n\n0.182\n3\ntrain\n\n\n0.219\n3\neval\n\n\n0.175\n4\ntrain\n\n\n0.201\n4\neval\n\n\n0.169\n5\ntrain\n\n\n0.206\n5\neval\n\n\n0.165\n6\ntrain\n\n\n0.240\n6\neval\n\n\n0.162\n7\ntrain\n\n\n0.180\n7\neval\n\n\n0.157\n8\ntrain\n\n\n0.186\n8\neval\n\n\n0.155\n9\ntrain\n\n\n0.222\n9\neval\n\n\n0.153\n10\ntrain\n\n\n0.190\n10\neval\n\n\n0.151\n11\ntrain\n\n\n0.164\n11\neval\n\n\n0.149\n12\ntrain\n\n\n0.186\n12\neval\n\n\n0.148\n13\ntrain\n\n\n0.158\n13\neval\n\n\n0.146\n14\ntrain\n\n\n0.146\n14\neval\n\n\n0.145\n15\ntrain\n\n\n0.152\n15\neval\n\n\n0.143\n16\ntrain\n\n\n0.148\n16\neval\n\n\n0.143\n17\ntrain\n\n\n0.142\n17\neval\n\n\n0.142\n18\ntrain\n\n\n0.142\n18\neval\n\n\n0.140\n19\ntrain\n\n\n0.140\n19\neval\n\n\n0.139\n20\ntrain\n\n\n0.138\n20\neval\n\n\n0.139\n21\ntrain\n\n\n0.139\n21\neval\n\n\n0.137\n22\ntrain\n\n\n0.139\n22\neval\n\n\n0.137\n23\ntrain\n\n\n0.138\n23\neval\n\n\n0.138\n24\ntrain\n\n\n0.137\n24\neval",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion_unet.html#sampling",
    "href": "diffusion_unet.html#sampling",
    "title": "Diffusion unet",
    "section": "Sampling",
    "text": "Sampling\n\nfrom miniai.fid import ImageEval\n\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\nbs = 2048\ntds2 = dsd.with_transform(transformi)\ndls2 = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\ndt = dls2.train\nxb,yb = next(iter(dt))\n\nie = ImageEval(cmodel, dls2, cbs=[DeviceCB()])\n\n\nsz = (512,1,32,32)\n\n\nsz = (2048,1,32,32)\n\n\ndef sigmas_karras(n, sigma_min=0.01, sigma_max=80., rho=7.):\n    ramp = torch.linspace(0, 1, n)\n    min_inv_rho = sigma_min**(1/rho)\n    max_inv_rho = sigma_max**(1/rho)\n    sigmas = (max_inv_rho + ramp * (min_inv_rho-max_inv_rho))**rho\n    return torch.cat([sigmas, tensor([0.])]).cuda()\n\ndef denoise(model, x, sig):\n    sig = sig[None] #* torch.ones((len(x),1), device=x.device)\n    c_skip,c_out,c_in = scalings(sig)\n    return model((x*c_in, sig))*c_out + x*c_skip\n\n\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    if not eta: return sigma_to, 0.\n    var_to,var_from = sigma_to**2,sigma_from**2\n    sigma_up = min(sigma_to, eta * (var_to * (var_from-var_to)/var_from)**0.5)\n    return (var_to-sigma_up**2)**0.5, sigma_up\n\n@torch.no_grad()\ndef sample_euler_ancestral(x, sigs, i, model, eta=1.):\n    sig,sig2 = sigs[i],sigs[i+1]\n    denoised = denoise(model, x, sig)\n    sigma_down,sigma_up = get_ancestral_step(sig, sig2, eta=eta)\n    x = x + (x-denoised)/sig*(sigma_down-sig)\n    return x + torch.randn_like(x)*sigma_up\n\n\n@torch.no_grad()\ndef sample_euler(x, sigs, i, model):\n    sig,sig2 = sigs[i],sigs[i+1]\n    denoised = denoise(model, x, sig)\n    return x + (x-denoised)/sig*(sig2-sig)\n\n@torch.no_grad()\ndef sample_heun(x, sigs, i, model, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    sig,sig2 = sigs[i],sigs[i+1]\n    n = len(sigs)\n    gamma = min(s_churn/(n-1), 2**0.5-1) if s_tmin&lt;=sig&lt;=s_tmax else 0.\n    eps = torch.randn_like(x) * s_noise\n    sigma_hat = sig * (gamma+1)\n    if gamma &gt; 0: x = x + eps * (sigma_hat**2-sig**2)**0.5\n    denoised = denoise(model, x, sig)\n    d = (x-denoised)/sig\n    dt = sig2-sigma_hat\n    x_2 = x + d*dt\n    if sig2==0: return x_2\n    denoised_2 = denoise(model, x_2, sig2)\n    d_2 = (x_2-denoised_2)/sig2\n    d_prime = (d+d_2)/2\n    return x + d_prime*dt\n\n\ndef sample(sampler, model, steps=100, sigma_max=80., **kwargs):\n    preds = []\n    x = torch.randn(sz).cuda()*sigma_max\n    sigs = sigmas_karras(steps, sigma_max=sigma_max)\n    for i in progress_bar(range(len(sigs)-1)):\n        x = sampler(x, sigs, i, model, **kwargs)\n        preds.append(x)\n    return preds\n\n\npreds = sample_lms(model, steps=20, order=3)\n# preds = sample(sample_euler_ancestral, model, steps=100, eta=1.)\n# preds = sample(sample_euler, model, steps=100)\n# preds = sample(sample_heun, model, steps=20, s_churn=0.5)\n\n\n\n\n\n\n    \n      \n      100.00% [20/20 00:04&lt;00:00]\n    \n    \n\n\n\ns = preds[-1]\ns.min(),s.max()\n\n(tensor(-1.09312, device='cuda:0'), tensor(1.43464, device='cuda:0'))\n\n\n\nshow_images(s[:25].clamp(-1,1), imsize=1.5)\n\n\n\n\n\n\n\n\n\n# lms 20\nie.fid(s),ie.kid(s),s.shape\n\n(6.195896366748002, 0.011938275769352913, torch.Size([2048, 1, 32, 32]))\n\n\n\npreds = sample_lms(model, steps=20, order=3)\ns = preds[-1]\nie.fid(s),ie.kid(s),s.shape\n\n\n\n\n\n\n    \n      \n      100.00% [20/20 00:04&lt;00:00]\n    \n    \n\n\n(4.967668251150826, 0.01714729703962803, torch.Size([2048, 1, 32, 32]))\n\n\n\npreds = sample_lms(model, steps=20, order=3)\ns = preds[-1]\nie.fid(s),ie.kid(s),s.shape\n\n\n\n\n\n\n    \n      \n      100.00% [20/20 00:04&lt;00:00]\n    \n    \n\n\n(4.607266664456915, 0.0245591439306736, torch.Size([2048, 1, 32, 32]))\n\n\n\nfrom scipy import integrate\n\n\ndef linear_multistep_coeff(order, t, i, j):\n    if order-1 &gt; i: raise ValueError(f'Order {order} too high for step {i}')\n    def fn(tau):\n        prod = 1.\n        for k in range(order):\n            if j == k: continue\n            prod *= (tau-t[i-k]) / (t[i-j]-t[i-k])\n        return prod\n    return integrate.quad(fn, t[i], t[i+1], epsrel=1e-4)[0]\n\n@torch.no_grad()\ndef sample_lms(model, steps=100, order=4, sigma_max=80.):\n    preds = []\n    x = torch.randn(sz).cuda()*sigma_max\n    sigs = sigmas_karras(steps, sigma_max=sigma_max)\n    ds = []\n    for i in progress_bar(range(len(sigs)-1)):\n        sig = sigs[i]\n        denoised = denoise(model, x, sig)\n        d = (x-denoised)/sig\n        ds.append(d)\n        if len(ds) &gt; order: ds.pop(0)\n        cur_order = min(i+1, order)\n        coeffs = [linear_multistep_coeff(cur_order, sigs, i, j) for j in range(cur_order)]\n        x = x + sum(coeff*d for coeff, d in zip(coeffs, reversed(ds)))\n        preds.append(x)\n    return preds",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "imgnet_tiny-wide.html",
    "href": "imgnet_tiny-wide.html",
    "title": "Tiny Imagenet",
    "section": "",
    "text": "import os\n# os.environ['CUDA_VISIBLE_DEVICES']='2'\n\n\nimport shutil,timm,os,torch,random,datasets,math\nimport fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\nfrom glob import glob\n\n    \nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.training import *\n\n\nfrom fastprogress import progress_bar\n\n\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\npath_data = Path('Data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'tiny-imagenet-200'\n\nurl = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\nif not path.exists():\n    path_zip = fc.urlsave(url, path_data)\n    shutil.unpack_archive('Data/tiny-imagenet-200.zip', 'data')\n\nbs = 512\n\nclass TinyDS:\n    def __init__(self, path):\n        self.path = Path(path)\n        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return self.files[i],Path(self.files[i]).parent.parent.name\n\ntds = TinyDS(path/'train')\n\n\ntds[0]\n\n('Data/tiny-imagenet-200/train/n02074367/images/n02074367_322.JPEG',\n 'n02074367')\n\n\n\npath_anno = path/'val'/'val_annotations.txt'\nanno = dict(o.split('\\t')[:2] for o in path_anno.read_text().splitlines())\n\n\nclass TinyValDS(TinyDS):\n    def __getitem__(self, i): return self.files[i],anno[os.path.basename(self.files[i])]\n\n\nvds = TinyValDS(path/'val')\n\n\nvds[0]\n\n('Data/tiny-imagenet-200/val/images/val_240.JPEG', 'n02883205')\n\n\n\nclass TfmDS:\n    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n    def __len__(self): return len(self.ds)\n    def __getitem__(self, i):\n        x,y = self.ds[i]\n        return self.tfmx(x),self.tfmy(y)\n\n\nid2str = (path/'wnids.txt').read_text().splitlines()\nstr2id = {v:k for k,v in enumerate(id2str)}\n\n\nxmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))\n\n\ndef tfmx(x):\n    img = read_image(x, mode=ImageReadMode.RGB)/255\n    return (img-xmean[:,None,None])/xstd[:,None,None]\n\ndef tfmy(y): return tensor(str2id[y])\n\ntfm_tds = TfmDS(tds, tfmx, tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\ndef denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clip(0,1)\n\nall_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\nsynsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\ndef tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n\ntfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     RandErase())\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\n\nnfs = (32,64,128,256,512,1024)\n\ndef get_dropmodel(act=act_gr, nfs=nfs, norm=nn.BatchNorm2d, drop=0.1):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n#     layers += [ResBlock(nfs[0], nfs[0], ks=3, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\ndef res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n    return nn.Sequential(*[\n        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n        for i in range(n_bk)])\n\nnbks = (3,2,2,1,1)\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nopt_func = partial(optim.AdamW, eps=1e-5)\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n\nepochs = 25\nlr = 3e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nlearn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\naug_tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     T.TrivialAugmentWide())\n\nnorm_tfm = T.Normalize(xmean, xstd)\nerase_tfm = RandErase()\n\nfrom PIL import Image\n\ndef tfmx(x, aug=False):\n    x = Image.open(x).convert('RGB')\n    if aug: x = aug_tfms(x)\n    x = TF.to_tensor(x)\n    x = norm_tfm(x)\n    if aug: x = erase_tfm(x[None])[0]\n    return x\n\ntfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\ndef conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True):\n    layers = []\n    if norm: layers.append(norm(ni))\n    if act : layers.append(act())\n    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n    return nn.Sequential(*layers)\n\ndef _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n    return nn.Sequential(conv(ni, nf, stride=1     , act=act, norm=norm, ks=ks),\n                         conv(nf, nf, stride=stride, act=act, norm=norm, ks=ks))\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n        super().__init__()\n        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nepochs = 50\nlr = 0.1\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nmodel = get_dropmodel(nbks=(1,2,8,2,2), nfs=(32, 64, 128, 512, 1024, 1536), drop=0.1)\nlearn = Learner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n    \n      \n      0.00% [0/50 00:00&lt;?]\n    \n    \n\n\n    \n      \n      1.02% [2/196 02:32&lt;4:06:46 5.598]\n    \n    \n\n\n\ntorch.save(learn.model, 'models/inettiny-wide-50')\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "resnet.html",
    "href": "resnet.html",
    "title": "ResNets",
    "section": "",
    "text": "Exported source\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nbs = 1024\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n\ndsd = load_dataset(name)\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\nExported source\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\niw = partial(init_weights, leaky=0.1)\nget_model??\n\n\nSignature:\nget_model(\n    act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n    nfs=None,\n    norm=None,\n)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef get_model(act=nn.ReLU, nfs=None, norm=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n                         nn.Flatten()).to(def_device)\nFile:      ~/BENEDICT_Only/Benedict_Projects/Benedict_ML/fastAIcourse/fastAIcourse/init.py\nType:      function",
    "crumbs": [
      "Blog",
      "ResNets"
    ]
  },
  {
    "objectID": "resnet.html#going-deeper",
    "href": "resnet.html#going-deeper",
    "title": "ResNets",
    "section": "Going deeper",
    "text": "Going deeper\n\nsource\n\nget_model\n\n get_model (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(8, 16,\n            32, 64, 128), norm=&lt;class\n            'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n\n\nExported source\ndef get_model(act=nn.ReLU, nfs=(8,16,32,64,128), norm=nn.BatchNorm2d):\n    layers = [conv(1, 8, stride=1, act=act, norm=norm)]\n    layers += [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1], 10, act=None, norm=norm, bias=True), nn.Flatten()).to(def_device)\n\n\n\nset_seed(42)\nlr,epochs = 6e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.812\n0.684\n0\ntrain\n\n\n0.815\n0.608\n0\neval\n\n\n0.888\n0.327\n1\ntrain\n\n\n0.869\n0.394\n1\neval\n\n\n0.909\n0.256\n2\ntrain\n\n\n0.900\n0.283\n2\neval\n\n\n0.927\n0.205\n3\ntrain\n\n\n0.916\n0.242\n3\neval\n\n\n0.943\n0.162\n4\ntrain\n\n\n0.921\n0.227\n4\neval",
    "crumbs": [
      "Blog",
      "ResNets"
    ]
  },
  {
    "objectID": "resnet.html#skip-connections",
    "href": "resnet.html#skip-connections",
    "title": "ResNets",
    "section": "Skip Connections",
    "text": "Skip Connections\nThe ResNet (residual network) was introduced in 2015 by Kaiming He et al in the article “Deep Residual Learning for Image Recognition”. The key idea is using a skip connection to allow deeper networks to train successfully.\n\n\nsource\n\nResBlock\n\n ResBlock (ni, nf, stride=1, ks=3, act=functools.partial(&lt;class\n           'fastAIcourse.init.GeneralRelu'&gt;, leak=0.1, sub=0.4),\n           norm=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\ndef _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n    return nn.Sequential(conv(ni, nf, stride=1, act=act, norm=norm, ks=ks),\n                         conv(nf, nf, stride=stride, act=None, norm=norm, ks=ks))\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n        super().__init__()\n        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n        self.act = act()\n\n    def forward(self, x): return self.act(self.convs(x) + self.idconv(self.pool(x)))\n\n\nPost-lesson update: Piotr Czapla noticed that we forgot to include norm=norm in the call to _conv_block above, so the resnets in the lesson didn’t have batchnorm in the resblocks! After fixing this, we discovered that initializing the conv2 batchnorm weights to zero makes things worse in every model we tried, so we removed that. That init method was originally introduced to handle training extremely deep models (much deeper than we use here) – it appears from this little test that it might be worse for less deep models.\n\nsource\n\n\nget_model\n\n get_model (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(8, 16,\n            32, 64, 128, 256), norm=&lt;class\n            'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n\n\nExported source\ndef get_model(act=nn.ReLU, nfs=(8,16,32,64,128,256), norm=nn.BatchNorm2d):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n    return nn.Sequential(*layers).to(def_device)\n\n\n\ndef _print_shape(hook, mod, inp, outp): print(type(mod).__name__, inp[0].shape, outp.shape)\nmodel = get_model()\nlearn = TrainLearner(model, dls, F.cross_entropy, cbs=[DeviceCB(), SingleBatchCB()])\nwith Hooks(model, _print_shape) as hooks: learn.fit(1, train=False)\n\nResBlock torch.Size([2048, 1, 28, 28]) torch.Size([2048, 8, 28, 28])\nResBlock torch.Size([2048, 8, 28, 28]) torch.Size([2048, 16, 14, 14])\nResBlock torch.Size([2048, 16, 14, 14]) torch.Size([2048, 32, 7, 7])\nResBlock torch.Size([2048, 32, 7, 7]) torch.Size([2048, 64, 4, 4])\nResBlock torch.Size([2048, 64, 4, 4]) torch.Size([2048, 128, 2, 2])\nResBlock torch.Size([2048, 128, 2, 2]) torch.Size([2048, 256, 1, 1])\nFlatten torch.Size([2048, 256, 1, 1]) torch.Size([2048, 256])\nLinear torch.Size([2048, 256]) torch.Size([2048, 10])\nBatchNorm1d torch.Size([2048, 10]) torch.Size([2048, 10])\n\n\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\n\nExported source\n@fc.patch\ndef summary(self:Learner):\n    res = '|Module|Input|Output|Num params|\\n|--|--|--|--|\\n'\n    tot = 0\n    def _f(hook, mod, inp, outp):\n        nonlocal res,tot\n        nparms = sum(o.numel() for o in mod.parameters())\n        tot += nparms\n        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparms}|\\n'\n    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, train=False, cbs=SingleBatchCB())\n    print(\"Tot params: \", tot)\n    if fc.IN_NOTEBOOK:\n        from IPython.display import Markdown\n        return Markdown(res)\n    else: print(res)\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  1228908\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nResBlock\n(2048, 1, 28, 28)\n(2048, 8, 28, 28)\n712\n\n\nResBlock\n(2048, 8, 28, 28)\n(2048, 16, 14, 14)\n3696\n\n\nResBlock\n(2048, 16, 14, 14)\n(2048, 32, 7, 7)\n14560\n\n\nResBlock\n(2048, 32, 7, 7)\n(2048, 64, 4, 4)\n57792\n\n\nResBlock\n(2048, 64, 4, 4)\n(2048, 128, 2, 2)\n230272\n\n\nResBlock\n(2048, 128, 2, 2)\n(2048, 256, 1, 1)\n919296\n\n\nFlatten\n(2048, 256, 1, 1)\n(2048, 256)\n0\n\n\nLinear\n(2048, 256)\n(2048, 10)\n2560\n\n\nBatchNorm1d\n(2048, 10)\n(2048, 10)\n20\n\n\n\n\n\n\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nMomentumLearner(model, dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr = 2e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.824\n0.687\n0\ntrain\n\n\n0.834\n0.553\n0\neval\n\n\n0.896\n0.344\n1\ntrain\n\n\n0.847\n0.468\n1\neval\n\n\n0.916\n0.252\n2\ntrain\n\n\n0.903\n0.288\n2\neval\n\n\n0.935\n0.196\n3\ntrain\n\n\n0.917\n0.238\n3\neval\n\n\n0.954\n0.145\n4\ntrain\n\n\n0.929\n0.210\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport timm\nfrom timm.models.resnet import BasicBlock, ResNet, Bottleneck\n\n\n' '.join(timm.list_models('*resnet*'))\n\n'cspresnet50 cspresnet50d cspresnet50w eca_resnet33ts ecaresnet26t ecaresnet50d ecaresnet50d_pruned ecaresnet50t ecaresnet101d ecaresnet101d_pruned ecaresnet200d ecaresnet269d ecaresnetlight gcresnet33ts gcresnet50t inception_resnet_v2 lambda_resnet26rpt_256 lambda_resnet26t lambda_resnet50ts legacy_seresnet18 legacy_seresnet34 legacy_seresnet50 legacy_seresnet101 legacy_seresnet152 nf_ecaresnet26 nf_ecaresnet50 nf_ecaresnet101 nf_resnet26 nf_resnet50 nf_resnet101 nf_seresnet26 nf_seresnet50 nf_seresnet101 resnet10t resnet14t resnet18 resnet18d resnet26 resnet26d resnet26t resnet32ts resnet33ts resnet34 resnet34d resnet50 resnet50_gn resnet50c resnet50d resnet50s resnet50t resnet51q resnet61q resnet101 resnet101c resnet101d resnet101s resnet152 resnet152c resnet152d resnet152s resnet200 resnet200d resnetaa34d resnetaa50 resnetaa50d resnetaa101d resnetblur18 resnetblur50 resnetblur50d resnetblur101d resnetrs50 resnetrs101 resnetrs152 resnetrs200 resnetrs270 resnetrs350 resnetrs420 resnetv2_50 resnetv2_50d resnetv2_50d_evos resnetv2_50d_frn resnetv2_50d_gn resnetv2_50t resnetv2_50x1_bit resnetv2_50x3_bit resnetv2_101 resnetv2_101d resnetv2_101x1_bit resnetv2_101x3_bit resnetv2_152 resnetv2_152d resnetv2_152x2_bit resnetv2_152x4_bit seresnet18 seresnet33ts seresnet34 seresnet50 seresnet50t seresnet101 seresnet152 seresnet152d seresnet200d seresnet269d seresnetaa50d skresnet18 skresnet34 skresnet50 skresnet50d tresnet_l tresnet_m tresnet_v2_l tresnet_xl vit_base_resnet26d_224 vit_base_resnet50d_224 vit_small_resnet26d_224 vit_small_resnet50d_s16_224 wide_resnet50_2 wide_resnet101_2'\n\n\nresnet18:  block=BasicBlock, layers=[2, 2, 2, 2]\nresnet18d: block=BasicBlock, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep', avg_down=True\nresnet10t: block=BasicBlock, layers=[1, 1, 1, 1], stem_width=32, stem_type='deep_tiered', avg_down=True\n\nmodel = timm.create_model('resnet18d', in_chans=1, num_classes=10)\n# model = ResNet(in_chans=1, block=BasicBlock, layers=[2,2,2,2], stem_width=32, avg_down=True)\n\n\nlr = 2e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.780\n0.632\n0\ntrain\n\n\n0.596\n1.484\n0\neval\n\n\n0.877\n0.328\n1\ntrain\n\n\n0.867\n0.363\n1\neval\n\n\n0.908\n0.248\n2\ntrain\n\n\n0.884\n0.332\n2\neval\n\n\n0.928\n0.192\n3\ntrain\n\n\n0.912\n0.239\n3\neval\n\n\n0.947\n0.143\n4\ntrain\n\n\n0.917\n0.224\n4\neval",
    "crumbs": [
      "Blog",
      "ResNets"
    ]
  },
  {
    "objectID": "superres.html",
    "href": "superres.html",
    "title": "Tiny Imagenet",
    "section": "",
    "text": "import os\n# os.environ['CUDA_VISIBLE_DEVICES']='1'\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.training import *\nfrom fastprogress import progress_bar\nfrom glob import glob\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "superres.html#data-processing",
    "href": "superres.html#data-processing",
    "title": "Tiny Imagenet",
    "section": "Data processing",
    "text": "Data processing\n\npath = Path.home()/'data'/'tiny-imagenet-200'\nbs = 512\n# bs = 32\nxmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))\n\n\ntfms = nn.Sequential(T.Pad(8), T.RandomCrop(64), T.RandomHorizontalFlip())\n\n\nclass TinyDS:\n    def __init__(self, path):\n        self.path = Path(path)\n        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i):\n        img = read_image(self.files[i], mode=ImageReadMode.RGB)/255\n        return tfms((img-xmean[:,None,None])/xstd[:,None,None])\n\nclass TfmDS:\n    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n    def __len__(self): return len(self.ds)\n    def __getitem__(self, i):\n        item = self.ds[i]\n        return self.tfmx(item),self.tfmy(item)\n\ndef denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clamp(0,1)\n\n\ndef tfmx(x, erase=True):\n    x = TF.resize(x, (32,32))[None]\n    x = F.interpolate(x, scale_factor=2)\n    if erase: x = rand_erase(x)\n    return x[0]\n\n\ntds = TinyDS(path/'train')\nvds = TinyDS(path/'val')\n\ntfm_tds = TfmDS(tds, tfmx)\ntfm_vds = TfmDS(vds, partial(tfmx, erase=False))\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\nxb,yb = next(iter(dls.train))\n\n\nshow_images(denorm(xb[:4]), imsize=2.5)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(yb[:4]), imsize=2.5)",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "superres.html#denoising-autoencoder",
    "href": "superres.html#denoising-autoencoder",
    "title": "Tiny Imagenet",
    "section": "Denoising autoencoder",
    "text": "Denoising autoencoder\n\ndef up_block(ni, nf, ks=3, act=act_gr, norm=None):\n    return nn.Sequential(nn.UpsamplingNearest2d(scale_factor=2),\n                         ResBlock(ni, nf, ks=ks, act=act, norm=norm))\n\n\ndef get_model(act=act_gr, nfs=(32,64,128,256,512,1024), norm=nn.BatchNorm2d, drop=0.1):\n    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [up_block(nfs[i], nfs[i-1], act=act, norm=norm) for i in range(len(nfs)-1,0,-1)]\n    layers += [ResBlock(nfs[0], 3, act=nn.Identity, norm=norm)]\n    return nn.Sequential(*layers).apply(iw)\n\n\niw = partial(init_weights, leaky=0.1)\n\n\nmetrics = MetricsCB()\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\nlr_cbs = [DeviceCB(), ProgressCB(), MixedPrecision()]\nopt_func = partial(optim.AdamW, eps=1e-5)\n\n\nLearner(get_model().apply(iw), dls, F.mse_loss, cbs=lr_cbs, opt_func=opt_func).lr_find(start_lr=1e-4, gamma=1.2)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;00:00]\n    \n    \n\n\n    \n      \n      12.24% [24/196 00:06&lt;00:45 1.320]\n    \n    \n\n\n\n\n\n\n\n\n\n\nepochs = 5\nlr = 1e-3\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(get_model().apply(iw), dls, F.mse_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.584\n0\ntrain\n\n\n0.361\n0\neval\n\n\n0.314\n1\ntrain\n\n\n0.268\n1\neval\n\n\n0.252\n2\ntrain\n\n\n0.225\n2\neval\n\n\n0.228\n3\ntrain\n\n\n0.211\n3\neval\n\n\n0.220\n4\ntrain\n\n\n0.207\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\np,t,inp = learn.capture_preds(inps=True)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.207\n0\neval\n\n\n\n\n\n\nshow_images(denorm(inp[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(p[:9]), imsize=2)",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "superres.html#unet",
    "href": "superres.html#unet",
    "title": "Tiny Imagenet",
    "section": "Unet",
    "text": "Unet\n\ndel(learn)\nclean_mem()\n\n\nclass TinyUnet(nn.Module):\n    def __init__(self, act=act_gr, nfs=(32,64,128,256,512,1024), norm=nn.BatchNorm2d):\n        super().__init__()\n        self.start = ResBlock(3, nfs[0], stride=1, act=act, norm=norm)\n        self.dn = nn.ModuleList([ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n                                 for i in range(len(nfs)-1)])\n        self.up = nn.ModuleList([up_block(nfs[i], nfs[i-1], act=act, norm=norm)\n                                 for i in range(len(nfs)-1,0,-1)])\n        self.up += [ResBlock(nfs[0], 3, act=act, norm=norm)]\n        self.end = ResBlock(3, 3, act=nn.Identity, norm=norm)\n\n    def forward(self, x):\n        layers = []\n        layers.append(x)\n        x = self.start(x)\n        for l in self.dn:\n            layers.append(x)\n            x = l(x)\n        n = len(layers)\n        for i,l in enumerate(self.up):\n            if i!=0: x += layers[n-i]\n            x = l(x)\n        return self.end(x+layers[0])\n\n\ndef zero_wgts(l):\n    with torch.no_grad():\n        l.weight.zero_()\n        l.bias.zero_()\n\n\nmodel = TinyUnet()\n\n\nlast_res = model.up[-1]\nzero_wgts(last_res.convs[-1][-1])\nzero_wgts(last_res.idconv[0])\nzero_wgts(model.end.convs[-1][-1])\n\n\nLearner(model, dls, F.mse_loss, cbs=lr_cbs, opt_func=opt_func).lr_find(start_lr=1e-4, gamma=1.2)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;00:00]\n    \n    \n\n\n    \n      \n      17.35% [34/196 00:08&lt;00:38 0.270]\n    \n    \n\n\n\n\n\n\n\n\n\n\nmodel = TinyUnet()\n\n\nlast_res = model.up[-1]\nzero_wgts(last_res.convs[-1][-1])\nzero_wgts(last_res.idconv[0])\nzero_wgts(model.end.convs[-1][-1])\n\n\nepochs = 20\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(model, dls, F.mse_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.163\n0\ntrain\n\n\n0.086\n0\neval\n\n\n0.107\n1\ntrain\n\n\n0.082\n1\neval\n\n\n0.097\n2\ntrain\n\n\n0.080\n2\neval\n\n\n0.094\n3\ntrain\n\n\n0.079\n3\neval\n\n\n0.092\n4\ntrain\n\n\n0.077\n4\neval\n\n\n0.091\n5\ntrain\n\n\n0.077\n5\neval\n\n\n0.090\n6\ntrain\n\n\n0.076\n6\neval\n\n\n0.089\n7\ntrain\n\n\n0.076\n7\neval\n\n\n0.088\n8\ntrain\n\n\n0.075\n8\neval\n\n\n0.088\n9\ntrain\n\n\n0.075\n9\neval\n\n\n0.087\n10\ntrain\n\n\n0.075\n10\neval\n\n\n0.087\n11\ntrain\n\n\n0.075\n11\neval\n\n\n0.086\n12\ntrain\n\n\n0.074\n12\neval\n\n\n0.086\n13\ntrain\n\n\n0.074\n13\neval\n\n\n0.086\n14\ntrain\n\n\n0.074\n14\neval\n\n\n0.086\n15\ntrain\n\n\n0.074\n15\neval\n\n\n0.085\n16\ntrain\n\n\n0.073\n16\neval\n\n\n0.085\n17\ntrain\n\n\n0.073\n17\neval\n\n\n0.085\n18\ntrain\n\n\n0.073\n18\neval\n\n\n0.085\n19\ntrain\n\n\n0.073\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\np,t,inp = learn.capture_preds(inps=True)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.073\n0\neval\n\n\n\n\n\n\nshow_images(denorm(inp[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(p[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(t[:9]), imsize=2)",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "superres.html#perceptual-loss",
    "href": "superres.html#perceptual-loss",
    "title": "Tiny Imagenet",
    "section": "Perceptual loss",
    "text": "Perceptual loss\n\n# del(learn)\n# clean_mem()\n\n\ncmodel = torch.load('models/inettiny-custom-25').cuda()\n\n\nxb,yb = next(iter(dls.valid))\nwith torch.autocast('cuda'),torch.no_grad(): preds = to_cpu(cmodel(yb.cuda().half()))\npreds.shape\n\ntorch.Size([1024, 200])\n\n\n\nid2str = (path/'wnids.txt').read_text().splitlines()\nall_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\nsynsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}\n\n\ntitles = [synsets[id2str[o]] for o in preds.argmax(dim=1)]\nshow_images(denorm(yb[:16]), imsize=2, titles=titles[:16])\n\n\n\n\n\n\n\n\n\nfor i in range(4,len(cmodel)): del(cmodel[4])\n\n\nlearn.model = torch.load('models/superres-cross.pkl')\n\n\nwith torch.autocast('cuda'),torch.no_grad():\n    feat = to_cpu(cmodel(yb.cuda())).float()\n    t = to_cpu(learn.model(yb.cuda())).float()\n    pred_feat = to_cpu(cmodel(t.cuda())).float()\n\nfeat.shape\n\ntorch.Size([1024, 256, 8, 8])\n\n\n\ndef comb_loss(inp, tgt):\n    with torch.autocast('cuda'):\n        with torch.no_grad(): tgt_feat = cmodel(tgt).float()\n        inp_feat = cmodel(inp).float()\n    feat_loss = F.mse_loss(inp_feat, tgt_feat)\n    return F.mse_loss(inp,tgt) + feat_loss/10\n\n\ndef get_unet():\n    model = TinyUnet()\n    last_res = model.up[-1]\n    zero_wgts(last_res.convs[-1][-1])\n    zero_wgts(last_res.idconv[0])\n    zero_wgts(model.end.convs[-1][-1])\n    return model\n\n\nLearner(get_unet(), dls, comb_loss, cbs=lr_cbs, opt_func=opt_func).lr_find(start_lr=1e-4, gamma=1.2)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;00:00]\n    \n    \n\n\n    \n      \n      16.84% [33/196 00:10&lt;00:50 0.735]\n    \n    \n\n\n\n\n\n\n\n\n\n\nepochs = 20\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(get_unet(), dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.602\n0\ntrain\n\n\n0.385\n0\neval\n\n\n0.477\n1\ntrain\n\n\n0.354\n1\neval\n\n\n0.434\n2\ntrain\n\n\n0.348\n2\neval\n\n\n0.415\n3\ntrain\n\n\n0.343\n3\neval\n\n\n0.404\n4\ntrain\n\n\n0.337\n4\neval\n\n\n0.397\n5\ntrain\n\n\n0.336\n5\neval\n\n\n0.390\n6\ntrain\n\n\n0.339\n6\neval\n\n\n0.384\n7\ntrain\n\n\n0.328\n7\neval\n\n\n0.381\n8\ntrain\n\n\n0.329\n8\neval\n\n\n0.378\n9\ntrain\n\n\n0.321\n9\neval\n\n\n0.374\n10\ntrain\n\n\n0.321\n10\neval\n\n\n0.370\n11\ntrain\n\n\n0.316\n11\neval\n\n\n0.368\n12\ntrain\n\n\n0.312\n12\neval\n\n\n0.365\n13\ntrain\n\n\n0.313\n13\neval\n\n\n0.362\n14\ntrain\n\n\n0.310\n14\neval\n\n\n0.360\n15\ntrain\n\n\n0.306\n15\neval\n\n\n0.357\n16\ntrain\n\n\n0.305\n16\neval\n\n\n0.355\n17\ntrain\n\n\n0.303\n17\neval\n\n\n0.354\n18\ntrain\n\n\n0.302\n18\neval\n\n\n0.354\n19\ntrain\n\n\n0.303\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\np,t,inp = learn.capture_preds(inps=True)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.303\n0\neval\n\n\n\n\n\n\nshow_images(denorm(inp[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(p[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(t[:9]), imsize=2)",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "superres.html#perceptual-loss-1",
    "href": "superres.html#perceptual-loss-1",
    "title": "Tiny Imagenet",
    "section": "Perceptual loss",
    "text": "Perceptual loss\n\nmodel = get_unet()\n\n\npmodel = torch.load('models/inettiny-custom-25')\nmodel.start.load_state_dict(pmodel[0].state_dict())\nfor i in range(5): model.dn[i].load_state_dict(pmodel[i+1].state_dict())\n\n\nfor o in model.dn.parameters(): o.requires_grad_(False)\n\n\nepochs = 1\nlr = 3e-3\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.444\n0\ntrain\n\n\n0.255\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor o in model.dn.parameters(): o.requires_grad_(True)\n\n\nepochs = 20\nlr = 3e-3\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.344\n0\ntrain\n\n\n0.249\n0\neval\n\n\n0.327\n1\ntrain\n\n\n0.246\n1\neval\n\n\n0.309\n2\ntrain\n\n\n0.252\n2\neval\n\n\n0.296\n3\ntrain\n\n\n0.243\n3\neval\n\n\n0.287\n4\ntrain\n\n\n0.226\n4\neval\n\n\n0.279\n5\ntrain\n\n\n0.232\n5\neval\n\n\n0.274\n6\ntrain\n\n\n0.226\n6\neval\n\n\n0.268\n7\ntrain\n\n\n0.221\n7\neval\n\n\n0.265\n8\ntrain\n\n\n0.240\n8\neval\n\n\n0.261\n9\ntrain\n\n\n0.215\n9\neval\n\n\n0.258\n10\ntrain\n\n\n0.226\n10\neval\n\n\n0.256\n11\ntrain\n\n\n0.213\n11\neval\n\n\n0.253\n12\ntrain\n\n\n0.213\n12\neval\n\n\n0.250\n13\ntrain\n\n\n0.205\n13\neval\n\n\n0.248\n14\ntrain\n\n\n0.207\n14\neval\n\n\n0.247\n15\ntrain\n\n\n0.202\n15\neval\n\n\n0.245\n16\ntrain\n\n\n0.202\n16\neval\n\n\n0.244\n17\ntrain\n\n\n0.199\n17\neval\n\n\n0.243\n18\ntrain\n\n\n0.199\n18\neval\n\n\n0.243\n19\ntrain\n\n\n0.198\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/superres-pcp.pkl')\n# learn.model = torch.load('models/superres-pcp.pkl').cuda()\n\n\np,t,inp = learn.capture_preds(inps=True)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.198\n0\neval\n\n\n\n\n\n\nshow_images(denorm(inp[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(p[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(t[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/superres-pcp.pkl')\n# learn.model = torch.load('models/superres-pcp.pkl').cuda()",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "superres.html#cross-convs",
    "href": "superres.html#cross-convs",
    "title": "Tiny Imagenet",
    "section": "Cross-convs",
    "text": "Cross-convs\n\ndef cross_conv(nf, act, norm):\n    return nn.Sequential(\n        ResBlock(nf, nf, act=act, norm=norm),\n        nn.Conv2d(nf, nf, 3, padding=1)\n    )\n\n\nclass TinyUnet(nn.Module):\n    def __init__(self, act=act_gr, nfs=(32,64,128,256,512,1024), norm=nn.BatchNorm2d):\n        super().__init__()\n        self.start = ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)\n        self.dn = nn.ModuleList([ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n                                 for i in range(len(nfs)-1)])\n        self.xs = nn.ModuleList([cross_conv(nfs[i], act, norm)\n                                 for i in range(len(nfs)-1,0,-1)])\n        self.xs += [cross_conv(nfs[0], act, norm)]\n        self.up = nn.ModuleList([up_block(nfs[i], nfs[i-1], act=act, norm=norm)\n                                 for i in range(len(nfs)-1,0,-1)])\n        self.up += [ResBlock(nfs[0], 3, act=act, norm=norm)]\n        self.end = ResBlock(3, 3, act=nn.Identity, norm=norm)\n\n    def forward(self, x):\n        layers = []\n        layers.append(x)\n        x = self.start(x)\n        for i,l in enumerate(self.dn):\n            layers.append(x)\n            x = l(x)\n        n = len(layers)\n        for i,l in enumerate(self.up):\n            if i!=0: x += self.xs[i](layers[n-i])\n            x = l(x)\n        return self.end(x+layers[0])\n\n\npmodel = torch.load('models/inettiny-custom-25')\n\n\nmodel = get_unet()\n\n\nmodel.start.load_state_dict(pmodel[0].state_dict())\nfor i in range(5): model.dn[i].load_state_dict(pmodel[i+1].state_dict())\nfor o in model.dn.parameters(): o.requires_grad_(False)\n\n\nepochs = 1\nlr = 3e-3\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.422\n0\ntrain\n\n\n0.243\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor o in model.dn.parameters(): o.requires_grad_(True)\n\n\nepochs = 20\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(model, dls, comb_loss, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.316\n0\ntrain\n\n\n0.234\n0\neval\n\n\n0.294\n1\ntrain\n\n\n0.222\n1\neval\n\n\n0.282\n2\ntrain\n\n\n0.221\n2\neval\n\n\n0.275\n3\ntrain\n\n\n0.224\n3\neval\n\n\n0.269\n4\ntrain\n\n\n0.223\n4\neval\n\n\n0.264\n5\ntrain\n\n\n0.221\n5\neval\n\n\n0.259\n6\ntrain\n\n\n0.215\n6\neval\n\n\n0.254\n7\ntrain\n\n\n0.208\n7\neval\n\n\n0.249\n8\ntrain\n\n\n0.206\n8\neval\n\n\n0.246\n9\ntrain\n\n\n0.211\n9\neval\n\n\n0.243\n10\ntrain\n\n\n0.202\n10\neval\n\n\n0.240\n11\ntrain\n\n\n0.199\n11\neval\n\n\n0.237\n12\ntrain\n\n\n0.199\n12\neval\n\n\n0.235\n13\ntrain\n\n\n0.197\n13\neval\n\n\n0.232\n14\ntrain\n\n\n0.193\n14\neval\n\n\n0.230\n15\ntrain\n\n\n0.192\n15\neval\n\n\n0.227\n16\ntrain\n\n\n0.191\n16\neval\n\n\n0.226\n17\ntrain\n\n\n0.190\n17\neval\n\n\n0.224\n18\ntrain\n\n\n0.189\n18\neval\n\n\n0.224\n19\ntrain\n\n\n0.189\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\np,t,inp = learn.capture_preds(inps=True)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.189\n0\neval\n\n\n\n\n\n\nshow_images(denorm(inp[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(p[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\nshow_images(denorm(t[:9]), imsize=2)\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/superres-cross.pkl')\n# learn.model = torch.load('models/superres-pcp.pkl').cuda()",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html",
    "href": "collaborative-filtering-deep-dive.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "This is chapter 8 of the book Practical Deep Learning for Coders, provided courtesy of O’Reilly Media. The full book is available as Jupyter Notebooks. A free course that covers the book is available here.\nClick Copy and Edit in the top right to run the code in this notebook yourself.\nOne very common problem to solve is when you have a number of users and a number of products, and you want to recommend which products are most likely to be useful for which users. There are many variations of this: for example, recommending movies (such as on Netflix), figuring out what to highlight for a user on a home page, deciding what stories to show in a social media feed, and so forth. There is a general solution to this problem, called collaborative filtering, which works like this: look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend other products that those users have used or liked.\nFor example, on Netflix you may have watched lots of movies that are science fiction, full of action, and were made in the 1970s. Netflix may not know these particular properties of the films you have watched, but it will be able to see that other people that have watched the same movies that you watched also tended to watch other movies that are science fiction, full of action, and were made in the 1970s. In other words, to use this approach we don’t necessarily need to know anything about the movies, except who like to watch them.\nThere is actually a more general class of problems that this approach can solve, not necessarily involving users and products. Indeed, for collaborative filtering we more commonly refer to items, rather than products. Items could be links that people click on, diagnoses that are selected for patients, and so forth.\nThe key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.\nFor this chapter we are going to work on this movie recommendation problem. We’ll start by getting some data suitable for a collaborative filtering model.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#a-first-look-at-the-data",
    "href": "collaborative-filtering-deep-dive.html#a-first-look-at-the-data",
    "title": "Collaborative Filtering",
    "section": "A First Look at the Data",
    "text": "A First Look at the Data\nWe do not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can use, called MovieLens. This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If you’re interested, it would be a great learning project to try and replicate this approach on the full 25-million recommendation dataset, which you can get from their website.\nThe dataset is available through the usual fastai function:\n\npath = untar_data(URLs.ML_100k)\npath\n\n\n\n\n\n\n    \n      \n      100.15% [4931584/4924029 00:02&lt;00:00]\n    \n    \n\n\nPath('/home/ben/.fastai/data/ml-100k')\n\n\nAccording to the README, the main table is in the file u.data. It is tab-separated and the columns are, respectively user, movie, rating, and timestamp. Since those names are not encoded, we need to indicate them when reading the file with Pandas. Here is a way to open this table and take a look:\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n\n\n\n\n\nAlthough this has all the information we need, it is not a particularly helpful way for humans to look at this data. Here is the same data cross-tabulated into a human-friendly table:\n\n\n\nimage.png\n\n\nWe have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\nIf we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and +1, with positive numbers indicating stronger matches and negative numbers weaker ones, and the categories are science-fiction, action, and old movies, then we could represent the movie The Last Skywalker as:\n\nlast_skywalker = np.array([0.98,0.9,-0.9])\nlast_skywalker\n\narray([ 0.98,  0.9 , -0.9 ])\n\n\nHere, for instance, we are scoring very science-fiction as 0.98, very action as 0.9, and very not old as -0.9. We could represent a user who likes modern sci-fi action movies as:\n\nuser1 = np.array([0.9,0.8,-0.6])\n\nand we can now calculate the match between this combination:\n\n(user1*last_skywalker).sum()\n\n2.1420000000000003\n\n\nWhen we multiply two vectors together and add up the results, this is known as the dot product. It is used a lot in machine learning, and forms the basis of matrix multiplication. We will be looking a lot more at matrix multiplication and dot products in &lt;&gt;.\n\njargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result.\n\nOn the other hand, we might represent the movie Casablanca as:\n\ncasablanca = np.array([-0.99,-0.3,0.8])\n\nThe match between this combination is:\n\n(user1*casablanca).sum()\n\n-1.611\n\n\nSince we don’t know what the latent factors actually are, and we don’t know how to score them for each user and movie, we should learn them.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#learning-the-latent-factors",
    "href": "collaborative-filtering-deep-dive.html#learning-the-latent-factors",
    "title": "Collaborative Filtering",
    "section": "Learning the Latent Factors",
    "text": "Learning the Latent Factors\nThere is surprisingly little difference between specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general gradient descent approach.\nStep 1 of this approach is to randomly initialize some parameters. These parameters will be a set of latent factors for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes let’s use 5 for now. Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle. For example, this is what it looks like in Microsoft Excel, with the top-left cell formula displayed as an example:\n\n\n\nimage.png\n\n\nStep 2 of this approach is to calculate our predictions. As we’ve discussed, we can do this by simply taking the dot product of each movie with each user. If, for instance, the first latent user factor represents how much the user likes action movies and the first latent movie factor represents if the movie has a lot of action or not, the product of those will be particularly high if either the user likes action movies and the movie has a lot of action in it or the user doesn’t like action movies and the movie doesn’t have any action in it. On the other hand, if we have a mismatch (a user loves action movies but the movie isn’t an action film, or the user doesn’t like action movies and it is one), the product will be very low.\nStep 3 is to calculate our loss. We can use any loss function that we wish; let’s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction.\nThat’s all we need. With this in place, we can optimize our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better.\nTo use the usual Learner.fit function we will need to get our data into a DataLoaders, so let’s focus on that now.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#creating-the-dataloaders",
    "href": "collaborative-filtering-deep-dive.html#creating-the-dataloaders",
    "title": "Collaborative Filtering",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\nWhen showing the data, we would rather see movie titles than their IDs. The table u.item contains the correspondence of IDs to titles:\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\n\n\n\n\nWe can merge this with our ratings table to get the user ratings by title:\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\ntitle\n\n\n\n\n0\n196\n242\n3\n881250949\nKolya (1996)\n\n\n1\n63\n242\n3\n875747190\nKolya (1996)\n\n\n2\n226\n242\n5\n883888671\nKolya (1996)\n\n\n3\n154\n242\n3\n879138235\nKolya (1996)\n\n\n4\n306\n242\n5\n876503793\nKolya (1996)\n\n\n\n\n\n\n\nWe can then build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the titles instead of the IDs:\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n542\nMy Left Foot (1989)\n4\n\n\n1\n422\nEvent Horizon (1997)\n3\n\n\n2\n311\nAfrican Queen, The (1951)\n4\n\n\n3\n595\nFace/Off (1997)\n4\n\n\n4\n617\nEvil Dead II (1987)\n1\n\n\n5\n158\nJurassic Park (1993)\n5\n\n\n6\n836\nChasing Amy (1997)\n3\n\n\n7\n474\nEmma (1996)\n3\n\n\n8\n466\nJackie Chan's First Strike (1996)\n3\n\n\n9\n554\nScream (1996)\n3\n\n\n\n\n\nTo represent collaborative filtering in PyTorch we can’t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\nn_users  = len(dls.classes['user'])\nprint(f'users : {n_users}')\nn_movies = len(dls.classes['title'])\nprint(f'n_movies : {n_movies}')\nn_factors = 10\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nusers : 944\nn_movies : 1665\n\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:\n\none_hot_3 = one_hot(3, n_users).float()\n\n\nuser_factors.t() @ one_hot_3\n\ntensor([-1.2604, -1.3016, -0.3323, -0.1222,  0.7545,  0.5075, -0.9962,  0.5073,\n        -1.1468, -0.6767])\n\n\nIt gives us the same vector as the one at index 3 in the matrix:\n\nuser_factors[3]\n\ntensor([-1.2604, -1.3016, -0.3323, -0.1222,  0.7545,  0.5075, -0.9962,  0.5073,\n        -1.1468, -0.6767])\n\n\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one—we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding.\n\njargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.\n\nIn computer vision, we have a very easy way to get all the information of a pixel through its RGB values: each pixel in a colored image is represented by three numbers. Those three numbers give us the redness, the greenness and the blueness, which is enough to get our model to work afterward.\nFor the problem at hand, we don’t have the same easy way to characterize a user or a movie. There are probably relations with genres: if a given user likes romance, they are likely to give higher scores to romance movies. Other factors might be whether the movie is more action-oriented versus heavy on dialogue, or the presence of a specific actor that a user might particularly like.\nHow do we determine numbers to characterize those? The answer is, we don’t. We will let our model learn them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not.\nThis is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\nAt the beginning, those numbers don’t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data about the relations between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance, and so on.\nWe are now in a position that we can create our whole model from scratch.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#collaborative-filtering-from-scratch",
    "href": "collaborative-filtering-deep-dive.html#collaborative-filtering-from-scratch",
    "title": "Collaborative Filtering",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nBefore we can write a model in PyTorch, we first need to learn the basics of object-oriented programming and Python. If you haven’t done any object-oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and getting some practice before moving on.\nThe key idea in object-oriented programming is the class. We have been using classes throughout this book, such as DataLoader, string, and Learner. Python also makes it easy for us to create new classes. Here is an example of a simple class:\n\nclass Example:\n    def __init__(self, a): self.a = a\n    def say(self,x): return f'Hello {self.a}, {x}.'\n\nThe most important piece of this is the special method called __init__ (pronounced dunder init). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behavior associated with this method name. In the case of __init__, this is the method Python will call when your new object is created. So, this is where you can set up any state that needs to be initialized upon object creation. Any parameters included when the user constructs an instance of your class will be passed to the __init__ method as parameters. Note that the first parameter to any method defined inside a class is self, so you can use this to set and get any attributes that you will need:\n\nex = Example('Sylvain')\nex.say('nice to meet you')\n\n'Hello Sylvain, nice to meet you.'\n\n\nAlso note that creating a new PyTorch module requires inheriting from Module. Inheritance is an important object-oriented concept that we will not discuss in detail here—in short, it means that we can add additional behavior to an existing class. PyTorch already provides a Module class, which provides some basic foundations that we want to build on. So, we add the name of this superclass after the name of the class that we are defining, as shown in the following example.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\n\n?Embedding\n\n\nInit signature: Embedding(ni, nf, std=0.01)\nDocstring:      Embedding layer with truncated normal initialization\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/mambaforge/envs/cfast/lib/python3.11/site-packages/fastai/layers.py\nType:           type\nSubclasses:     \n\n\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n\nIf you haven’t seen object-oriented programming before, then don’t worry, you won’t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax.\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\nx,y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\nNow that we have defined our architecture, and created our parameter matrices, we need to create a Learner to optimize our model. In the past we have used special functions, such as cnn_learner, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain Learner class:\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nWe are now ready to fit our model:\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.341333\n1.278711\n00:06\n\n\n1\n1.106853\n1.121683\n00:07\n\n\n2\n0.968404\n0.997846\n00:07\n\n\n3\n0.819099\n0.896653\n00:07\n\n\n4\n0.786428\n0.883222\n00:07\n\n\n\n\n\nThe first thing we can do to make this model a little bit better is to force those predictions to be between 0 and 5. For this, we just need to use sigmoid_range, like in &lt;&gt;. One thing we discovered empirically is that it’s better to have the range go a little bit over 5, so we use (0, 5.5):\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.961934\n0.981212\n00:07\n\n\n1\n0.872060\n0.914676\n00:07\n\n\n2\n0.678659\n0.871931\n00:07\n\n\n3\n0.484518\n0.878333\n00:07\n\n\n4\n0.382838\n0.882200\n00:07\n\n\n\n\n\nThis is a reasonable start, but we can do better. One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it.\nThat’s because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. So first of all, let’s adjust our model architecture:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nLet’s try training this and see how it goes:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.931116\n0.944752\n00:08\n\n\n1\n0.847470\n0.865787\n00:08\n\n\n2\n0.604884\n0.859536\n00:08\n\n\n3\n0.428252\n0.881941\n00:09\n\n\n4\n0.294601\n0.888727\n00:09\n\n\n\n\n\nInstead of being better, it ends up being worse (at least at the end of training). Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As we’ve seen, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularization technique. One approach that can be helpful is weight decay.\n\nWeight Decay\nWeight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is:\n\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\n\n\n\n\nSo, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters):\nloss_with_wd = loss + wd * (parameters**2).sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing:\nparameters.grad += wd * 2 * parameters\nIn practice, since wd is a parameter that we choose, we can just make it twice as big, so we don’t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.925284\n0.952555\n00:08\n\n\n1\n0.844705\n0.864576\n00:08\n\n\n2\n0.628166\n0.861570\n00:09\n\n\n3\n0.411920\n0.887054\n00:07\n\n\n4\n0.299360\n0.894271\n00:07\n\n\n\n\n\nMuch better!\n\n\nCreating Our Own Embedding Module\nSo far, we’ve used Embedding without thinking about how it really works. Let’s re-create DotProductBias without using this class. We’ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall from &lt;&gt; that optimizers require that they can get all the parameters of a module from the module’s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n(#0) []\n\n\nTo tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn’t actually add any functionality (other than automatically calling requires_grad_ for us). It’s only used as a “marker” to show what to include in parameters:\n\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nAll PyTorch modules use nn.Parameter for any trainable parameters, which is why we haven’t needed to explicitly use this wrapper up until now:\n\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n\n(#1) [Parameter containing:\ntensor([[ 0.2356],\n        [-0.5954],\n        [-0.3385]], requires_grad=True)]\n\n\n\ntype(t.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\nWe can create a tensor as a parameter, with random initialization, like so:\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\nLet’s use this to create DotProductBias again, but without Embedding:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nThen let’s train it again to check we get around the same results we saw in the previous section:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.923547\n0.947034\n00:08\n\n\n1\n0.867490\n0.879930\n00:09\n\n\n2\n0.727192\n0.838880\n00:09\n\n\n3\n0.591778\n0.828161\n00:09\n\n\n4\n0.482628\n0.827882\n00:08\n\n\n\n\n\nNow, let’s take a look at what our model has learned.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#interpreting-embeddings-and-biases",
    "href": "collaborative-filtering-deep-dive.html#interpreting-embeddings-and-biases",
    "title": "Collaborative Filtering",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nOur model is already useful, in that it can provide us with movie recommendations for our users—but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Robocop 3 (1993)',\n 'Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Mortal Kombat: Annihilation (1997)',\n 'Beautician and the Beast, The (1997)']\n\n\nThink about what this means. What it’s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don’t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias:\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Silence of the Lambs, The (1991)',\n 'L.A. Confidential (1997)',\n \"Schindler's List (1993)\"]\n\n\nSo, for instance, even if you don’t normally enjoy detective movies, you might enjoy LA Confidential!\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. Here’s what our movies look like based on two of the strongest PCA components.\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see here that the model seems to have discovered a concept of classic versus pop culture movies, or perhaps it is critically acclaimed that is represented here.\n\nj: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!\n\nWe defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. We’ll look at how to do that next.\n\nUsing fastai.collab\nWe can create and train a collaborative filtering model using the exact structure shown earlier by using fastai’s collab_learner:\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.937684\n0.953967\n00:09\n\n\n1\n0.840035\n0.876971\n00:09\n\n\n2\n0.718138\n0.833762\n00:09\n\n\n3\n0.596694\n0.819051\n00:08\n\n\n4\n0.479306\n0.819536\n00:08\n\n\n\n\n\nThe names of the layers can be seen by printing the model:\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nWe can use these to replicate any of the analyses we did in the previous section—for instance:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Silence of the Lambs, The (1991)',\n 'Rear Window (1954)']\n\n\nAnother interesting thing we can do with these learned embeddings is to look at distance.\n\n\nEmbedding Distance\nOn a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: \\(\\sqrt{x^{2}+y^{2}}\\) (assuming that x and y are the distances between the coordinates on each axis). For a 50-dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.\nIf there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two movies’ embedding vectors can define that similarity. We can use this to find the most similar movie to Silence of the Lambs:\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Before the Rain (Pred dozhdot) (1994)'\n\n\nNow that we have succesfully trained a model, let’s see how to deal with the situation where we have no data for a user. How can we make recommendations to new users?",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#bootstrapping-a-collaborative-filtering-model",
    "href": "collaborative-filtering-deep-dive.html#bootstrapping-a-collaborative-filtering-model",
    "title": "Collaborative Filtering",
    "section": "Bootstrapping a Collaborative Filtering Model",
    "text": "Bootstrapping a Collaborative Filtering Model\nThe biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\nBut even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of use your common sense. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste.\nBetter still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\nOne thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don’t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of best ever movies lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\nSuch a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\nIn a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It’s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.\nOur dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization (PMF). Another approach, which generally works similarly well given the same data, is deep learning.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#deep-learning-for-collaborative-filtering",
    "href": "collaborative-filtering-deep-dive.html#deep-learning-for-collaborative-filtering",
    "title": "Collaborative Filtering",
    "section": "Deep Learning for Collaborative Filtering",
    "text": "Deep Learning for Collaborative Filtering\nTo turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way.\nSince we’ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nLet’s implement this class:\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\nAnd use it to create a model:\n\nmodel = CollabNN(*embs)\n\nCollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. self.layers is identical to the mini-neural net we created in &lt;&gt; for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply sigmoid_range as we have in previous models.\nLet’s see if it trains:\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.05)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.947911\n0.961177\n00:10\n\n\n1\n0.922033\n0.905163\n00:08\n\n\n2\n0.858137\n0.887984\n00:08\n\n\n3\n0.823720\n0.875852\n00:09\n\n\n4\n0.773479\n0.876700\n00:09\n\n\n\n\n\nfastai provides this model in fastai.collab if you pass use_nn=True in your call to collab_learner (including calling get_emb_sz for you), and it lets you easily create more layers. For instance, here we’re creating two hidden layers, of size 100 and 50, respectively:\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.013264\n1.002862\n00:11\n\n\n1\n0.877074\n0.926017\n00:10\n\n\n2\n0.885206\n0.896862\n00:10\n\n\n3\n0.817585\n0.866366\n00:10\n\n\n4\n0.791544\n0.864616\n00:12\n\n\n\n\n\nlearn.model is an object of type EmbeddingNN. Let’s take a look at fastai’s code for this class:\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)\n\nWow, that’s not a lot of code! This class inherits from TabularModel, which is where it gets all its functionality from. In __init__ it calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received.\n\nSidebar: kwargs and Delegates\nEmbeddingNN includes **kwargs as a parameter to __init__. In Python **kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs. And **kwargs in an argument list means”insert all key/value pairs in the kwargs dict as named arguments here”. This approach is used in many popular libraries, such as matplotlib, in which the main plot function simply has the signature plot(*args, **kwargs). The plot documentation says “The kwargs are Line2D properties” and then lists those properties.\nWe’re using **kwargs in EmbeddingNN to avoid having to write all the arguments to TabularModel a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesn’t know what parameters are available. Consequently things like tab completion of parameter names and pop-up lists of signatures won’t work.\nfastai resolves this by providing a special @delegates decorator, which automatically changes the signature of the class or function (EmbeddingNN in this case) to insert all of its keyword arguments into the signature.\n\n\nEnd sidebar\nAlthough the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That’s exactly what TabularModel does. In fact, we’ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we’d better spend some time learning about TabularModel, and how to use it to get great results! We’ll do that in the next chapter.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#conclusion",
    "href": "collaborative-filtering-deep-dive.html#conclusion",
    "title": "Collaborative Filtering",
    "section": "Conclusion",
    "text": "Conclusion\nFor our first non-computer vision application, we looked at recommendation systems and saw how gradient descent can learn intrinsic factors or biases about items from a history of ratings. Those can then give us information about the data.\nWe also built our first model in PyTorch. We will do a lot more of this in the next section of the book, but first, let’s finish our dive into the other general applications of deep learning, continuing with tabular data.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "collaborative-filtering-deep-dive.html#questionnaire",
    "href": "collaborative-filtering-deep-dive.html#questionnaire",
    "title": "Collaborative Filtering",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat problem does collaborative filtering solve?\nHow does it solve it?\nWhy might a collaborative filtering predictive model fail to be a very useful recommendation system?\nWhat does a crosstab representation of collaborative filtering data look like?\nWrite the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!).\nWhat is a latent factor? Why is it “latent”?\nWhat is a dot product? Calculate a dot product manually using pure Python with lists.\nWhat does pandas.DataFrame.merge do?\nWhat is an embedding matrix?\nWhat is the relationship between an embedding and a matrix of one-hot-encoded vectors?\nWhy do we need Embedding if we could use one-hot-encoded vectors for the same thing?\nWhat does an embedding contain before we start training (assuming we’re not using a pretained model)?\nCreate a class (without peeking, if possible!) and use it.\nWhat does x[:,0] return?\nRewrite the DotProduct class (without peeking, if possible!) and train a model with it.\nWhat is a good loss function to use for MovieLens? Why?\nWhat would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\nWhat is the use of bias in a dot product model?\nWhat is another name for weight decay?\nWrite the equation for weight decay (without peeking!).\nWrite the equation for the gradient of weight decay. Why does it help reduce weights?\nWhy does reducing weights lead to better generalization?\nWhat does argsort do in PyTorch?\nDoes sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\nHow do you print the names and details of the layers in a model?\nWhat is the “bootstrapping problem” in collaborative filtering?\nHow could you deal with the bootstrapping problem for new users? For new movies?\nHow can feedback loops impact collaborative filtering systems?\nWhen using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\nWhy is there an nn.Sequential in the CollabNN model?\nWhat kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?\n\n\nFurther Research\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens. (NB: even the type of brackets used in forward has changed!)\nFind three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas.\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forum for ideas. Note that there are more columns in the full dataset—see if you can use those too (the next chapter might give you ideas).\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter.",
    "crumbs": [
      "Blog",
      "Collaborative Filtering"
    ]
  },
  {
    "objectID": "attention.html",
    "href": "attention.html",
    "title": "Attention",
    "section": "",
    "text": "import math,torch\nfrom torch import nn\nfrom fastAIcourse.activations import *\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom diffusers.models.attention import Attention as AttentionBlock\n\n\nset_seed(42)\nx = torch.randn(64,32,16,16)\n\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\n\ntorch.Size([64, 256, 32])\n\n\n\nni = 32\n\n\nsk = nn.Linear(ni, ni)\nsq = nn.Linear(ni, ni)\nsv = nn.Linear(ni, ni)\n\n\nk = sk(t)\nq = sq(t)\nv = sv(t)\n\n\n(q@k.transpose(1,2)).shape\n\ntorch.Size([64, 256, 256])\n\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, ni):\n        super().__init__()\n        self.scale = math.sqrt(ni)\n        self.norm = nn.GroupNorm(1, ni)\n        self.q = nn.Linear(ni, ni)\n        self.k = nn.Linear(ni, ni)\n        self.v = nn.Linear(ni, ni)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, x):\n        inp = x\n        n,c,h,w = x.shape\n        x = self.norm(x)\n        x = x.view(n, c, -1).transpose(1, 2)\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = self.proj(x)\n        x = x.transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\n\nsa = SelfAttention(32)\n\n\nra = sa(x)\nra.shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nra[0,0,0]\n\ntensor([ 1.91,  1.42,  0.84, -2.16,  0.63, -1.24, -0.08, -1.68, -0.79,  1.61, -0.39, -1.43, -0.75, -0.60, -0.83,  0.75],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ndef cp_parms(a,b):\n    b.weight = a.weight\n    b.bias = a.bias\n\n\nat = AttentionBlock(32, norm_num_groups=1)\nsrc = sa.q,sa.k,sa.v,sa.proj,sa.norm\ndst = at.query,at.key,at.value,at.proj_attn,at.group_norm\nfor s,d in zip(src,dst): cp_parms(s,d)\n\n\nrb = at(x)\nrb[0,0,0]\n\n\nsqkv = nn.Linear(ni, ni*3)\nst = sqkv(t)\nst.shape\n\ntorch.Size([64, 256, 96])\n\n\n\nq,k,v = torch.chunk(st, 3, dim=-1)\nq.shape\n\n\n(k@q.transpose(1,2)).shape\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, ni):\n        super().__init__()\n        self.scale = math.sqrt(ni)\n        self.norm = nn.BatchNorm2d(ni)\n        self.qkv = nn.Linear(ni, ni*3)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, inp):\n        n,c,h,w = inp.shape\n        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, ni):\n        super().__init__()\n        self.scale = math.sqrt(ni)\n        self.norm = nn.BatchNorm2d(ni)\n        self.qkv = nn.Linear(ni, ni*3)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, x):\n        x = self.norm(x).transpose(1, 2)\n        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        return self.proj(x).transpose(1,2)\n\n\nsa = SelfAttention(32)\nsa(x).shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nsa(x).std()\n\ntensor(1.0047, grad_fn=&lt;StdBackward0&gt;)\n\n\n\ndef heads_to_batch(x, heads):\n    n,sl,d = x.shape\n    x = x.reshape(n, sl, heads, -1)\n    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n\ndef batch_to_heads(x, heads):\n    n,sl,d = x.shape\n    x = x.reshape(-1, heads, sl, d)\n    return x.transpose(2, 1).reshape(-1,sl,d*heads)\n\n\nfrom einops import rearrange\n\n\nt2 = rearrange(t , 'n s (h d) -&gt; (n h) s d', h=8)\nt.shape, t2.shape\n\n(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))\n\n\n\nt3 = rearrange(t2, '(n h) s d -&gt; n s (h d)', h=8)\n\n\nt2.shape,t3.shape\n\n(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))\n\n\n\n(t==t3).all()\n\ntensor(True)\n\n\n\nclass SelfAttentionMultiHead(nn.Module):\n    def __init__(self, ni, nheads):\n        super().__init__()\n        self.nheads = nheads\n        self.scale = math.sqrt(ni/nheads)\n        self.norm = nn.BatchNorm2d(ni)\n        self.qkv = nn.Linear(ni, ni*3)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, inp):\n        n,c,h,w = inp.shape\n        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n        x = self.qkv(x)\n        x = rearrange(x, 'n s (h d) -&gt; (n h) s d', h=self.nheads)\n        q,k,v = torch.chunk(x, 3, dim=-1)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = rearrange(x, '(n h) s d -&gt; n s (h d)', h=self.nheads)\n        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\n\nsa = SelfAttentionMultiHead(32, 4)\nsx = sa(x)\nsx.shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nsx.mean(),sx.std()\n\n(tensor(0.0248, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0069, grad_fn=&lt;StdBackward0&gt;))\n\n\n\nnm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\nnmx,nmw = nm(t,t,t)\nnmx = nmx+t\n\n\nnmx.mean(),nmx.std()\n\n(tensor(-0.0021, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0015, grad_fn=&lt;StdBackward0&gt;))\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Attention"
    ]
  },
  {
    "objectID": "autoencoder.html",
    "href": "autoencoder.html",
    "title": "Autoencoders",
    "section": "",
    "text": "x,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n\n\n\n\n inplace.&lt;locals&gt;._f (b)\n\n\nbs = 256\ntds = dsd.with_transform(transformi)\n\n\nds = tds['train']\nimg = ds[0]['image']\nshow_image(img, figsize=(1,1));\n\n\n\n\n\n\n\n\n\ncf = collate_dict(ds)\n\n\nsource\n\n\n\n\n data_loaders (dsd, bs, **kwargs)\n\n\nsource\n\n\n\n\n collate_ (b)\n\n\ndls = data_loaders(tds, bs, collate_fn=collate_)\n\n\ndt = dls['train']\ndv = dls['test']\n\nxb,yb = next(iter(dt))\n\n\nlabels = ds.features[y].names\n\n\nlabels\n\n['T - shirt / top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n\nlbl_getter = itemgetter(*yb[:16])\ntitles = lbl_getter(labels)\n\n\nmpl.rcParams['figure.dpi'] = 70\nshow_images(xb[:16], imsize=1.7, titles=titles)",
    "crumbs": [
      "Blog",
      "Autoencoders"
    ]
  },
  {
    "objectID": "autoencoder.html#data",
    "href": "autoencoder.html#data",
    "title": "Autoencoders",
    "section": "",
    "text": "x,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n\n\n\n\n inplace.&lt;locals&gt;._f (b)\n\n\nbs = 256\ntds = dsd.with_transform(transformi)\n\n\nds = tds['train']\nimg = ds[0]['image']\nshow_image(img, figsize=(1,1));\n\n\n\n\n\n\n\n\n\ncf = collate_dict(ds)\n\n\nsource\n\n\n\n\n data_loaders (dsd, bs, **kwargs)\n\n\nsource\n\n\n\n\n collate_ (b)\n\n\ndls = data_loaders(tds, bs, collate_fn=collate_)\n\n\ndt = dls['train']\ndv = dls['test']\n\nxb,yb = next(iter(dt))\n\n\nlabels = ds.features[y].names\n\n\nlabels\n\n['T - shirt / top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n\nlbl_getter = itemgetter(*yb[:16])\ntitles = lbl_getter(labels)\n\n\nmpl.rcParams['figure.dpi'] = 70\nshow_images(xb[:16], imsize=1.7, titles=titles)",
    "crumbs": [
      "Blog",
      "Autoencoders"
    ]
  },
  {
    "objectID": "autoencoder.html#warmup---classify",
    "href": "autoencoder.html#warmup---classify",
    "title": "Autoencoders",
    "section": "Warmup - classify",
    "text": "Warmup - classify\n\nfrom torch import optim\n\nbs = 256\nlr = 0.4\n\n\ncnn = nn.Sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,16),           #2x2\n    conv(16,10, act=False),\n    nn.Flatten()).to(def_device)\n\n\nopt = optim.SGD(cnn.parameters(), lr=lr)\nloss,acc = fit(5, cnn, F.cross_entropy, opt, dt, dv)\n\n0 0.9032665767669678 0.669\n1 0.5674820697307587 0.7961\n2 0.4913020154953003 0.8247\n3 0.4490947906255722 0.8401\n4 0.42843773872852325 0.8477\n\n\n\ndsd['train'][0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9}",
    "crumbs": [
      "Blog",
      "Autoencoders"
    ]
  },
  {
    "objectID": "autoencoder.html#autoencoder",
    "href": "autoencoder.html#autoencoder",
    "title": "Autoencoders",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nsource\n\ndeconv\n\n deconv (ni, nf, ks=3, act=True)\n\n\nsource\n\n\neval\n\n eval (model, loss_func, valid_dl, epoch=0)\n\n\nsource\n\n\nfit\n\n fit (epochs, model, loss_func, opt, train_dl, valid_dl)\n\n\nae = nn.Sequential(   #28x28\n    nn.ZeroPad2d(2),  #32x32\n    conv(1,2),        #16x16\n#    conv(2,4),        #8x8\n#     conv(4,8),        #4x4\n#     deconv(8,4),      #8x8\n#    deconv(4,2),      #16x16\n    deconv(2,1, act=False), #32x32\n    nn.ZeroPad2d(-2), #28x28\n    nn.Sigmoid()\n).to(def_device)\n\n\neval(ae, F.mse_loss, dv)\n\n0 0.151\n\n\n\nopt = optim.SGD(ae.parameters(), lr=0.01)\nfit(5, ae, F.mse_loss, opt, dt, dv)\n\n0 0.136\n1 0.115\n2 0.087\n3 0.071\n4 0.060\n\n\n\nopt = optim.SGD(ae.parameters(), lr=0.1)\nfit(5, ae, F.mse_loss, opt, dt, dv)\n\n0 0.026\n1 0.020\n2 0.017\n3 0.016\n4 0.014\n\n\n\np = ae(xb)\nshow_images(p[:16].data.cpu(), imsize=1.5)\n\n\n\n\n\n\n\n\n\np = ae(xb)\nshow_images(p[:16].data.cpu(), imsize=1.5)\n\n\n\n\n\n\n\n\n\nshow_images(xb[:16].data.cpu(), imsize=1.5)",
    "crumbs": [
      "Blog",
      "Autoencoders"
    ]
  },
  {
    "objectID": "neuralnet.html",
    "href": "neuralnet.html",
    "title": "Neural Net Foundations",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n!pip list | grep \"fastAIcourse\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.0.4\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Neural Net Foundations"
    ]
  },
  {
    "objectID": "neuralnet.html#initial-checks",
    "href": "neuralnet.html#initial-checks",
    "title": "Neural Net Foundations",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n!pip list | grep \"fastAIcourse\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.0.4\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Neural Net Foundations"
    ]
  },
  {
    "objectID": "neuralnet.html#gather-data",
    "href": "neuralnet.html#gather-data",
    "title": "Neural Net Foundations",
    "section": "Gather Data",
    "text": "Gather Data\n\nfrom fastbook import search_images_ddg\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\nimport os\n\n\nfrom nbdevAuto.functions import *\n\n\ndownload_pic('indian actor close up',\n             name = 'indian', \n             folder = './Data/actors_test', \n             n_images = 10)\n\nDownloading image_path.0\nDownloading image_path.1\nDownloading image_path.2\nDownloading image_path.3\nDownloading image_path.4\nDownloading image_path.5\nDownloading image_path.6\nDownloading image_path.7\nDownloading image_path.8\nDownloading image_path.9\n\n\n\n\n\n\n\n\n\n\nread_from_file??\n\n\nSignature: read_from_file(file_path)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef read_from_file(file_path):\n    from fastbook import search_images_ddg\n    from fastdownload import download_url\n    import os\n    import shutil\n    from PIL import Image\n    countries = ()\n    with open(file_path, 'r') as file:\n        for line in file:\n            # Remove any leading/trailing whitespace and newline characters\n            country = line.strip()\n            # Add the country to the tuple\n            countries += (country,)\n    return countries\nFile:      ~/mambaforge/envs/cfast/lib/python3.11/site-packages/nbdevAuto/functions.py\nType:      function\n\n\n\n\n\nCode\nsearches = ('Indian', 'Chinese', 'American', 'Nigerian', 'Pakistani', 'Japanese')\npath = Path('Data/actors')\n\ncreate_data_folder(folder_path = path,\n                   searches = searches,\n                   before = 'Famous ',\n                   after = ' actor close up',\n                   amount = 400,\n                   recreate = False\n                  )\n\n\ncreated Indian folder\ncreated Chinese folder\ncreated American folder\ncreated Nigerian folder\ncreated Pakistani folder\ncreated Japanese folder\nError with 400 images of Famous Indian actor close up: 'next'\nError with 380 images of Famous Indian actor close up: 'next'\ndownloading 360 images for:Famous Indian actor close up\nError with 400 images of Famous Chinese actor close up: 'next'\nError with 380 images of Famous Chinese actor close up: 'next'\ndownloading 360 images for:Famous Chinese actor close up\nError with 400 images of Famous American actor close up: 'next'\nError with 380 images of Famous American actor close up: 'next'\ndownloading 360 images for:Famous American actor close up\nError with 400 images of Famous Nigerian actor close up: 'next'\ndownloading 380 images for:Famous Nigerian actor close up\nError with 400 images of Famous Pakistani actor close up: 'next'\nError with 380 images of Famous Pakistani actor close up: 'next'\ndownloading 360 images for:Famous Pakistani actor close up\nError with 400 images of Famous Japanese actor close up: 'next'\nError with 380 images of Famous Japanese actor close up: 'next'\ndownloading 360 images for:Famous Japanese actor close up\nNumber of images failed: 34\nresizing images for: Indian\nresizing images for: Chinese\nresizing images for: American\nresizing images for: Nigerian\nresizing images for: Pakistani\nresizing images for: Japanese",
    "crumbs": [
      "Blog",
      "Neural Net Foundations"
    ]
  },
  {
    "objectID": "neuralnet.html#design-model",
    "href": "neuralnet.html#design-model",
    "title": "Neural Net Foundations",
    "section": "Design Model",
    "text": "Design Model\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.3, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(5)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.531562\n1.645555\n0.533011\n00:51\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.552908\n1.435370\n0.476651\n00:08\n\n\n1\n1.172976\n1.425597\n0.462158\n00:07\n\n\n2\n0.828362\n1.448297\n0.420290\n00:07\n\n\n3\n0.590487\n1.448689\n0.425121\n00:07\n\n\n4\n0.427939\n1.446647\n0.425121\n00:07\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(6, nrows=2)",
    "crumbs": [
      "Blog",
      "Neural Net Foundations"
    ]
  },
  {
    "objectID": "neuralnet.html#testing",
    "href": "neuralnet.html#testing",
    "title": "Neural Net Foundations",
    "section": "Testing",
    "text": "Testing\n\nimage= f'./Data/actors_test/indian{8}.jpg'\n\nprint(classify_images(learn, image))\nImage.open(image).to_thumb(256,256)\n\n\n\n\n\n\n\n\n{'American': 0.29763, 'Chinese': 0.07823, 'Indian': 98.55846, 'Japanese': 0.02067, 'Nigerian': 0.00509, 'Pakistani': 1.03991}",
    "crumbs": [
      "Blog",
      "Neural Net Foundations"
    ]
  },
  {
    "objectID": "neuralnet.html#new-models",
    "href": "neuralnet.html#new-models",
    "title": "Neural Net Foundations",
    "section": "New models",
    "text": "New models\n\n!pip list | grep 'timm'\n\ntimm                          0.9.5\n\n\n\nimport timm\n\n\ntimm.list_models('convnext*')[0:5]\n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols']\n\n\n\ntimm.list_models('resnet1*')[0:5]\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet101']\n\n\n\nlearn1 = vision_learner(dls, 'resnet26', metrics=error_rate).to_fp16()\nlearn1.fine_tune(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.312677\n1.824935\n0.495974\n00:07\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.092882\n1.420130\n0.436393\n00:07\n\n\n1\n0.892234\n1.551688\n0.444444\n00:07\n\n\n2\n0.667233\n1.496551\n0.433172\n00:07\n\n\n3\n0.505152\n1.428328\n0.434783\n00:07\n\n\n4\n0.393912\n1.432097\n0.428341\n00:07\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn1)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(classify_images(learn1, image))\nImage.open(image).to_thumb(256,256)\n\n\n\n\n\n\n\n\n{'American': 1.93272, 'Chinese': 0.29763, 'Indian': 94.33807, 'Japanese': 0.74532, 'Nigerian': 0.12889, 'Pakistani': 2.55736}\n\n\n\n\n\n\n\n\n\n\nlearn2 = vision_learner(dls, 'resnetv2_50', metrics=error_rate).to_fp16()\nlearn2.fine_tune(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.409034\n1.555330\n0.563607\n00:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.257049\n1.527638\n0.534622\n00:17\n\n\n1\n1.107014\n1.643291\n0.481481\n00:18\n\n\n2\n0.895890\n1.621696\n0.471820\n00:18\n\n\n3\n0.723650\n1.563223\n0.473430\n00:18\n\n\n4\n0.597290\n1.563260\n0.470209\n00:18\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn2)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(classify_images(learn2, image))\nImage.open(image).to_thumb(256,256)\n\n\n\n\n\n\n\n\n{'American': 0.3549, 'Chinese': 0.33474, 'Indian': 93.41635, 'Japanese': 5.27019, 'Nigerian': 0.00385, 'Pakistani': 0.61998}\n\n\n\n\n\n\n\n\n\n\nlearn3 = vision_learner(dls, 'convnextv2_tiny', metrics=error_rate).to_fp16()\nlearn3.fit_one_cycle(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.345684\n1.306399\n0.450886\n02:10\n\n\n1\n1.648355\n1.032094\n0.365539\n02:14\n\n\n2\n1.210625\n0.989129\n0.338164\n02:16\n\n\n3\n0.911790\n0.994874\n0.341385\n02:13\n\n\n4\n0.740028\n0.988997\n0.334944\n02:26\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn3)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(classify_images(learn3, image))\nImage.open(image).to_thumb(256,256)\n\n\n\n\n\n\n\n\n{'American': 1.42206, 'Chinese': 0.01462, 'Indian': 91.9322, 'Japanese': 1.72204, 'Nigerian': 0.01778, 'Pakistani': 4.8913}\n\n\n\n\n\n\n\n\n\n\nm = learn3.model\nm\n\nSequential(\n  (0): TimmBody(\n    (model): ConvNeXt(\n      (stem): Sequential(\n        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      )\n      (stages): Sequential(\n        (0): ConvNeXtStage(\n          (downsample): Identity()\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (1): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (2): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (3): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (4): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (5): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (6): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (7): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (8): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (3): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): GlobalResponseNormMlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (grn): GlobalResponseNorm()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n      )\n      (norm_pre): Identity()\n      (head): NormMlpClassifierHead(\n        (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n        (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n        (flatten): Flatten(start_dim=1, end_dim=-1)\n        (pre_logits): Identity()\n        (drop): Dropout(p=0.0, inplace=False)\n        (fc): Identity()\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1536, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=6, bias=False)\n  )\n)\n\n\n\nl = m.get_submodule('0.model.stem.1')\nlist(l.parameters())\n\n[Parameter containing:\n tensor([ 4.9317e+00, -1.9534e-03,  1.1269e+00,  3.4428e-01,  2.0585e-01,  4.7114e-01, -1.5931e-02,  1.9189e+00,  3.0262e+00,  1.5578e+00,  5.8386e-01, -2.5743e-03,  3.3899e+00,  1.2300e+00,\n         -7.5090e-03,  2.1205e+00,  1.7195e+00,  7.8101e-01,  2.3211e+00,  3.1620e+00,  1.6729e+00,  1.3192e+00,  3.6899e-01,  2.2085e+00,  1.8041e-01,  1.5685e-01,  1.8197e+00, -7.1151e-03,\n          2.3748e+00,  3.6727e+00,  3.9811e-01,  2.5017e-02,  3.6421e-01,  1.1991e+00,  7.3035e-01,  2.8002e-01,  2.4208e+00,  3.0896e-01,  7.0029e-01,  6.3881e-01,  7.9266e-01,  2.5648e-03,\n          2.7810e-01,  5.4637e-01,  8.0494e-01,  3.3132e-01,  7.9945e-01,  5.9783e-01,  1.9429e-01,  3.4769e-01,  2.2478e+00,  7.3358e-04,  1.1625e+00,  2.1141e-03,  3.2332e+00,  5.9113e-01,\n          2.1750e-01,  4.1986e+00,  2.9994e-01,  8.3881e-01, -5.9712e-03,  8.3365e-03,  6.7004e-01,  2.3832e-01,  1.2561e+00,  2.7700e-01,  6.5003e-01,  2.4996e-01,  4.6959e+00,  6.5632e-01,\n          1.1964e-02,  4.7649e+00,  3.3537e+00,  2.8546e-01,  5.7686e-01,  5.2371e-01,  2.9007e+00,  4.1275e-01,  6.5497e-01,  3.0553e+00, -4.9391e-03,  1.1040e-01,  1.3789e+00,  3.0278e-01,\n          3.1023e+00,  4.8067e-01,  1.2621e+00,  2.9946e-01,  3.0945e+00,  3.5589e-01,  2.8409e+00,  2.4281e-01,  2.1764e-01,  2.9489e+00,  4.3438e-01,  1.6354e-01], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([ 1.2808e-02,  7.2253e-01,  3.1496e-01, -9.9257e-01,  1.2050e-02, -2.4607e+00, -8.3316e-01,  2.0556e+00,  5.7929e-02,  1.3837e-01,  1.0043e-02, -2.6629e-03,  2.5386e-02, -1.6238e+00,\n          2.2846e-01,  5.1050e-02,  7.4879e-02, -4.8713e-02, -7.2320e-03,  3.4651e-02, -4.3118e-03,  6.7141e-03,  2.5983e-03, -1.2359e-01,  5.1444e-02,  4.4971e-02, -9.4435e-02,  1.2114e+00,\n          2.5141e-01,  4.6139e-02, -4.3074e-03,  3.2234e-01,  7.9720e-03, -6.1009e-02,  1.9836e-02,  2.4125e-02,  5.0120e-01,  7.6677e-03, -1.9238e-02, -1.4486e-04,  3.5432e-02,  2.8747e-01,\n          1.8336e-02, -6.7369e-01, -8.2248e-02,  4.9743e-02,  1.6106e-02,  2.0235e+00,  2.7785e-02, -1.1059e-02,  7.6895e-02,  5.9566e-02,  8.3243e-02, -3.7374e-03,  4.1498e-02,  1.0022e-01,\n          7.8927e-03, -1.2302e-02,  2.3708e-02, -7.7988e-03,  3.3583e-01, -5.2203e-01,  3.7889e-02, -1.4866e-02,  2.2963e+00, -3.8432e-02,  1.1283e-01, -1.3709e-02, -3.0943e-03,  9.6218e-03,\n         -1.4228e-02,  4.5164e-02,  3.0798e-02,  3.9817e-03,  9.9890e-02,  1.1873e+00, -1.4091e-01, -1.5202e+00,  1.4482e-01,  7.1060e-02, -5.8411e-02,  2.2041e-02, -7.0340e-02,  1.4650e-03,\n          6.1044e-02, -2.2964e-01,  2.8174e-01, -2.2585e-02, -6.0580e-02, -2.3969e-02, -5.6086e-02, -2.3145e-02,  2.1335e-02, -2.9009e-02, -3.2377e-02,  1.1139e-02], device='cuda:0',\n        requires_grad=True)]",
    "crumbs": [
      "Blog",
      "Neural Net Foundations"
    ]
  },
  {
    "objectID": "minibatch_training.html",
    "href": "minibatch_training.html",
    "title": "Mini Batch",
    "section": "",
    "text": "Exported source\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor,nn\nimport torch.nn.functional as F\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\npath_data = Path('Data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])",
    "crumbs": [
      "Blog",
      "Mini Batch"
    ]
  },
  {
    "objectID": "minibatch_training.html#initial-setup",
    "href": "minibatch_training.html#initial-setup",
    "title": "Mini Batch",
    "section": "Initial setup",
    "text": "Initial setup\n\nData\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 20\n\n\nsource\n\n\nModel\n\n Model (n_in, nh, n_out)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\n\nmodel = Model(m, nh, 10)\npred = model(x_train)\npred.shape\n\ntorch.Size([50000, 10])\n\n\n\n\nCross entropy loss\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}\\]\nor more concisely:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}\\]\nIn practice, we will need the log of the softmax when we calculate the loss.\n\nsource\n\n\nlog_softmax\n\n log_softmax (x)\n\n\n\nExported source\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\n\n\nlog_softmax(pred)\n\ntensor([[-1.92, -2.34, -2.38,  ..., -2.53, -2.64, -2.51],\n        [-2.02, -2.34, -2.29,  ..., -2.56, -2.60, -2.45],\n        [-1.90, -2.38, -2.31,  ..., -2.39, -2.59, -2.48],\n        ...,\n        [-1.97, -2.29, -2.28,  ..., -2.59, -2.64, -2.57],\n        [-1.94, -2.35, -2.20,  ..., -2.56, -2.72, -2.58],\n        [-2.04, -2.28, -2.26,  ..., -2.60, -2.65, -2.61]], grad_fn=&lt;LogBackward0&gt;)\n\n\nNote that the formula\n\\[\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)\\]\ngives a simplification when we compute the log softmax\n\\[\\log \\left ( ab \\right ) = \\log(a) + \\log(b)\\]\n\nsource\n\n\nlog_softmax\n\n log_softmax (x)\n\n\n\nExported source\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\n\n\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\\]\nwhere a is the maximum of the \\(x_{j}\\).\n\nsource\n\n\nlogsumexp\n\n logsumexp (x)\n\n\n\nExported source\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\n\nThis way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.\n\nsource\n\n\nlog_softmax\n\n log_softmax (x)\n\n\n\nExported source\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\n\ntest_close(logsumexp(pred), pred.logsumexp(-1))\nsm_pred = log_softmax(pred)\nsm_pred\n\ntensor([[-1.92, -2.34, -2.38,  ..., -2.53, -2.64, -2.51],\n        [-2.02, -2.34, -2.29,  ..., -2.56, -2.60, -2.45],\n        [-1.90, -2.38, -2.31,  ..., -2.39, -2.59, -2.48],\n        ...,\n        [-1.97, -2.29, -2.28,  ..., -2.59, -2.64, -2.57],\n        [-1.94, -2.35, -2.20,  ..., -2.56, -2.72, -2.58],\n        [-2.04, -2.28, -2.26,  ..., -2.60, -2.65, -2.61]], grad_fn=&lt;SubBackward0&gt;)\n\n\nThe cross entropy loss for some target \\(x\\) and some prediction \\(p(x)\\) is given by:\n\\[ -\\sum x\\, \\log p(x) \\]\nBut since our \\(x\\)s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as \\(-\\log(p_{i})\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\ny_train[:3]\n\ntensor([5, 0, 4])\n\n\n\nsm_pred[0,5],sm_pred[1,0],sm_pred[2,4]\n\n(tensor(-2.26, grad_fn=&lt;SelectBackward0&gt;),\n tensor(-2.02, grad_fn=&lt;SelectBackward0&gt;),\n tensor(-2.37, grad_fn=&lt;SelectBackward0&gt;))\n\n\n\nsm_pred[[0,1,2], y_train[:3]]\n\ntensor([-2.26, -2.02, -2.37], grad_fn=&lt;IndexBackward0&gt;)\n\n\n\nsource\n\n\nnll\n\n nll (input, target)\n\n\n\nExported source\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\n\n\nloss = nll(sm_pred, y_train)\nloss\n\ntensor(2.33, grad_fn=&lt;NegBackward0&gt;)\n\n\nThen use PyTorch’s implementation.\n\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n\nIn PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.\n\ntest_close(F.cross_entropy(pred, y_train), loss, 1e-3)",
    "crumbs": [
      "Blog",
      "Mini Batch"
    ]
  },
  {
    "objectID": "minibatch_training.html#basic-training-loop",
    "href": "minibatch_training.html#basic-training-loop",
    "title": "Mini Batch",
    "section": "Basic training loop",
    "text": "Basic training loop\nBasically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better\n\nloss_func = F.cross_entropy\n\n\nbs=50                  # batch size\n\nxb = x_train[0:bs]     # a mini-batch from x\npreds = model(xb)      # predictions\npreds[0], preds.shape\n\n(tensor([ 0.37, -0.06, -0.10,  0.13,  0.04,  0.02,  0.05, -0.25, -0.35, -0.22], grad_fn=&lt;SelectBackward0&gt;),\n torch.Size([50, 10]))\n\n\n\nyb = y_train[0:bs]\nyb\n\ntensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n        3, 9, 8, 5, 9, 3])\n\n\n\nloss_func(preds, yb)\n\ntensor(2.33, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\npreds[:5]\n\ntensor([[ 0.37, -0.06, -0.10,  0.13,  0.04,  0.02,  0.05, -0.25, -0.35, -0.22],\n        [ 0.23, -0.08, -0.03,  0.16,  0.00, -0.06,  0.01, -0.31, -0.35, -0.20],\n        [ 0.37, -0.10, -0.03,  0.10, -0.09,  0.02, -0.05, -0.12, -0.32, -0.21],\n        [ 0.26,  0.05,  0.08,  0.27,  0.06,  0.02,  0.13, -0.31, -0.36, -0.29],\n        [ 0.28, -0.09,  0.07,  0.23,  0.12,  0.08,  0.03, -0.36, -0.35, -0.23]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\npreds.argmax(dim=1)\n\ntensor([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n        0, 0, 0, 0, 0, 0])\n\n\n\nsource\n\naccuracy\n\n accuracy (out, yb)\n\n\n\nExported source\ndef accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n\n\n\naccuracy(preds, yb)\n\ntensor(0.08)\n\n\n\nlr = 0.5   # learning rate\nepochs = 10 # how many epochs to train for\n\n\nsource\n\n\nreport\n\n report (loss, preds, yb)\n\n\n\nExported source\ndef report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')\n\n\n\nxb,yb = x_train[:bs],y_train[:bs]\npreds = model(xb)\nreport(loss_func(preds, yb), preds, yb)\n\n2.33, 0.08\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad   * lr\n                    l.weight.grad.zero_()\n                    l.bias  .grad.zero_()\n    report(loss, preds, yb)\n\n0.12, 0.96\n0.15, 0.92\n0.11, 0.96\n0.12, 0.94\n0.10, 0.96\n0.08, 0.98\n0.09, 0.98\n0.08, 0.96\n0.07, 0.96\n0.08, 0.98\n\n\n$L = l(n(w,x)) N=n(w,x) and L= l(N) $\n$ = $",
    "crumbs": [
      "Blog",
      "Mini Batch"
    ]
  },
  {
    "objectID": "minibatch_training.html#dataset-and-dataloader",
    "href": "minibatch_training.html#dataset-and-dataloader",
    "title": "Mini Batch",
    "section": "Dataset and DataLoader",
    "text": "Dataset and DataLoader\n\nDataset\nIt’s clunky to iterate through minibatches of x and y values separately:\n    xb = x_train[s]\n    yb = y_train[s]\nInstead, let’s do these two steps together, by introducing a Dataset class:\n    xb,yb = train_ds[s]\n\nsource\n\n\nDataset\n\n Dataset (x, y)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Dataset():\n    def __init__(self, x, y): self.x,self.y = x,y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i): return self.x[i],self.y[i]\n\n\n\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nassert len(train_ds)==len(x_train)\nassert len(valid_ds)==len(x_valid)\n\n\nxb,yb = train_ds[0:5]\nassert xb.shape==(5,28*28)\nassert yb.shape==(5,)\nxb,yb\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 4, 1, 9]))\n\n\n\nmodel,opt = get_model()\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        xb,yb = train_ds[i:min(n,i+bs)]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.29, 0.90\n0.26, 0.90\n0.25, 0.90\n0.21, 0.92\n0.22, 0.92\n0.20, 0.94\n0.24, 0.88\n0.24, 0.88\n0.25, 0.90\n0.22, 0.90\n\n\n\n\nDataLoader\nPreviously, our loop iterated over batches (xb, yb) like this:\nfor i in range(0, n, bs):\n    xb,yb = train_ds[i:min(n,i+bs)]\n    ...\nLet’s make our loop much cleaner, using a data loader:\nfor xb,yb in train_dl:\n    ...\n\nsource\n\n\nDataLoader\n\n DataLoader (ds, bs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DataLoader():\n    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n    def __iter__(self):\n        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n\n\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\n\n\nxb,yb = next(iter(valid_dl))\nxb.shape\n\ntorch.Size([50, 784])\n\n\n\nyb\n\ntensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n        8, 3, 7, 7, 8, 4])\n\n\n\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\n\n\n\n\nmodel,opt = get_model()\n\n\nsource\n\n\nfit\n\n fit ()\n\n\n\nExported source\ndef fit():\n    for epoch in range(epochs):\n        for xb,yb in train_dl:\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        report(loss, preds, yb)\n\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.32, 0.90\n0.25, 0.90\n0.22, 0.90\n0.21, 0.92\n0.17, 0.94\n0.15, 0.94\n0.13, 0.96\n0.13, 0.94\n0.12, 0.96\n0.12, 0.98\n\n\n(tensor(0.15, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.96))\n\n\n\n\nRandom sampling\nWe want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized.\n\nsource\n\n\nSampler\n\n Sampler (ds, shuffle=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nimport random\n\n\n\n\nExported source\nclass Sampler():\n    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n    def __iter__(self):\n        res = list(range(self.n))\n        if self.shuffle: random.shuffle(res)\n        return iter(res)\n\n\n\n\nExported source\nfrom itertools import islice\n\n\n\nss = Sampler(train_ds)\n\n\nit = iter(ss)\nfor o in range(5): print(next(it))\n\n0\n1\n2\n3\n4\n\n\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nss = Sampler(train_ds, shuffle=True)\nlist(islice(ss, 5))\n\n[38864, 48491, 457, 38117, 46961]\n\n\n\nsource\n\n\nBatchSampler\n\n BatchSampler (sampler, bs, drop_last=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nimport fastcore.all as fc\n\n\n\n\nExported source\nclass BatchSampler():\n    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n\n\n\nbatchs = BatchSampler(ss, 4)\nlist(islice(batchs, 5))\n\n[[48663, 21900, 25004, 10521],\n [2935, 15296, 32118, 49052],\n [9673, 12394, 26887, 23110],\n [11959, 11965, 44887, 23025],\n [7462, 42558, 2649, 48055]]\n\n\n\nsource\n\n\ncollate\n\n collate (b)\n\n\n\nExported source\ndef collate(b):\n    xs,ys = zip(*b)\n    return torch.stack(xs),torch.stack(ys)\n\n\n\nsource\n\n\nDataLoader\n\n DataLoader (ds, batchs, collate_fn=&lt;function collate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DataLoader():\n    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)\n\n\n\ntrain_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\nvalid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp)\nvalid_dl = DataLoader(valid_ds, batchs=valid_samp)\n\n\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\n\n\n\n\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\nmodel,opt = get_model()\n\n\nfit()\n\n0.48, 0.86\n0.14, 0.96\n0.18, 0.94\n0.11, 0.96\n0.13, 0.98\n0.12, 0.94\n0.08, 0.98\n0.16, 0.96\n0.28, 0.94\n0.10, 0.96\n\n\n\n\nMultiprocessing DataLoader\n\n\nExported source\nimport torch.multiprocessing as mp\nfrom fastcore.basics import store_attr\n\n\n\ntrain_ds[[3,6,8,1]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\n\ntrain_ds.__getitem__([3,6,8,1])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\n\nfor o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n\n\n\nsource\n\n\nDataLoader\n\n DataLoader (ds, batchs, n_workers=1, collate_fn=&lt;function collate&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DataLoader():\n    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n    def __iter__(self):\n        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))\n\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\nit = iter(train_dl)\n\n\nxb,yb = next(it)\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\n\nPyTorch DataLoader\n\n\nExported source\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n\n\n\ntrain_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\nvalid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)\n\n\ntrain_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)\n\n\nmodel,opt = get_model()\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.21, 0.92\n0.33, 0.88\n0.21, 0.96\n0.31, 0.92\n0.10, 0.98\n0.09, 0.96\n0.21, 0.94\n0.15, 0.98\n0.23, 0.94\n0.03, 1.00\n\n\n(tensor(0.07, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.96))\n\n\nPyTorch can auto-generate the BatchSampler for us:\n\ntrain_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)\n\nPyTorch can also generate the Sequential/RandomSamplers too:\n\ntrain_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\nvalid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)\n\n\nmodel,opt = get_model()\nfit()\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.20, 0.92\n0.27, 0.96\n0.25, 0.92\n0.02, 1.00\n0.07, 1.00\n0.25, 0.92\n0.35, 0.94\n0.19, 0.90\n0.02, 1.00\n0.20, 0.94\n\n\n(tensor(0.06, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.98))\n\n\nOur dataset actually already knows how to sample a batch of indices all at once:\n\ntrain_ds[[4,6,7]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([9, 1, 3]))\n\n\n…that means that we can actually skip the batch_sampler and collate_fn entirely:\n\ntrain_dl = DataLoader(train_ds, sampler=train_samp)\nvalid_dl = DataLoader(valid_ds, sampler=valid_samp)\n\n\nxb,yb = next(iter(train_dl))\nxb.shape,yb.shape\n\n(torch.Size([1, 50, 784]), torch.Size([1, 50]))",
    "crumbs": [
      "Blog",
      "Mini Batch"
    ]
  },
  {
    "objectID": "minibatch_training.html#validation",
    "href": "minibatch_training.html#validation",
    "title": "Mini Batch",
    "section": "Validation",
    "text": "Validation\nYou always should also have a validation set, in order to identify if you are overfitting.\nWe will calculate and print the validation loss at the end of each epoch.\n(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)\n\nsource\n\nfit\n\n fit (epochs, model, loss_func, opt, train_dl, valid_dl)\n\n\n\nExported source\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc,count = 0.,0.,0\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred,yb).item()*n\n                tot_acc  += accuracy (pred,yb).item()*n\n        print(epoch, tot_loss/count, tot_acc/count)\n    return tot_loss/count, tot_acc/count\n\n\n\nsource\n\n\nget_dls\n\n get_dls (train_ds, valid_ds, bs, **kwargs)\n\n\n\nExported source\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n\n\nNow, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:\n\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\nmodel,opt = get_model()\n\n\n\n\n0 0.20109995853155851 0.9422000014781952\n1 0.1778342595230788 0.9489000052213669\n2 0.19040836207568645 0.9405000030994415\n3 0.16341516491025687 0.9513000059127807\n4 0.1552597831375897 0.956200003027916\n5 0.15336507064756005 0.9582000058889389\n6 0.17772509098984302 0.9491000014543534\n7 0.15932741371914744 0.9568000066280365\n8 0.16316431357990951 0.9562000054121017\n9 0.1839916431158781 0.9480000025033951\nCPU times: user 27.6 s, sys: 40.1 s, total: 1min 7s\nWall time: 11.3 s",
    "crumbs": [
      "Blog",
      "Mini Batch"
    ]
  },
  {
    "objectID": "ddim.html",
    "href": "ddim.html",
    "title": "Denoising Diffusion Implicit Models - DDIM",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,random,logging\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom functools import partial\n\nfrom fastcore.foundation import L\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.fid import *\nfrom fastprogress.fastprogress import progress_bar\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nmpl.rcParams['image.cmap'] = 'gray_r'\nlogging.disable(logging.WARNING)\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\nfrom diffusers import UNet2DModel, DDIMPipeline, DDPMPipeline, DDIMScheduler, DDPMScheduler",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Implicit Models - DDIM"
    ]
  },
  {
    "objectID": "ddim.html#diffusers-ddpm-scheduler",
    "href": "ddim.html#diffusers-ddpm-scheduler",
    "title": "Denoising Diffusion Implicit Models - DDIM",
    "section": "Diffusers DDPM Scheduler",
    "text": "Diffusers DDPM Scheduler\n\nclass UNet(UNet2DModel): pass\n\n\nmodel = torch.load('models/fashion_ddpm3_25.pkl').cuda()\n# model = torch.load('models/fashion_no-t.pkl').cuda()\n\n\nsched = DDPMScheduler(beta_end=0.01)\n\n\nx_t = torch.randn((4,1,32,32)).cuda()\n\n\nt = 99\nt_batch = torch.full((len(x_t),), t, device=x_t.device, dtype=torch.long)\nwith torch.no_grad(): noise = model(x_t, t_batch).sample\n\n\nres = sched.step(noise, t, x_t)\n\n\nres.prev_sample.shape\n\ntorch.Size([4, 1, 32, 32])\n\n\n\nsz = (512,1,32,32)\n\n\nx_t = torch.randn(sz).cuda()\npreds = []\n\nfor t in progress_bar(sched.timesteps):\n    with torch.no_grad(): noise = model(x_t, t).sample\n    x_t = sched.step(noise, t, x_t).prev_sample\n    preds.append(x_t.float().cpu())\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 03:39&lt;00:00]\n    \n    \n\n\nCPU times: user 3min 34s, sys: 5.14 s, total: 3min 40s\nWall time: 3min 39s\n\n\n\ns = preds[-1].clamp(-0.5,0.5)*2\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\nbs = 2048\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\nie = ImageEval(cmodel, dls, cbs=[DeviceCB()])\n\n\nie.fid(s),ie.kid(s)\n\n(30.25244140625, 0.07350349426269531)\n\n\n\nie.fid(xb),ie.kid(xb)\n\n(1.296875, -0.00011661575990729034)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Implicit Models - DDIM"
    ]
  },
  {
    "objectID": "ddim.html#diffusers-ddim-scheduler",
    "href": "ddim.html#diffusers-ddim-scheduler",
    "title": "Denoising Diffusion Implicit Models - DDIM",
    "section": "Diffusers DDIM Scheduler",
    "text": "Diffusers DDIM Scheduler\n\nsched = DDIMScheduler(beta_end=0.01)\nsched.set_timesteps(333)\n\n\ndef diff_sample(model, sz, sched, **kwargs):\n    x_t = torch.randn(sz).cuda()\n    preds = []\n    for t in progress_bar(sched.timesteps):\n        with torch.no_grad(): noise = model(x_t, t).sample\n        x_t = sched.step(noise, t, x_t, **kwargs).prev_sample\n        preds.append(x_t.float().cpu())\n    return preds\n\n\npreds = diff_sample(model, sz, sched, eta=1.)\ns = (preds[-1]*2).clamp(-1,1)\n\n\n\n\n\n\n    \n      \n      100.00% [333/333 01:11&lt;00:00]\n    \n    \n\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nie.fid(s),ie.kid(s)\n\n(36.6961669921875, 0.1079036071896553)\n\n\n\nsched.set_timesteps(200)\npreds = diff_sample(model, sz, sched, eta=1.)\ns = (preds[-1]*2).clamp(-1,1)\nie.fid(s),ie.kid(s)\n\n\n\n\n\n\n    \n      \n      100.00% [200/200 00:42&lt;00:00]\n    \n    \n\n\n(33.8856201171875, 0.16268639266490936)\n\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nsched.set_timesteps(100)\npreds = diff_sample(model, sz, sched, eta=1.)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:21&lt;00:00]\n    \n    \n\n\n\ns = (preds[-1]*2).clamp(-1,1)\n\n\nie.fid(s),ie.kid(s)\n\n(35.12646484375, 0.10706407576799393)\n\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nsched.set_timesteps(50)\npreds = diff_sample(model, sz, sched, eta=1.)\ns = (preds[-1]*2).clamp(-1,1)\nie.fid(s),ie.kid(s)\n\n\n\n\n\n\n    \n      \n      100.00% [50/50 00:11&lt;00:00]\n    \n    \n\n\n(43.124267578125, 0.21125255525112152)\n\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nsched.set_timesteps(25)\npreds = diff_sample(model, sz, sched, eta=1.)\ns = (preds[-1]*2).clamp(-1,1)\nie.fid(s),ie.kid(s)\n\n\n\n\n\n\n    \n      \n      100.00% [25/25 00:05&lt;00:00]\n    \n    \n\n\n(51.3458251953125, 0.21316410601139069)\n\n\n\nshow_images(s[:25], imsize=1.5)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Implicit Models - DDIM"
    ]
  },
  {
    "objectID": "ddim.html#implementing-ddim",
    "href": "ddim.html#implementing-ddim",
    "title": "Denoising Diffusion Implicit Models - DDIM",
    "section": "Implementing DDIM",
    "text": "Implementing DDIM\n\nfrom types import SimpleNamespace\n\n\nn_steps=1000\n\n\ndef linear_sched(betamin=0.0001,betamax=0.02,n_steps=1000):\n    beta = torch.linspace(betamin, betamax, n_steps)\n    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())\n\n\nsc = linear_sched(betamax=0.01, n_steps=n_steps)\nabar = sc.abar\n\n\n\n\nimage.png\n\n\n\ndef ddim_step(x_t, t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta):\n    vari = ((bbar_t1/bbar_t) * (1-abar_t/abar_t1))\n    sig = vari.sqrt()*eta\n    x_0_hat = ((x_t-bbar_t.sqrt()*noise) / abar_t.sqrt())\n    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n    if t&gt;0: x_t += sig * torch.randn(x_t.shape).to(x_t)\n    return x_t\n\n\n@torch.no_grad()\ndef sample(f, model, sz, n_steps, skips=1, eta=1.):\n    tsteps = list(reversed(range(0, n_steps, skips)))\n    x_t = torch.randn(sz).to(model.device)\n    preds = []\n    for i,t in enumerate(progress_bar(tsteps)):\n        abar_t1 = abar[tsteps[i+1]] if t &gt; 0 else torch.tensor(1)\n        noise = model(x_t,t).sample\n        x_t = f(x_t, t, noise, abar[t], abar_t1, 1-abar[t], 1-abar_t1, eta)\n        preds.append(x_t.float().cpu())\n    return preds\n\n\nsamples = sample(ddim_step, model, sz, 1000, 10)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:22&lt;00:00]\n    \n    \n\n\nCPU times: user 21.7 s, sys: 450 ms, total: 22.2 s\nWall time: 22.2 s\n\n\n\ns = (samples[-1]*2)#.clamp(-1,1)\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nie.fid(s),ie.kid(s)\n\n(34.0198974609375, 0.11981192231178284)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Implicit Models - DDIM"
    ]
  },
  {
    "objectID": "ddim.html#triangular-noise",
    "href": "ddim.html#triangular-noise",
    "title": "Denoising Diffusion Implicit Models - DDIM",
    "section": "Triangular noise",
    "text": "Triangular noise\n\ndef noisify(x0, ᾱ):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    t = np.random.triangular(0, 0.5, 1, (n,))*n_steps\n    t = tensor(t, dtype=torch.long)\n    ε = torch.randn(x0.shape, device=device)\n    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n    return (xt, t.to(device)), ε\n\n\n(xt,t),ε = noisify(xb,abar)\nplt.hist(t);",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Implicit Models - DDIM"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Getting Started",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!nvcc --version\n\n/bin/bash: line 1: nvcc: command not found\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\n\n\n\nimport torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available.\")\n    num_gpu = torch.cuda.device_count()\n    for i in range(num_gpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n        print(f\"GPU {i}: {gpu_name}, Memory: {gpu_memory:.2f} GB\")\nelse:\n    print(\"GPU is not available.\")\n\nGPU is available.\nGPU 0: NVIDIA GeForce RTX 2060, Memory: 6.00 GB\n\n\n\n# Check if CUDA (GPU) is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#check-for-all-installs-and-versions",
    "href": "intro.html#check-for-all-installs-and-versions",
    "title": "Getting Started",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!nvcc --version\n\n/bin/bash: line 1: nvcc: command not found\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\n\n\n\nimport torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available.\")\n    num_gpu = torch.cuda.device_count()\n    for i in range(num_gpu):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # Convert bytes to GB\n        print(f\"GPU {i}: {gpu_name}, Memory: {gpu_memory:.2f} GB\")\nelse:\n    print(\"GPU is not available.\")\n\nGPU is available.\nGPU 0: NVIDIA GeForce RTX 2060, Memory: 6.00 GB\n\n\n\n# Check if CUDA (GPU) is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#generate-data-images",
    "href": "intro.html#generate-data-images",
    "title": "Getting Started",
    "section": "Generate Data Images",
    "text": "Generate Data Images\n\n\nCode\nfrom fastbook import search_images_ddg\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\nfrom nbdevAuto import functions\nimport os\nimport shutil\n\n\nLet’s start by searching for a bird photo and seeing what kind of result we get. We’ll start by getting URLs from a search:\n\nfunctions.download_pic('bird', n_images = 1, folder = './Data', show_progress=True, recreate = False)\n\nImage file exists.\n\n\n\n\n\n\n\n\n\n…and then download a URL and take a look at it:\nNow let’s do the same with “forest photos”:\n\nfunctions.download_pic('forest', folder = './Data')\n\nImage file exists.\n\n\n\n\n\n\n\n\n\n\nsearches = ('forest','bird')\npath = Path('Data/bird_or_not')\n\nfunctions.create_data_folder(folder_path=path,\n                             searches=searches,\n                             amount=200,\n                            recreate= False)\n\nFolder already exists: Data/bird_or_not",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#creating-the-model",
    "href": "intro.html#creating-the-model",
    "title": "Getting Started",
    "section": "Creating the model",
    "text": "Creating the model\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')],\n    \n)\n\ndls = dls.dataloaders(path, bs = 16)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nHere what each of the DataBlock parameters means:\nblocks=(ImageBlock, CategoryBlock),\nThe inputs to our model are images, and the outputs are categories (in this case, “bird” or “forest”).\nget_items=get_image_files, \nTo find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path).\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nSplit the data into training and validation sets randomly, using 20% of the data for the validation set.\nget_y=parent_label,\nThe labels (y values) is the name of the parent of each file (i.e. the name of the folder they’re in, which will be bird or forest).\nitem_tfms=[Resize(192, method='squish')]\nBefore training, resize each image to 192x192 pixels by “squishing” it (as opposed to cropping it).",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#traning-the-model",
    "href": "intro.html#traning-the-model",
    "title": "Getting Started",
    "section": "Traning the model",
    "text": "Traning the model\nNow we’re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 10 seconds…)\nfastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so we’ll use that.1\n\nimport timm\n\n\ntimm.list_models('resnet1*')[0:5]\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet101']\n\n\n\nlearn = vision_learner(dls, 'resnet18', metrics=error_rate)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.657275\n0.040000\n0.014085\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.046995\n0.029015\n0.014085\n00:02\n\n\n1\n0.095600\n0.020682\n0.014085\n00:01\n\n\n2\n0.145258\n0.025311\n0.000000\n00:01\n\n\n3\n0.101540\n0.023066\n0.000000\n00:01\n\n\n4\n0.095942\n0.017011\n0.000000\n00:01",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#step-3-use-our-model-and-build-your-own",
    "href": "intro.html#step-3-use-our-model-and-build-your-own",
    "title": "Getting Started",
    "section": "Step 3: Use our model (and build your own!)",
    "text": "Step 3: Use our model (and build your own!)\nLet’s see what our model thinks about that bird we downloaded at the start:\n\n??PILImage\n\n\nInit signature: PILImage()\nSource:        \nclass PILImage(PILBase): \n    \"A RGB Pillow `Image` that can show itself and converts to `TensorImage`\"\n    pass\nFile:           ~/mambaforge/envs/cfast/lib/python3.11/site-packages/fastai/vision/core.py\nType:           BypassNewMeta\nSubclasses:     PILImageBW\n\n\n\n\nis_bird,_,probs = learn.predict(PILImage.create('Data/bird0.jpg'))\nprint(f\"This is a: {is_bird}.\")\nprint(f\"Probability it's a bird: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: bird.\nProbability it's a bird: 0.9932\n\n\n\nfunctions.classify_images(learn, 'Data/bird0.jpg')\n\n\n\n\n\n\n\n\n{'bird': 99.31602, 'forest': 0.68398}",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "intro.html#deep-learning-is-not-just-for-image-classification",
    "href": "intro.html#deep-learning-is-not-just-for-image-classification",
    "title": "Getting Started",
    "section": "Deep Learning Is Not Just for Image Classification",
    "text": "Deep Learning Is Not Just for Image Classification\n\nSegmentationDataLoaders - Easier than datablocks\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\nTabular analysis - income prediction\n\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(3)\n\ndls.show_batch()\n\nCollaboration filtering - ratings/ recommendations\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(10)\nlearn.show_results()\n\nExample\n\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=32)\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\nThis reduces the batch size to 32 (we will explain this later). If you keep hitting the same error, change 32 to 16\n\nExample\n\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy)\nlearn.fine_tune(4, 1e-2)",
    "crumbs": [
      "Blog",
      "Getting Started"
    ]
  },
  {
    "objectID": "karras.html",
    "href": "karras.html",
    "title": "Karras pre-conditioning",
    "section": "",
    "text": "import timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastprogress import progress_bar\nfrom diffusers import UNet2DModel, DDIMPipeline, DDPMPipeline, DDIMScheduler, DDPMScheduler\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nn_steps = 1000\nbs = 512\ndsd = load_dataset(name)\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs)\n\ndl = dls.train\nxb,yb = b = next(iter(dl))\n# sig_data = xb.std()\nsig_data = 0.66\ny is clean signal, n is N(0,1) noise.\ndef scalings(sig):\n    totvar = sig**2+sig_data**2\n    # c_skip,c_out,c_in\n    return sig_data**2/totvar,sig*sig_data/totvar.sqrt(),1/totvar.sqrt()\nsig_samp = (torch.randn([10000])*1.2-1.2).exp()\nplt.hist(sig_samp);\nimport seaborn as sns\nsns.kdeplot(sig_samp, clip=(0,10));\ndef noisify(x0):\n    device = x0.device\n    sig = (torch.randn([len(x0)])*1.2-1.2).exp().to(x0).reshape(-1,1,1,1)\n    noise = torch.randn_like(x0, device=device)\n    c_skip,c_out,c_in = scalings(sig)\n    noised_input = x0 + noise*sig\n    target = (x0-c_skip*noised_input)/c_out\n    return (noised_input*c_in,sig.squeeze()),target\ndef collate_ddpm(b): return noisify(default_collate(b)[xl])\ndef dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=8)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\ndl = dls.train\n(noised_input,sig),target = b = next(iter(dl))\nshow_images(noised_input[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))\nshow_images(target[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))\nnoised_input.mean(),noised_input.std(),target.mean(),target.std()\n\n(tensor(-0.69019), tensor(1.01665), tensor(-0.40007), tensor(1.03293))",
    "crumbs": [
      "Blog",
      "Karras pre-conditioning"
    ]
  },
  {
    "objectID": "karras.html#train",
    "href": "karras.html#train",
    "title": "Karras pre-conditioning",
    "section": "Train",
    "text": "Train\n\nclass UNet(UNet2DModel):\n    def forward(self, x): return super().forward(*x).sample\n\n\ndef init_ddpm(model):\n    for o in model.down_blocks:\n        for p in o.resnets:\n            p.conv2.weight.data.zero_()\n            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n\n    for o in model.up_blocks:\n        for p in o.resnets: p.conv2.weight.data.zero_()\n\n    model.conv_out.weight.data.zero_()\n\n\nlr = 1e-2\nepochs = 25\nopt_func = partial(optim.Adam, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.703\n0\ntrain\n\n\n0.348\n0\neval\n\n\n0.263\n1\ntrain\n\n\n0.224\n1\neval\n\n\n0.201\n2\ntrain\n\n\n0.197\n2\neval\n\n\n0.188\n3\ntrain\n\n\n0.186\n3\neval\n\n\n0.179\n4\ntrain\n\n\n0.179\n4\neval\n\n\n0.172\n5\ntrain\n\n\n0.174\n5\neval\n\n\n0.169\n6\ntrain\n\n\n0.165\n6\neval\n\n\n0.160\n7\ntrain\n\n\n0.167\n7\neval\n\n\n0.158\n8\ntrain\n\n\n0.161\n8\neval\n\n\n0.154\n9\ntrain\n\n\n0.164\n9\neval\n\n\n0.152\n10\ntrain\n\n\n0.151\n10\neval\n\n\n0.151\n11\ntrain\n\n\n0.153\n11\neval\n\n\n0.147\n12\ntrain\n\n\n0.150\n12\neval\n\n\n0.147\n13\ntrain\n\n\n0.150\n13\neval\n\n\n0.145\n14\ntrain\n\n\n0.146\n14\neval\n\n\n0.143\n15\ntrain\n\n\n0.144\n15\neval\n\n\n0.142\n16\ntrain\n\n\n0.144\n16\neval\n\n\n0.141\n17\ntrain\n\n\n0.141\n17\neval\n\n\n0.140\n18\ntrain\n\n\n0.141\n18\neval\n\n\n0.139\n19\ntrain\n\n\n0.140\n19\neval\n\n\n0.139\n20\ntrain\n\n\n0.140\n20\neval\n\n\n0.138\n21\ntrain\n\n\n0.138\n21\neval\n\n\n0.137\n22\ntrain\n\n\n0.137\n22\neval\n\n\n0.137\n23\ntrain\n\n\n0.139\n23\neval\n\n\n0.137\n24\ntrain\n\n\n0.137\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n# torch.save(learn.model, 'models/fashion_karras.pkl')\n# model = learn.model = torch.load('models/fashion_karras.pkl').cuda()\n\n\ndef denoise(target, noised_input): return target*c_out + noised_input*c_skip\n\n\nwith torch.no_grad():\n    sigr = sig.cuda().reshape(-1,1,1,1)\n    c_skip,c_out,c_in = scalings(sigr)\n    targ_pred = learn.model((noised_input.cuda(),sig.cuda()))\n    x0_pred = denoise(targ_pred, noised_input.cuda()/c_in)\n\n\nshow_images(noised_input[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))\n\n\n\n\n\n\n\n\n\nshow_images(x0_pred[:25].clamp(-1,1), imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))\n\n\n\n\n\n\n\n\n\nshow_images(denoise(target.cuda(), noised_input.cuda()/c_in)[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))\n\n\n\n\n\n\n\n\n\nsig_r = tensor(80.).cuda().reshape(-1,1,1,1)\nc_skip,c_out,c_in = scalings(sig_r)\nx_r = torch.randn(32,1,32,32).to(model.device)*sig_r\nwith torch.no_grad():\n    targ_pred = learn.model((x_r*c_in,sig_r.squeeze()))\n    x0_pred = denoise(targ_pred, x_r)\nshow_images(x0_pred[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nx0_pred.max(),x0_pred.min(),x0_pred.mean(),x0_pred.std()\n\n(tensor(0.63882, device='cuda:0'),\n tensor(-1.18548, device='cuda:0'),\n tensor(-0.54164, device='cuda:0'),\n tensor(0.43493, device='cuda:0'))",
    "crumbs": [
      "Blog",
      "Karras pre-conditioning"
    ]
  },
  {
    "objectID": "karras.html#sampling",
    "href": "karras.html#sampling",
    "title": "Karras pre-conditioning",
    "section": "Sampling",
    "text": "Sampling\n\nfrom miniai.fid import ImageEval\n\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\nbs = 2048\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\nie = ImageEval(cmodel, dls, cbs=[DeviceCB()])\n\n\nsz = (512,1,32,32)\n\n\nsz = (2048,1,32,32)\n\n\ndef sigmas_karras(n, sigma_min=0.01, sigma_max=80., rho=7., device='cpu'):\n    ramp = torch.linspace(0, 1, n)\n    min_inv_rho = sigma_min**(1/rho)\n    max_inv_rho = sigma_max**(1/rho)\n    sigmas = (max_inv_rho + ramp * (min_inv_rho-max_inv_rho))**rho\n    return torch.cat([sigmas, tensor([0.])]).to(device)\n\n\nsk = sigmas_karras(100)\nplt.plot(sk);\n\n\n\n\n\n\n\n\n\ndef denoise(model, x, sig):\n    c_skip,c_out,c_in = scalings(sig)\n    return model((x*c_in, sig))*c_out + x*c_skip\n\n\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    if not eta: return sigma_to, 0.\n    var_to,var_from = sigma_to**2,sigma_from**2\n    sigma_up = min(sigma_to, eta * (var_to * (var_from-var_to)/var_from)**0.5)\n    return (var_to-sigma_up**2)**0.5, sigma_up\n\n\n@torch.no_grad()\ndef sample_euler_ancestral(x, sigs, i, model, eta=1.):\n    sig,sig2 = sigs[i],sigs[i+1]\n    denoised = denoise(model, x, sig)\n    sigma_down,sigma_up = get_ancestral_step(sig, sig2, eta=eta)\n    x = x + (x-denoised)/sig*(sigma_down-sig)\n    return x + torch.randn_like(x)*sigma_up\n\n\n@torch.no_grad()\ndef sample_euler(x, sigs, i, model):\n    sig,sig2 = sigs[i],sigs[i+1]\n    denoised = denoise(model, x, sig)\n    return x + (x-denoised)/sig*(sig2-sig)\n\n\n@torch.no_grad()\ndef sample_heun(x, sigs, i, model, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    sig,sig2 = sigs[i],sigs[i+1]\n    n = len(sigs)\n    gamma = min(s_churn/(n-1), 2**0.5-1) if s_tmin&lt;=sig&lt;=s_tmax else 0.\n    eps = torch.randn_like(x) * s_noise\n    sigma_hat = sig * (gamma+1)\n    if gamma &gt; 0: x = x + eps * (sigma_hat**2-sig**2)**0.5\n    denoised = denoise(model, x, sig)\n    d = (x-denoised)/sig\n    dt = sig2-sigma_hat\n    x_2 = x + d*dt\n    if sig2==0: return x_2\n    denoised_2 = denoise(model, x_2, sig2)\n    d_2 = (x_2-denoised_2)/sig2\n    d_prime = (d+d_2)/2\n    return x + d_prime*dt\n\n\ndef sample(sampler, model, steps=100, sigma_max=80., **kwargs):\n    preds = []\n    x = torch.randn(sz).to(model.device)*sigma_max\n    sigs = sigmas_karras(steps, device=model.device, sigma_max=sigma_max)\n    for i in progress_bar(range(len(sigs)-1)):\n        x = sampler(x, sigs, i, model, **kwargs)\n        preds.append(x)\n    return preds\n\n\n# preds = sample_lms(model, steps=20, order=3)\n# preds = sample(sample_euler_ancestral, model, steps=100, eta=0.5)\npreds = sample(sample_euler, model, steps=100)\n# preds = sample(sample_heun, model, steps=20, s_churn=0.5)\n\n\n\n\n\n\n    \n      \n      100.00% [20/20 00:09&lt;00:00]\n    \n    \n\n\n\ns = preds[-1]\ns.min(),s.max()\n\n(tensor(-1.08955, device='cuda:0'), tensor(1.46819, device='cuda:0'))\n\n\n\nshow_images(s[:25].clamp(-1,1), imsize=1.5)\n\n\n\n\n\n\n\n\n\n# euler 100\nie.fid(s),ie.kid(s),s.shape\n\n(5.231043207481207, 0.0031656520441174507, torch.Size([2048, 1, 32, 32]))\n\n\n\n# euler 100\nie.fid(s),ie.kid(s),s.shape\n\n(5.406003616592329, 0.015411057509481907, torch.Size([2048, 1, 32, 32]))\n\n\n\n# ancestral 100 0.5\nie.fid(s),ie.kid(s),s.shape\n\n(5.452807558586642, 0.0071729626506567, torch.Size([2048, 1, 32, 32]))\n\n\n\n# heun 50\nie.fid(s),ie.kid(s),s.shape\n\n(6.221842673288506, 0.023713070899248123, torch.Size([2048, 1, 32, 32]))\n\n\n\n# heun 20\nie.fid(s),ie.kid(s),s.shape\n\n(5.610681075267394, -0.005569742992520332, torch.Size([2048, 1, 32, 32]))\n\n\n\n# heun 20, churn 0.5\nie.fid(s),ie.kid(s),s.shape\n\n(5.2517917311790825, 0.026914160698652267, torch.Size([2048, 1, 32, 32]))\n\n\n\n# lms 20\nie.fid(s),ie.kid(s),s.shape\n\n(5.061003587997561, 0.019381564110517502, torch.Size([2048, 1, 32, 32]))\n\n\n\n# reals\nie.fid(xb)\n\n2.5736580178430586\n\n\n\nfrom scipy import integrate\n\n\ndef linear_multistep_coeff(order, t, i, j):\n    if order-1 &gt; i: raise ValueError(f'Order {order} too high for step {i}')\n    def fn(tau):\n        prod = 1.\n        for k in range(order):\n            if j == k: continue\n            prod *= (tau-t[i-k]) / (t[i-j]-t[i-k])\n        return prod\n    return integrate.quad(fn, t[i], t[i+1], epsrel=1e-4)[0]\n\n\n@torch.no_grad()\ndef sample_lms(model, steps=100, order=4, sigma_max=80.):\n    preds = []\n    x = torch.randn(sz).to(model.device)*sigma_max\n    sigs = sigmas_karras(steps, device=model.device, sigma_max=sigma_max)\n    ds = []\n    for i in progress_bar(range(len(sigs)-1)):\n        sig = sigs[i]\n        denoised = denoise(model, x, sig)\n        d = (x-denoised)/sig\n        ds.append(d)\n        if len(ds) &gt; order: ds.pop(0)\n        cur_order = min(i+1, order)\n        coeffs = [linear_multistep_coeff(cur_order, sigs, i, j) for j in range(cur_order)]\n        x = x + sum(coeff*d for coeff, d in zip(coeffs, reversed(ds)))\n        preds.append(x)\n    return preds",
    "crumbs": [
      "Blog",
      "Karras pre-conditioning"
    ]
  },
  {
    "objectID": "matrix.html",
    "href": "matrix.html",
    "title": "Matrix and tensor",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#initial-checks",
    "href": "matrix.html#initial-checks",
    "title": "Matrix and tensor",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#matrix-multiplication-from-foundations",
    "href": "matrix.html#matrix-multiplication-from-foundations",
    "title": "Matrix and tensor",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#get-data",
    "href": "matrix.html#get-data",
    "title": "Matrix and tensor",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('Data')\npath_data.mkdir(exist_ok = True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\npath_gz\n\nPosixPath('Data/mnist.pkl.gz')\n\n\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding = 'latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz):\n        yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\n\n\n\n\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n&lt;itertools.islice&gt;\n\n\n\nnext(it)\n\n0.0\n\n\n\nisit = islice(it,5)\n\n\nnext(isit)\n\n0.0\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.19140625, 0.9296875, 0.98828125, 0.98828125]\n\n\n\nit= iter(lst1)\nimg = list(iter(lambda: list(islice(it,28)), []))\n\n\nplt.imshow(img);",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#matrix-and-tensor",
    "href": "matrix.html#matrix-and-tensor",
    "title": "Matrix and tensor",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): \n        self.xs = xs\n        \n    def __getitem__(self, idxs): \n        return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20, 15]\n\n0.98828125\n\n\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\ntens = tensor(img)\ntens[20,15]\n\ntensor(0.9883)\n\n\n\nx_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_valid.shape\n\ntorch.Size([10000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\n\nimgs = x_train.reshape((-1, 28, 28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0])\n\n\n\n\n\n\n\n\n\nimgs[0, 20, 15]\n\ntensor(0.9883)\n\n\n\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\n\nmin(y_train), max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#random-numbers",
    "href": "matrix.html#random-numbers",
    "title": "Matrix and tensor",
    "section": "Random Numbers",
    "text": "Random Numbers\n\nBased on the Wichmann Hill Algorithm used before Python 2.3\n\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.0657])\nIn child: tensor([0.0657])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n\n\n\n\n\n\n\n2.54 ms ± 20.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n39.9 µs ± 4.94 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#matrix-multiplication",
    "href": "matrix.html#matrix-multiplication",
    "title": "Matrix and tensor",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar, ac = m1.shape\nbr, bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):\n    for j in range(bc):\n        for k in range(ac):\n            t1[i,j] += m1[i,k] * m2 [k, j]\n\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\ntorch.set_printoptions(precision = 2, linewidth = 140, sci_mode = False)\n\n\nimport numpy as np\nnp.set_printoptions(precision = 2, linewidth = 140)\n\n\ndef matmul(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    (ar,ac),(br,bc)\n    \n    t1 = torch.zeros(ar, bc)\n    t1.shape\n    \n    for i in range(ar):  #5\n        for j in range(bc):  #10\n            for k in range(ac): #784\n                t1[i,j] += m1[i,k] * m2 [k, j]\n    return t1\n\n\n\n\n653 ms ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nar * bc * ac\n\n39200",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#numba",
    "href": "matrix.html#numba",
    "title": "Matrix and tensor",
    "section": "Numba",
    "text": "Numba\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n\n\nCPU times: user 404 ms, sys: 79.7 ms, total: 484 ms\nWall time: 757 ms\n\n\n20.0\n\n\n\n\n\n1.24 µs ± 111 ns per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape, b.shape\n    \n    t1 = torch.zeros(ar, bc)\n    t1.shape\n    \n    for i in range(ar):  #5\n        for j in range(bc): \n            t1[i,j] = dot(a[i,:], b[:, j])\n            #for k in range(ac): #784\n            #    t1[i,j] += m1[i,k] * m2 [k, j]\n\n    return t1\n\n\nm1a, m2a = m1.numpy(), m2.numpy()\n\n\nfrom fastcore.test import *\n\n\ntest_close(t1, matmul(m1a, m2a))\n\n\n\n\n381 µs ± 6.52 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\na = [10 ,6 ,4]\nb = [2, 8, 7]\na1 = np.array(a)\nb1 = np.array(b)\na1 + b1\n\narray([12, 14, 11])\n\n\n\na2 = tensor(a)\nb2 = tensor(b)\n\na2 + b2\n\ntensor([12, 14, 11])\n\n\n\n(a2 &lt; b2).float().mean()\n\ntensor(0.67)\n\n\n\nm = tensor([[1,2,3],[4,5,6],[7,8,9]])\nm\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\\[\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}\\]\n\nsf = (m*m).sum()\nsf\n\ntensor(285)\n\n\n\nsf.sqrt()\n\ntensor(16.88)\n\n\n\nm[2, :], m[:,2]\n\n(tensor([7, 8, 9]), tensor([3, 6, 9]))\n\n\n\nm[2]\n\ntensor([7, 8, 9])\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape, b.shape\n    t1 = torch.zeros(ar, bc)\n    \n    for i in range(ar):  #5\n        for j in range(bc): \n            t1[i,j] = (a[i,:] * b[:, j]).sum()\n    return t1\n\n\ntest_close(t1, matmul(m1, m2))\n\n\n\n\n1.02 ms ± 16.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape, b.shape\n    t1 = torch.zeros(ar, bc)\n    \n    for i in range(ar):  #5\n        for j in range(bc): \n            t1[i,j] = torch.dot(a[i,:], b[:, j])\n    return t1\n\n\ntest_close(t1, matmul(m1, m2))\n\n\n\n\n793 µs ± 8.07 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#broadcasting-with-a-scalar",
    "href": "matrix.html#broadcasting-with-a-scalar",
    "title": "Matrix and tensor",
    "section": "Broadcasting with a scalar",
    "text": "Broadcasting with a scalar\n\na2\n\ntensor([10,  6,  4])\n\n\n\na2 &gt; 0\n\ntensor([True, True, True])\n\n\n\na2 + 1\n\ntensor([11,  7,  5])\n\n\n\nm\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\na2\n\ntensor([10,  6,  4])",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#broadcasting-a-vector-to-a-matrix",
    "href": "matrix.html#broadcasting-a-vector-to-a-matrix",
    "title": "Matrix and tensor",
    "section": "Broadcasting a vector to a matrix",
    "text": "Broadcasting a vector to a matrix\n\nc = tensor([10.,20, 30]);c\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\nm.shape, c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nc+ m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt  = c.expand_as(m)\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nt.untyped_storage()\n\n 0\n 0\n 32\n 65\n 0\n 0\n 160\n 65\n 0\n 0\n 240\n 65\n[torch.storage.UntypedStorage(device=cpu) of size 12]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\n\nc.unsqueeze(0), c[None, :]\n\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))\n\n\n\nm[:,:, None]\n\ntensor([[[1],\n         [2],\n         [3]],\n\n        [[4],\n         [5],\n         [6]],\n\n        [[7],\n         [8],\n         [9]]])\n\n\n\nc.shape, c.unsqueeze(0).shape\n\n(torch.Size([3]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(1), c[: ,None]\n\n(tensor([[10.],\n         [20.],\n         [30.]]),\n tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([3, 1]))\n\n\n\nc[None].shape, c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nc[: ,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nm + c[:, None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\n\nm + c[None, :]\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nc[None, :] * c[:, None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[None, :] &gt; c[:, None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\nc\n\ntensor([10., 20., 30.])\n\n\n\nm * m\n\ntensor([[ 1,  4,  9],\n        [16, 25, 36],\n        [49, 64, 81]])",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#matmul-with-broadcasting",
    "href": "matrix.html#matmul-with-broadcasting",
    "title": "Matrix and tensor",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\n\ndigit = m1[0]\ndigit.shape, m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]* m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape, b.shape\n    t1 = torch.zeros(ar, bc)\n    \n    for i in range(ar):  #5\n        for j in range(bc):  #10\n            # t1[i,j] = (a[i,:] * b[:, j]).sum()\n            # t1[i,j] = torch.dot(a[i,:], b[:, j])\n            t1[i] = (a[i, :, None] * b).sum(dim = 0)\n    return t1\n\n\ntest_close(t1, matmul(m1, m2))\n\n\n\n\n1.34 ms ± 27.4 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nresult=matmul(m1, m2)\nresult\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntr = matmul(x_train, weights)\ntr.shape,tr\n\n(torch.Size([50000, 10]),\n tensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n         [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n         [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n         ...,\n         [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n         [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n         [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]]))\n\n\n\n\n\nCPU times: user 14 s, sys: 8.28 ms, total: 14 s\nWall time: 13.1 s\n\n\n\n\n\n26.3 µs ± 6.27 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\n\n\n50.7 ms ± 5.95 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\n\n\nThe slowest run took 6.67 times longer than the fastest. This could mean that an intermediate result is being cached.\n8.14 µs ± 8.83 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\n\n\n13.3 ms ± 1.75 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#einstein-summation",
    "href": "matrix.html#einstein-summation",
    "title": "Matrix and tensor",
    "section": "Einstein Summation",
    "text": "Einstein Summation\n\neinsum is a compact representation for combining products and sums in a general way.\n\n\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nmr = torch.einsum('ik,kj-&gt;ikj',m1,m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nmr = torch.einsum('ik,kj-&gt;ij',m1,m2)\n\n\ndef matmul(a, b): return torch.einsum('ik,kj-&gt;ij',a,b)\n\n\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n\n\n\n\n11.3 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#pytorch-op",
    "href": "matrix.html#pytorch-op",
    "title": "Matrix and tensor",
    "section": "pytorch op",
    "text": "pytorch op\n\ntest_close(tr, x_train@weights, eps=1e-3)\n\n\n\n\n12.5 ms ± 2.99 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#cuda",
    "href": "matrix.html#cuda",
    "title": "Matrix and tensor",
    "section": "CUDA",
    "text": "CUDA\n\n!conda list | grep cudatoolkit\n\ncudatoolkit               11.8.0              h4ba93d1_12    conda-forge\n\n\n\ndef matmul(grid, a, b, c):\n    i, j = grid\n    if i &lt; c.shape[0] and j &lt; c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]):\n            tmp += a[i, k] * b[k, j]\n            c[i, j] = tmp\n\n\nres = torch.zeros(ar,bc)\nmatmul((0,0), m1, m2, res)\nres\n\ntensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])\n\n\n\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): \n            kernel((i, j), *args, **kwargs)\n\n\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nfrom numba import cuda\n\n\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i &lt; c.shape[0] and j &lt; c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]):\n            tmp += a[i, k] * b[k, j]\n            c[i, j] = tmp\n\n\nr = np.zeros(tr.shape)\n#m1g, m2g, rg = cuda.to_device(x_train), cuda.to_device(weights), cuda.to_device(r)\n\n\nm1g, m2g, rg = map(cuda.to_device, (x_train, weights, r))\n\n\nm1g, m1g.shape\n\n(&lt;numba.cuda.cudadrv.devicearray.DeviceNDArray&gt;,\n (50000, 784))\n\n\n\nTPB = 16\nrr, rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(3125, 1)\n\n\n\nmatmul[blockspergrid, (TPB, TPB)](m1g, m2g, rg)\n\n\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n\n\nmatmul[blockspergrid, (TPB, TPB)](m1g, m2g, rg)\nr = rg.copy_to_host()\n\n\nm1c, m2c = x_train.cuda(), weights.cuda()\nm1gpu, m2gpu = m1.cuda(), m2.cuda()\n\ncpu() copys from GPU to CPU\n\nr = (m1c @ m2c).cpu()\n\n\n\n\nThe slowest run took 7.32 times longer than the fastest. This could mean that an intermediate result is being cached.\n126 µs ± 113 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\n\n\n1.24 ms ± 28.4 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nr, r.shape\n\n(tensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n         [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n         [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n         ...,\n         [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n         [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n         [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]]),\n torch.Size([50000, 10]))\n\n\n\nimport gc\nimport torch\ntorch.cuda.empty_cache()\ngc.collect()\n\n90\n\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\") # clearing cache \nlibc.malloc_trim(0)\n\n1\n\n\n\nfor name in dir():\n    if not name.startswith('_'):\n        del globals()[name]",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#clustering",
    "href": "matrix.html#clustering",
    "title": "Matrix and tensor",
    "section": "Clustering",
    "text": "Clustering\n\nClustering techniques are unsupervised learning algorithms that try to group unlabelled data into “clusters”, using the (typically spatial) structure of the data itself.\n\n\nimport math, matplotlib.pyplot as plt, operator, torch\nfrom functools import partial\n\n\ntorch.manual_seed(42)\ntorch.set_printoptions(precision = 3, linewidth = 140, sci_mode = False)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#create-data",
    "href": "matrix.html#create-data",
    "title": "Matrix and tensor",
    "section": "Create data",
    "text": "Create data\n\nn_clusters= 6\nn_samples = 250\ncentroids = torch.rand(n_clusters, 2) * 70 - 35\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\ndef sample(m):\n    return MultivariateNormal(m,\n                              torch.diag(tensor([5.,5.]))).sample((n_samples,))\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\n\ndef plot_data(centroids, data, n_samples, ax = None):\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i * n_samples:(i + 1) * n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s = 1)\n        ax.plot(*centroid, markersize=10, marker = \"x\", color='k', mew = 5)\n        ax.plot(*centroid, markersize=5, marker= \"x\", color = 'm', mew = 2)\n\n\nplot_data(centroids, data, n_samples)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#mean-shift",
    "href": "matrix.html#mean-shift",
    "title": "Matrix and tensor",
    "section": "Mean shift",
    "text": "Mean shift\n\nmidp = data.mean(0)\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\n\n\n\n\n\ndef gaussian(d, bw):\n    return torch.exp(-0.5*((d/bw))**2) / (bw * math.sqrt(2*math.pi))\n\n\ndef plot_func(f):\n    x = torch.linspace(0, 10, 100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\n\n\n\n\n\nf = partial(gaussian, bw=2.5)\n\n\nf(tensor(4.))\n\ntensor(0.044)\n\n\n\ndef tri(d, i): return (-d + i).clamp_min(0)/i\n\n\nplot_func(partial(tri, i = 8))\n\n\n\n\n\n\n\n\n\nX = data.clone()\nx = data[0]\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape, X.shape, x[None].shape\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\n(x- X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\ndist = torch.einsum('ik-&gt;i',((x - X) ** 2))\ndist.sqrt()\n\ntensor([ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617])\n\n\n\ndist = ((x - X) ** 2).sum(1).sqrt()\ndist\n\ntensor([ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617])\n\n\n\nweight = gaussian(dist, 2.5)\nweight[:8]\n\ntensor([0.160, 0.047, 0.025, 0.053, 0.007, 0.041, 0.005, 0.009])\n\n\n\nweight.shape,weight[:,None].shape, X.shape\n\n(torch.Size([1500]), torch.Size([1500, 1]), torch.Size([1500, 2]))\n\n\n\nweight[:,None] * X\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.einsum('ik-&gt;i',((x - X) ** 2))\n        weight = gaussian(dist, 2.5)\n\n        X[i] = (weight[:, None] * X).sum(0)/weight.sum()\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n\n\nCPU times: user 867 ms, sys: 144 µs, total: 867 ms\nWall time: 866 ms\n\n\n\nplot_data(centroids+2, X, n_samples)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#animation",
    "href": "matrix.html#animation",
    "title": "Matrix and tensor",
    "section": "Animation",
    "text": "Animation\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax = ax)\n\n\nX = data.clone()\nfig, ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames = 5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "matrix.html#gpu-batched-algorithm",
    "href": "matrix.html#gpu-batched-algorithm",
    "title": "Matrix and tensor",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\n\nbs = 5\nX = data.clone()\nx = X[:bs]\nx.shape, X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a, b):\n    return (((a[None] -b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X,x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X,x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None, :] - x[:, None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\ngaussian??\n\n\nSignature: gaussian(d, bw)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef gaussian(d, bw):\n    return torch.exp(-0.5*((d/bw))**2) / (bw * math.sqrt(2*math.pi))\nFile:      /tmp/ipykernel_1054164/117635507.py\nType:      function\n\n\n\n\nweight = gaussian(dist_b(X,x),2)\n\n\nweight.shape, X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[..., None].shape, x[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 5, 2]))\n\n\n\nnum = (weight[...,None] * X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ntorch.einsum('ij,jk -&gt;ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim = True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\nbs = 5\n\n\ndef meanshift(data, bs = 500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs, n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n            div = weight.sum(1, keepdim = True)\n            X[s] = weight@X/div\n\n    return X\n\n\ndata = data.cuda()\n\n\nX = meanshift(data,bs).cpu()\n\n\n\n\n17.7 ms ± 1.79 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\n\n\n3.91 ms ± 11.2 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\n\n\n3.68 ms ± 8.66 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids+2, X, n_samples)",
    "crumbs": [
      "Blog",
      "Matrix and tensor"
    ]
  },
  {
    "objectID": "how-does-a-neural-net-really-work.html",
    "href": "how-does-a-neural-net-really-work.html",
    "title": "NeuralNet Deep dive",
    "section": "",
    "text": "A neural network is just a mathematical function. In the most standard kind of neural network, the function:\nThis represents one “layer”. Then these three steps are repeated, using the outputs of the previous layer as the inputs to the next layer. Initially, the parameters in this function are selected randomly. Therefore a newly created neural network doesn’t do anything useful at all – it’s just random!\nTo get the function to “learn” to do something useful, we have to change the parameters to make them “better” in some way. We do this using gradient descent. Let’s see how this works…\n!pip list | grep ipywidgets\n\nipywidgets                    8.0.4\nimport ipywidgets as widgets\nbtn_upload = widgets.FileUpload(\n    button_style='success',\n    description='Upload',\n    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n    multiple=False  # True to accept multiple files upload else False\n)\ndisplay(btn_upload)\nExported source\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    import numpy as np, matplotlib.pyplot as plt\n    from fastai.basics import torch\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.rc('figure', dpi=90)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\nfrom ipywidgets import interact\nimport numpy as np, matplotlib.pyplot as plt\nfrom fastai.basics import *\nTo learn how gradient descent works, we’re going to start by fitting a quadratic, since that’s a function most of us are probably more familiar with than a neural network. Here’s the quadratic we’re going to try to fit:\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, \"$3x^2 + 2x + 1$\")\nThis quadratic is of the form \\(ax^2+bx+c\\), with parameters \\(a=3\\), \\(b=2\\), \\(c=1\\). To make it easier to try out different quadratics for fitting a model to the data we’ll create, let’s create a function that calculates the value of a point on any quadratic:\ndef quad(a, b, c, x): return a*x**2 + b*x + c\nquad(3,2,1, 1.5)\n\n10.75\nIf we fix some particular values of a, b, and c, then we’ll have made a quadratic. To fix values passed to a function in python, we use the partial function, like so:\nfrom functools import partial\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\nSo for instance, we can recreate our previous quadratic:\nf2 = mk_quad(3,2,1)\nplot_function(f2)\nNow let’s simulate making some noisy measurements of our quadratic f. We’ll then use gradient descent to see if we can recreate the original function from the data.\nHere’s a couple of functions to add some random noise to data:\nfrom numpy.random import normal, seed, uniform\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\nLet’s use the now to create our noisy measurements based on the quadratic above:\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 0.5)\nHere’s the first few values of each of x and y:\nx[:5],y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[10.4034],\n         [ 6.7691],\n         [ 5.8721],\n         [ 4.0551],\n         [ 2.3391]], dtype=torch.float64))\nAs you can see, they’re tensors. A tensor is just like an array in numpy (if you’re not familiar with numpy, I strongly recommend reading this great book, because it’s a critical foundation for nearly all numeric programming in Python. Furthermore, PyTorch, which most researchers use for deep learning, is modeled closely on numpy.) A tensor can be a single number (a scalar or rank-0 tensor), a list of numbers (a vector or rank-1 tensor), a table of numbers (a matrix or rank-2 tensor), a table of tables of numbers (a rank-3 tensor), and so forth.\nWe’re not going to learn much about our data by just looking at the raw numbers, so let’s draw a picture:\nplt.scatter(x,y);\nHow do we find values of a, b, and c which fit this data? One approach is to try a few values and see what fits. Here’s a function which overlays a quadratic on top of our data, along with some sliders to change a, b, and c, and see how it looks:\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a,b,c), ylim=(-3,13))\nReminder: If the sliders above aren’t working for you, that’s because the interactive features of this notebook don’t work in Kaggle’s Reader mode. They only work in Edit mode. Please click “Copy & Edit” in the top right of this window, then in the menu click Run and then Run all. Then you’ll be able to use all the interactive sliders in this notebook.\nTry moving slider a a bit to the left. Does that look better or worse? How about if you move it a bit to the right? Find out which direction seems to improve the fit of the quadratic to the data, and move the slider a bit in that direction. Next, do the same for slider b: first figure out which direction improves the fit, then move it a bit in that direction. Then do the same for c.\nOK, now go back to slider a and repeat the process. Do it again for b and c as well.\nDid you notice that by going back and doing the sliders a second time that you were able to improve things a bit further? That’s an important insight – it’s only after changing b and c, for instance, that you realise that a actually needs some adjustment based on those new values.\nOne thing that’s making this tricky is that we don’t really have a great sense of whether our fit is really better or worse. It would be easier if we had a numeric measure of that. On easy metric we could use is mean absolute error – which is the distance from each data point to the curve:\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\nWe’ll update our interactive function to print this at the top for us.\nUse this to repeat the approach we took before to try to find the best fit, but this time just use the value of the metric to decide which direction to move each slider, and how far to move it.\nThis time around, try doing it in the opposite order: c, then b, then a.\nYou’ll probably find that you have to go through the set of sliders a couple of times to get the best fit.\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\nIn a modern neural network we’ll often have tens of millions of parameters to fit, or more, and thousands or millions of data points to fit them to. We’re not going to be able to do that by moving sliders around! We’ll need to automate this process.\nThankfully, that turns out to be pretty straightforward. We can use calculus to figure out, for each parameter, whether we should increase or decrease it.\nUh oh, calculus! If you haven’t touched calculus since school, you might be getting ready to run away at this point. But don’t worry, we don’t actually need much calculus at all. Just derivatives, which measure the rate of change of a function. We don’t even need to calculate them ourselves, because the computer will do it for us! If you’ve forgotten what a derivitive is, then watch the first three of these fantastic videos by Professor Dave. It’s only 15 minutes in total, so give it a go! Then come back here and we’ll continue on our journey…",
    "crumbs": [
      "Blog",
      "NeuralNet Deep dive"
    ]
  },
  {
    "objectID": "how-does-a-neural-net-really-work.html#automating-gradient-descent",
    "href": "how-does-a-neural-net-really-work.html#automating-gradient-descent",
    "title": "NeuralNet Deep dive",
    "section": "Automating gradient descent",
    "text": "Automating gradient descent\nThe basic idea is this: if we know the gradient of our mae() function with respect to our parameters, a, b, and c, then that means we know how adjusting (for instance) a will change the value of mae(). If, say, a has a negative gradient, then we know that increasing a will decrease mae(). Then we know that’s what we need to do, since we trying to make mae() as low as possible.\nSo, we find the gradient of mae() for each of our parameters, and then adjust our parameters a bit in the opposite direction to the sign of the gradient.\nTo do this, first we need a function that takes all the parameters a, b, and c as a single vector input, and returns the value mae() based on those parameters:\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\nLet’s try it:\n\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.3176, dtype=torch.float64)\n\n\nYup, that’s the same as the starting mae() we had in our plot before.\nWe’re first going to do exactly the same thing as we did manually – pick some arbritrary starting point for our parameters. We’ll put them all into a single tensor:\n\nabc = torch.tensor([1.1,1.1,1.1])\n\nTo tell PyTorch that we want it to calculate gradients for these parameters, we need to call requires_grad_():\n\nabc.requires_grad_()\n\ntensor([1.1000, 1.1000, 1.1000], requires_grad=True)\n\n\nWe can now calculate mae(). Generally, when doing gradient descent, the thing we’re trying to minimise is called the loss:\n\nloss = quad_mae(abc)\nloss\n\ntensor(2.3176, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nTo get PyTorch to now calculate the gradients, we need to call backward()\n\nloss.backward()\n\nThe gradients will be stored for us in an attribute called grad:\n\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\nAccording to these gradients, all our parameters are a little low. So let’s increase them a bit. If we subtract the gradient, multiplied by a small number, that should improve them a bit:\n\nwith torch.no_grad():\n    abc -= abc.grad*0.01\n    loss = quad_mae(abc)\n    \nprint(f'loss={loss:.2f} and {abc}')\n\nloss=2.30 and tensor([1.1135, 1.1003, 1.1050], requires_grad=True)\n\n\nYes, our loss has gone down!\nThe “small number” we multiply is called the learning rate, and is the most important hyper-parameter to set when training a neural network.\nBTW, you’ll see we had to wrap our calculation of the new parameters in with torch.no_grad(). That disables the calculation of gradients for any operations inside that context manager. We have to do that, because abc -= abc.grad*0.01 isn’t actually part of our quadratic model, so we don’t want derivitives to include that calculation.\nWe can use a loop to do a few more iterations of this:\n\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f} and {abc}')\n\nstep=0; loss=2.30 and tensor([1.1406, 1.1009, 1.1150], requires_grad=True)\nstep=1; loss=2.26 and tensor([1.1812, 1.1019, 1.1300], requires_grad=True)\nstep=2; loss=2.19 and tensor([1.2353, 1.1032, 1.1500], requires_grad=True)\nstep=3; loss=2.11 and tensor([1.3029, 1.1047, 1.1750], requires_grad=True)\nstep=4; loss=2.01 and tensor([1.3841, 1.1066, 1.2050], requires_grad=True)\nstep=5; loss=1.88 and tensor([1.4788, 1.1088, 1.2400], requires_grad=True)\nstep=6; loss=1.74 and tensor([1.5868, 1.1119, 1.2790], requires_grad=True)\nstep=7; loss=1.58 and tensor([1.7080, 1.1158, 1.3220], requires_grad=True)\nstep=8; loss=1.40 and tensor([1.8415, 1.1215, 1.3680], requires_grad=True)\nstep=9; loss=1.24 and tensor([1.9861, 1.1301, 1.4160], requires_grad=True)\n\n\nAs you can see, our loss keeps going down!\nIf you keep running this loop for long enough however, you’ll see that the loss eventually starts increasing for a while. That’s because once the parameters get close to the correct answer, our parameter updates will jump right over the correct answer! To avoid this, we need to decrease our learning rate as we train. This is done using a learning rate schedule, and can be automated in most deep learning frameworks, such as fastai and PyTorch.",
    "crumbs": [
      "Blog",
      "NeuralNet Deep dive"
    ]
  },
  {
    "objectID": "how-does-a-neural-net-really-work.html#how-a-neural-network-approximates-any-given-function",
    "href": "how-does-a-neural-net-really-work.html#how-a-neural-network-approximates-any-given-function",
    "title": "NeuralNet Deep dive",
    "section": "How a neural network approximates any given function",
    "text": "How a neural network approximates any given function\nBut neural nets are much more convenient and powerful than this example showed, because we can learn much more than just a quadratic with them. How does that work?\nThe trick is that a neural network is a very expressive function. In fact – it’s infinitely expressive. A neural network can approximate any computable function, given enough parameters. A “computable function” can cover just about anything you can imagine: understand and translate human speech; paint a picture; diagnose a disease from medical imaging; write an essay; etc…\nThe way a neural network approximates a function actually turns out to be very simple. The key trick is to combine two extremely basic steps:\n\nMatrix multiplication, which is just multiplying things together and then adding them up\nThe function \\(max(x,0)\\), which simply replaces all negative numbers with zero.\n\nIn PyTorch, the function \\(max(x,0)\\) is written as np.clip(x,0). The combination of a linear function and this max() is called a rectified linear function, and it can be implemented like this:\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0)\n\nHere’s what it looks like:\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\n\n\n\n\nBTW, instead of torch.clip(y, 0.), we can instead use F.relu(x), which does exactly the same thing. In PyTorch, F refers to the torch.nn.functional module.\n\nimport torch.nn.functional as F\n\n\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\n\n\n\n\nTo understand how this function works, try using this interactive version to play around with the parameters m and b:\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))\n\n\n\n\nAs you see, m changes the slope, and b changes where the “hook” appears. This function doesn’t do much on its own, but look what happens when we add two of them together:\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nIf you play around with that for a while, you notice something quite profound: with enough of these rectified linear functions added together, you could approximate any function with a single input, to whatever accuracy you like! Any time the function doesn’t quite match, you can just add a few more additions to the mix to make it a bit closer. As an experiment, perhaps you’d like to try creating your own plot_triple_relu interactive function, and maybe even include the scatter plot of our data from before, to see how close you can get?\nThis exact same approach can be expanded to functions of 2, 3, or more parameters.",
    "crumbs": [
      "Blog",
      "NeuralNet Deep dive"
    ]
  },
  {
    "objectID": "how-does-a-neural-net-really-work.html#how-to-recognise-an-owl",
    "href": "how-does-a-neural-net-really-work.html#how-to-recognise-an-owl",
    "title": "NeuralNet Deep dive",
    "section": "How to recognise an owl",
    "text": "How to recognise an owl\nOK great, we’ve created a nifty little example showing that we can drawing squiggly lines that go through some points. So what?\nWell… the truth is that actually drawing squiggly lines (or planes, or high-dimensional hyperplanes…) through some points is literally all that deep learning does! If your data points are, say, the RGB values of pixels in photos of owls, then you can create an owl-recogniser model by following the exact steps above.\nThis may, at first, sound about as useful as the classic “how to draw an owl” guide:\n\n\n\nimage.png\n\n\nStudents often ask me at this point “OK Jeremy, but how do neural nets actually work”. But at a foundational level, there is no “step 2”. We’re done – the above steps will, given enough time and enough data, create (for example) an owl recogniser, if you feed in enough owls (and non-owls).\nThe devil, I guess, is in the “given enough time and enough data” part of the above sentence. There’s a lot of tweaks we can make to reduce both of these things. For instance, instead of running our calculations on a normal CPU, as we’ve done above, we could do thousands of them simultaneously by taking advantage of a GPU. We could greatly reduce the amount of computation and data needed by using a convolution instead of a matrix multiplication, which basically means skipping over a bunch of the multiplications and additions for bits that you’d guess won’t be important. We could make things much faster if, instead of starting with random parameters, we start with parameters of someone else’s model that does something similar to what we want (this is called transfer learning).\nAnd, of course, there’s lots of helpful software out there to do this stuff for you without too much fuss. Like, say, fastai.\nLearning these things is what we teach in our course, which, like everything we make, is totally free. So if you’re interested in learning more, do check it out!\nAs always, if you enjoyed this notebook, please upvote it to help others find it, and to encourage me to write more. If you upvote it, be careful you don’t accidentally upvote your copy that’s created when you click “Copy & Edit” – you can find my original at this link.",
    "crumbs": [
      "Blog",
      "NeuralNet Deep dive"
    ]
  },
  {
    "objectID": "imgnet_tiny.html",
    "href": "imgnet_tiny.html",
    "title": "Tiny Imagenet",
    "section": "",
    "text": "import os\n# os.environ['CUDA_VISIBLE_DEVICES']='2'\nimport shutil,timm,os,torch,random,datasets,math\nimport fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\nfrom glob import glob\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.training import *\nfrom fastprogress import progress_bar\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "imgnet_tiny.html#data-processing",
    "href": "imgnet_tiny.html#data-processing",
    "title": "Tiny Imagenet",
    "section": "Data processing",
    "text": "Data processing\n\npath_data = Path('Data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'tiny-imagenet-200'\n\n\nurl = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\nif not path.exists():\n    path_zip = fc.urlsave(url, path_data)\n    shutil.unpack_archive('Data/tiny-imagenet-200.zip', 'data')\n\n\nbs = 512\n\n\nclass TinyDS:\n    def __init__(self, path):\n        self.path = Path(path)\n        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return self.files[i],Path(self.files[i]).parent.parent.name\n\n\ntds = TinyDS(path/'train')\n\n\ntds[0]\n\n('Data/tiny-imagenet-200/train/n02074367/images/n02074367_322.JPEG',\n 'n02074367')\n\n\n\npath_anno = path/'val'/'val_annotations.txt'\nanno = dict(o.split('\\t')[:2] for o in path_anno.read_text().splitlines())\n\n\nclass TinyValDS(TinyDS):\n    def __getitem__(self, i): return self.files[i],anno[os.path.basename(self.files[i])]\n\n\nvds = TinyValDS(path/'val')\n\n\nvds[0]\n\n('Data/tiny-imagenet-200/val/images/val_240.JPEG', 'n02883205')\n\n\n\nlen(tds)\n\n100000\n\n\n\nclass TfmDS:\n    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n    def __len__(self): return len(self.ds)\n    def __getitem__(self, i):\n        x,y = self.ds[i]\n        return self.tfmx(x),self.tfmy(y)\n\n\nid2str = (path/'wnids.txt').read_text().splitlines()\nstr2id = {v:k for k,v in enumerate(id2str)}\n\n\nxmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))\n\n\ndef tfmx(x):\n    img = read_image(x, mode=ImageReadMode.RGB)/255\n    return (img-xmean[:,None,None])/xstd[:,None,None]\n\n\ndef tfmy(y): return tensor(str2id[y])\n\n\ntfm_tds = TfmDS(tds, tfmx, tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\n\nxi,yi = tfm_tds[0]\nid2str[yi]\n\n'n02074367'\n\n\n\ndef denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clip(0,1)\n\n\nshow_image(denorm(xi));\n\n\n\n\n\n\n\n\n\ndltrn = DataLoader(tfm_tds, batch_size=bs, shuffle=True, num_workers=8)\n\n\nxb,yb = b = next(iter(dltrn))\n\n\nshow_image(denorm(xb[0]));\n\n\n\n\n\n\n\n\n\nall_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\nsynsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}\n\n\ntitles = [synsets[id2str[o]] for o in yb]\n', '.join(titles[:20])\n\n'seashore, dam, orangutan, American alligator, acorn, bighorn, cliff, poncho, stopwatch, monarch, cliff dwelling, poncho, chest, mantis, standard poodle, gazelle, dam, sombrero, barn, projectile'\n\n\n\nshow_images(denorm(xb[:9]), titles=titles[:9], imsize=2.5)\n\n\n\n\n\n\n\n\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "imgnet_tiny.html#basic-model",
    "href": "imgnet_tiny.html#basic-model",
    "title": "Tiny Imagenet",
    "section": "Basic model",
    "text": "Basic model\n\ndef tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n\n\ntfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     RandErase())\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\n\n\nnfs = (32,64,128,256,512,1024)\n\n\ndef get_dropmodel(act=act_gr, nfs=nfs, norm=nn.BatchNorm2d, drop=0.1):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n#     layers += [ResBlock(nfs[0], nfs[0], ks=3, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nlearn = TrainLearner(get_dropmodel(), dls, F.cross_entropy, cbs=[SingleBatchCB(), augcb, DeviceCB()])\nlearn.fit(1)\nxb,yb = learn.batch\nshow_images(denorm(xb.cpu())[:9], imsize=2.5)\n\n\n\n\n\n\n\n\n\nlearn.summary()\n\nTot params: 19775824; MFLOPS: 303.7\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nConv2d\n(512, 3, 64, 64)\n(512, 32, 64, 64)\n2432\n9.8\n\n\nResBlock\n(512, 32, 64, 64)\n(512, 64, 32, 32)\n57792\n58.7\n\n\nResBlock\n(512, 64, 32, 32)\n(512, 128, 16, 16)\n230272\n58.7\n\n\nResBlock\n(512, 128, 16, 16)\n(512, 256, 8, 8)\n919296\n58.7\n\n\nResBlock\n(512, 256, 8, 8)\n(512, 512, 4, 4)\n3673600\n58.7\n\n\nResBlock\n(512, 512, 4, 4)\n(512, 1024, 2, 2)\n14687232\n58.7\n\n\nAdaptiveAvgPool2d\n(512, 1024, 2, 2)\n(512, 1024, 1, 1)\n0\n0.0\n\n\nFlatten\n(512, 1024, 1, 1)\n(512, 1024)\n0\n0.0\n\n\nDropout\n(512, 1024)\n(512, 1024)\n0\n0.0\n\n\nLinear\n(512, 1024)\n(512, 200)\n204800\n0.2\n\n\nBatchNorm1d\n(512, 200)\n(512, 200)\n400\n0.0\n\n\n\n\n\n\nopt_func = partial(optim.AdamW, eps=1e-5)\n\n\nlr_cbs = [DeviceCB(), augcb, MixedPrecision(), ProgressCB()]\n\n\nlearn = Learner(get_dropmodel(), dls, F.cross_entropy, cbs=lr_cbs, opt_func=opt_func)\nlearn.lr_find()\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n    \n      \n      20.92% [41/196 03:15&lt;12:20 5.256]\n    \n    \n\n\n\n\n\n\n\n\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n\n\nepochs = 25\nlr = 0.1\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nlearn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.112\n4.370\n0\ntrain\n\n\n0.169\n3.852\n0\neval\n\n\n0.224\n3.580\n1\ntrain\n\n\n0.245\n3.385\n1\neval\n\n\n0.272\n3.244\n2\ntrain\n\n\n0.241\n3.557\n2\neval\n\n\n0.306\n3.025\n3\ntrain\n\n\n0.228\n3.717\n3\neval\n\n\n0.336\n2.856\n4\ntrain\n\n\n0.280\n3.414\n4\neval\n\n\n0.356\n2.741\n5\ntrain\n\n\n0.252\n3.563\n5\neval\n\n\n0.381\n2.611\n6\ntrain\n\n\n0.328\n3.046\n6\neval\n\n\n0.406\n2.480\n7\ntrain\n\n\n0.311\n3.181\n7\neval\n\n\n0.426\n2.379\n8\ntrain\n\n\n0.356\n2.861\n8\neval\n\n\n0.453\n2.255\n9\ntrain\n\n\n0.353\n2.905\n9\neval\n\n\n0.471\n2.166\n10\ntrain\n\n\n0.381\n2.751\n10\neval\n\n\n0.489\n2.070\n11\ntrain\n\n\n0.388\n2.806\n11\neval\n\n\n0.513\n1.963\n12\ntrain\n\n\n0.451\n2.325\n12\neval\n\n\n0.531\n1.870\n13\ntrain\n\n\n0.455\n2.388\n13\neval\n\n\n0.554\n1.753\n14\ntrain\n\n\n0.478\n2.210\n14\neval\n\n\n0.587\n1.610\n15\ntrain\n\n\n0.510\n2.089\n15\neval\n\n\n0.617\n1.470\n16\ntrain\n\n\n0.524\n2.032\n16\neval\n\n\n0.655\n1.311\n17\ntrain\n\n\n0.537\n1.999\n17\neval\n\n\n0.695\n1.142\n18\ntrain\n\n\n0.547\n1.980\n18\neval\n\n\n0.743\n0.944\n19\ntrain\n\n\n0.566\n1.912\n19\neval\n\n\n0.794\n0.748\n20\ntrain\n\n\n0.581\n1.889\n20\neval\n\n\n0.842\n0.579\n21\ntrain\n\n\n0.586\n1.890\n21\neval\n\n\n0.877\n0.461\n22\ntrain\n\n\n0.594\n1.857\n22\neval\n\n\n0.906\n0.367\n23\ntrain\n\n\n0.594\n1.874\n23\neval\n\n\n0.910\n0.351\n24\ntrain\n\n\n0.593\n1.889\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/inettiny-basic-25')",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "imgnet_tiny.html#deeper",
    "href": "imgnet_tiny.html#deeper",
    "title": "Tiny Imagenet",
    "section": "Deeper",
    "text": "Deeper\n\ndef res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n    return nn.Sequential(*[\n        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n        for i in range(n_bk)])\n\n\nnbks = (3,2,2,1,1)\n\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nlearn = TrainLearner(get_dropmodel(), dls, F.cross_entropy, cbs=[SingleBatchCB(), augcb, DeviceCB()])\nlearn.fit(1)\nxb,yb = learn.batch\nshow_images(denorm(xb.cpu())[:9], imsize=2.5)\n\n\n\n\n\n\n\n\n\nlearn.summary()\n\nTot params: 21426800; MFLOPS: 710.9\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nResBlock\n(512, 3, 64, 64)\n(512, 32, 64, 64)\n28320\n115.1\n\n\nSequential\n(512, 32, 64, 64)\n(512, 64, 32, 32)\n206016\n209.7\n\n\nSequential\n(512, 64, 32, 32)\n(512, 128, 16, 16)\n525952\n134.2\n\n\nSequential\n(512, 128, 16, 16)\n(512, 256, 8, 8)\n2100480\n134.2\n\n\nSequential\n(512, 256, 8, 8)\n(512, 512, 4, 4)\n3673600\n58.7\n\n\nSequential\n(512, 512, 4, 4)\n(512, 1024, 2, 2)\n14687232\n58.7\n\n\nAdaptiveAvgPool2d\n(512, 1024, 2, 2)\n(512, 1024, 1, 1)\n0\n0.0\n\n\nFlatten\n(512, 1024, 1, 1)\n(512, 1024)\n0\n0.0\n\n\nDropout\n(512, 1024)\n(512, 1024)\n0\n0.0\n\n\nLinear\n(512, 1024)\n(512, 200)\n204800\n0.2\n\n\nBatchNorm1d\n(512, 200)\n(512, 200)\n400\n0.0\n\n\n\n\n\n\nopt_func = partial(optim.AdamW, eps=1e-5)\n\n\nlearn = Learner(get_dropmodel(), dls, F.cross_entropy, cbs=lr_cbs, opt_func=opt_func)\nlearn.lr_find()\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;00:00]\n    \n    \n\n\n    \n      \n      28.57% [56/196 00:19&lt;00:49 9.499]\n    \n    \n\n\n\n\n\n\n\n\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n\nepochs = 25\nlr = 3e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nlearn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.073\n4.595\n0\ntrain\n\n\n0.099\n4.283\n0\neval\n\n\n0.175\n3.825\n1\ntrain\n\n\n0.192\n3.682\n1\neval\n\n\n0.238\n3.393\n2\ntrain\n\n\n0.203\n3.798\n2\neval\n\n\n0.291\n3.075\n3\ntrain\n\n\n0.248\n3.404\n3\neval\n\n\n0.329\n2.873\n4\ntrain\n\n\n0.275\n3.290\n4\neval\n\n\n0.359\n2.709\n5\ntrain\n\n\n0.352\n2.843\n5\neval\n\n\n0.386\n2.569\n6\ntrain\n\n\n0.357\n2.794\n6\neval\n\n\n0.417\n2.423\n7\ntrain\n\n\n0.389\n2.662\n7\neval\n\n\n0.439\n2.305\n8\ntrain\n\n\n0.418\n2.449\n8\neval\n\n\n0.465\n2.183\n9\ntrain\n\n\n0.418\n2.449\n9\neval\n\n\n0.484\n2.081\n10\ntrain\n\n\n0.429\n2.423\n10\neval\n\n\n0.505\n1.998\n11\ntrain\n\n\n0.465\n2.254\n11\neval\n\n\n0.527\n1.889\n12\ntrain\n\n\n0.435\n2.413\n12\neval\n\n\n0.548\n1.796\n13\ntrain\n\n\n0.494\n2.174\n13\neval\n\n\n0.571\n1.681\n14\ntrain\n\n\n0.491\n2.162\n14\neval\n\n\n0.594\n1.579\n15\ntrain\n\n\n0.537\n1.955\n15\neval\n\n\n0.619\n1.462\n16\ntrain\n\n\n0.539\n1.974\n16\neval\n\n\n0.654\n1.313\n17\ntrain\n\n\n0.573\n1.818\n17\neval\n\n\n0.686\n1.178\n18\ntrain\n\n\n0.585\n1.785\n18\neval\n\n\n0.724\n1.024\n19\ntrain\n\n\n0.590\n1.752\n19\neval\n\n\n0.761\n0.872\n20\ntrain\n\n\n0.611\n1.704\n20\neval\n\n\n0.800\n0.723\n21\ntrain\n\n\n0.611\n1.686\n21\neval\n\n\n0.831\n0.616\n22\ntrain\n\n\n0.619\n1.679\n22\neval\n\n\n0.857\n0.530\n23\ntrain\n\n\n0.620\n1.686\n23\neval\n\n\n0.866\n0.501\n24\ntrain\n\n\n0.618\n1.684\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/inettiny-custom-25')",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "imgnet_tiny.html#more-augmentation",
    "href": "imgnet_tiny.html#more-augmentation",
    "title": "Tiny Imagenet",
    "section": "More augmentation",
    "text": "More augmentation\n\naug_tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     T.TrivialAugmentWide())\n\nnorm_tfm = T.Normalize(xmean, xstd)\nerase_tfm = RandErase()\n\n\nfrom PIL import Image\n\n\ndef tfmx(x, aug=False):\n    x = Image.open(x).convert('RGB')\n    if aug: x = aug_tfms(x)\n    x = TF.to_tensor(x)\n    x = norm_tfm(x)\n    if aug: x = erase_tfm(x[None])[0]\n    return x\n\n\ntfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\ndef conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True):\n    layers = []\n    if norm: layers.append(norm(ni))\n    if act : layers.append(act())\n    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n    return nn.Sequential(*layers)\n\ndef _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n    return nn.Sequential(conv(ni, nf, stride=1     , act=act, norm=norm, ks=ks),\n                         conv(nf, nf, stride=stride, act=act, norm=norm, ks=ks))\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n        super().__init__()\n        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\ndef get_model(): return get_dropmodel(nbks=(4,3,3,2,1), drop=0.1)\n\n\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=[SingleBatchCB(), DeviceCB()])\nlearn.fit(1)\nxb,yb = learn.batch\nshow_images(denorm(xb.cpu())[:9], imsize=2.5)\n\n\n\n\n\n\n\n\n\nepochs = 50\nlr = 0.1\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(get_model(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.026\n5.044\n0\ntrain\n\n\n0.035\n4.920\n0\neval\n\n\n0.052\n4.712\n1\ntrain\n\n\n0.075\n4.434\n1\neval\n\n\n0.086\n4.417\n2\ntrain\n\n\n0.108\n4.135\n2\neval\n\n\n0.121\n4.149\n3\ntrain\n\n\n0.141\n4.008\n3\neval\n\n\n0.153\n3.908\n4\ntrain\n\n\n0.180\n3.725\n4\neval\n\n\n0.182\n3.706\n5\ntrain\n\n\n0.203\n3.588\n5\neval\n\n\n0.212\n3.532\n6\ntrain\n\n\n0.175\n3.909\n6\neval\n\n\n0.236\n3.393\n7\ntrain\n\n\n0.244\n3.477\n7\neval\n\n\n0.254\n3.289\n8\ntrain\n\n\n0.212\n3.779\n8\neval\n\n\n0.269\n3.196\n9\ntrain\n\n\n0.296\n3.069\n9\neval\n\n\n0.286\n3.110\n10\ntrain\n\n\n0.318\n2.962\n10\neval\n\n\n0.297\n3.041\n11\ntrain\n\n\n0.237\n3.614\n11\neval\n\n\n0.308\n2.978\n12\ntrain\n\n\n0.319\n2.962\n12\neval\n\n\n0.320\n2.922\n13\ntrain\n\n\n0.327\n2.917\n13\neval\n\n\n0.331\n2.867\n14\ntrain\n\n\n0.251\n3.495\n14\neval\n\n\n0.341\n2.810\n15\ntrain\n\n\n0.306\n3.130\n15\neval\n\n\n0.352\n2.769\n16\ntrain\n\n\n0.370\n2.665\n16\neval\n\n\n0.364\n2.707\n17\ntrain\n\n\n0.310\n3.141\n17\neval\n\n\n0.370\n2.673\n18\ntrain\n\n\n0.385\n2.654\n18\neval\n\n\n0.376\n2.642\n19\ntrain\n\n\n0.384\n2.647\n19\neval\n\n\n0.384\n2.594\n20\ntrain\n\n\n0.328\n3.035\n20\neval\n\n\n0.392\n2.554\n21\ntrain\n\n\n0.298\n3.251\n21\neval\n\n\n0.401\n2.515\n22\ntrain\n\n\n0.383\n2.665\n22\neval\n\n\n0.409\n2.472\n23\ntrain\n\n\n0.397\n2.596\n23\neval\n\n\n0.418\n2.430\n24\ntrain\n\n\n0.374\n2.836\n24\neval\n\n\n0.429\n2.378\n25\ntrain\n\n\n0.417\n2.603\n25\neval\n\n\n0.436\n2.343\n26\ntrain\n\n\n0.403\n2.647\n26\neval\n\n\n0.445\n2.293\n27\ntrain\n\n\n0.387\n2.714\n27\neval\n\n\n0.457\n2.238\n28\ntrain\n\n\n0.427\n2.439\n28\neval\n\n\n0.467\n2.175\n29\ntrain\n\n\n0.460\n2.317\n29\neval\n\n\n0.481\n2.126\n30\ntrain\n\n\n0.457\n2.280\n30\neval\n\n\n0.492\n2.065\n31\ntrain\n\n\n0.459\n2.333\n31\neval\n\n\n0.506\n2.000\n32\ntrain\n\n\n0.508\n2.073\n32\neval\n\n\n0.519\n1.934\n33\ntrain\n\n\n0.505\n2.095\n33\neval\n\n\n0.535\n1.859\n34\ntrain\n\n\n0.509\n2.084\n34\neval\n\n\n0.552\n1.778\n35\ntrain\n\n\n0.484\n2.218\n35\neval\n\n\n0.570\n1.702\n36\ntrain\n\n\n0.532\n1.971\n36\neval\n\n\n0.588\n1.616\n37\ntrain\n\n\n0.561\n1.867\n37\neval\n\n\n0.612\n1.522\n38\ntrain\n\n\n0.576\n1.841\n38\neval\n\n\n0.630\n1.437\n39\ntrain\n\n\n0.578\n1.788\n39\neval\n\n\n0.660\n1.322\n40\ntrain\n\n\n0.585\n1.772\n40\neval\n\n\n0.680\n1.232\n41\ntrain\n\n\n0.596\n1.708\n41\neval\n\n\n0.703\n1.141\n42\ntrain\n\n\n0.614\n1.664\n42\neval\n\n\n0.730\n1.036\n43\ntrain\n\n\n0.631\n1.618\n43\neval\n\n\n0.751\n0.954\n44\ntrain\n\n\n0.635\n1.591\n44\neval\n\n\n0.768\n0.891\n45\ntrain\n\n\n0.642\n1.598\n45\neval\n\n\n0.783\n0.832\n46\ntrain\n\n\n0.647\n1.565\n46\neval\n\n\n0.793\n0.800\n47\ntrain\n\n\n0.650\n1.564\n47\neval\n\n\n0.800\n0.770\n48\ntrain\n\n\n0.649\n1.566\n48\neval\n\n\n0.803\n0.761\n49\ntrain\n\n\n0.649\n1.561\n49\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/inettiny-trivaug-50')",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Hugging Face Datasets",
    "section": "",
    "text": "!pip list | grep fastAIcourse\n!pip list | grep datasets\n\nfastAIcourse                  0.0.91      /home/ben/BENEDICT_Only/Benedict_Projects/Benedict_ML/fastAIcourse\ndatasets                      2.14.4\ndatasetsforecast              0.0.8\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\nds_builder.info.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\nds_builder.info.splits\n\n{'train': SplitInfo(name='train', num_bytes=31296607, num_examples=60000, shard_lengths=None, dataset_name='fashion_mnist'),\n 'test': SplitInfo(name='test', num_bytes=5233810, num_examples=10000, shard_lengths=None, dataset_name='fashion_mnist')}\ndsd = load_dataset(name)\ndsd\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\ntrain,test = dsd['train'],dsd['test']\ntrain[0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9}\nx,y = ds_builder.info.features\nx,y\n\n('image', 'label')\nx,y = 'image','label'\nimg = train[0][x]\nimg\nxb = train[:5][x]\nyb = train[:5][y]\nxb, yb\n\n([&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;],\n [9, 0, 0, 3, 0])\nfeaty = train.features[y]\nfeaty\n\nClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\nfeaty.int2str(yb)\n\n['Ankle boot',\n 'T - shirt / top',\n 'T - shirt / top',\n 'Dress',\n 'T - shirt / top']\ntrain['label'][:5]\n\n[9, 0, 0, 3, 0]\nsource",
    "crumbs": [
      "Blog",
      "Hugging Face Datasets"
    ]
  },
  {
    "objectID": "datasets.html#plotting-images",
    "href": "datasets.html#plotting-images",
    "title": "Hugging Face Datasets",
    "section": "Plotting images",
    "text": "Plotting images\n\nb = next(iter(dl))\nxb = b['image']\nimg = xb[0]\nplt.imshow(img[0]);\n\n\n\n\n\n\n\n\n/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Other Parameters\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section See Also\n  else: warn(msg)\n\nsource\n\nshow_image\n\n show_image (im, ax=None, figsize=None, title=None, noframe=True,\n             cmap=None, norm=None, aspect=None, interpolation=None,\n             alpha=None, vmin=None, vmax=None, origin=None, extent=None,\n             interpolation_stage=None, filternorm=True, filterrad=4.0,\n             resample=None, url=None, data=None)\n\nShow a PIL or PyTorch image on ax.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nim\n\n\n\n\n\nax\nNoneType\nNone\n\n\n\nfigsize\nNoneType\nNone\n\n\n\ntitle\nNoneType\nNone\n\n\n\nnoframe\nbool\nTrue\n\n\n\ncmap\nNoneType\nNone\nThe Colormap instance or registered colormap name used to map scalar datato colors.This parameter is ignored if X is RGB(A).\n\n\nnorm\nNoneType\nNone\nThe normalization method used to scale scalar data to the [0, 1] rangebefore mapping to colors using cmap. By default, a linear scaling isused, mapping the lowest value to 0 and the highest to 1.If given, this can be one of the following:- An instance of .Normalize or one of its subclasses (see :ref:colormapnorms).- A scale name, i.e. one of “linear”, “log”, “symlog”, “logit”, etc. For a list of available scales, call matplotlib.scale.get_scale_names(). In that case, a suitable .Normalize subclass is dynamically generated and instantiated.This parameter is ignored if X is RGB(A).\n\n\naspect\nNoneType\nNone\nThe aspect ratio of the Axes. This parameter is particularlyrelevant for images since it determines whether data pixels aresquare.This parameter is a shortcut for explicitly calling.Axes.set_aspect. See there for further details.- ‘equal’: Ensures an aspect ratio of 1. Pixels will be square (unless pixel sizes are explicitly made non-square in data coordinates using extent).- ‘auto’: The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels.Normally, None (the default) means to use :rc:image.aspect. However, ifthe image uses a transform that does not contain the axes data transform,then None means to not modify the axes aspect at all (in that case, directlycall .Axes.set_aspect if desired).\n\n\ninterpolation\nNoneType\nNone\nThe interpolation method used.Supported values are ‘none’, ‘antialiased’, ‘nearest’, ‘bilinear’,‘bicubic’, ‘spline16’, ‘spline36’, ‘hanning’, ‘hamming’, ‘hermite’,‘kaiser’, ‘quadric’, ‘catrom’, ‘gaussian’, ‘bessel’, ‘mitchell’,‘sinc’, ‘lanczos’, ‘blackman’.The data X is resampled to the pixel size of the image on thefigure canvas, using the interpolation method to either up- ordownsample the data.If interpolation is ‘none’, then for the ps, pdf, and svgbackends no down- or upsampling occurs, and the image data ispassed to the backend as a native image. Note that different ps,pdf, and svg viewers may display these raw pixels differently. Onother backends, ‘none’ is the same as ‘nearest’.If interpolation is the default ‘antialiased’, then ‘nearest’interpolation is used if the image is upsampled by more than afactor of three (i.e. the number of display pixels is at leastthree times the size of the data array). If the upsampling rate issmaller than 3, or the image is downsampled, then ‘hanning’interpolation is used to act as an anti-aliasing filter, unless theimage happens to be upsampled by exactly a factor of two or one.See:doc:/gallery/images_contours_and_fields/interpolation_methodsfor an overview of the supported interpolation methods, and:doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.Some interpolation methods require an additional radius parameter,which can be set by filterrad. Additionally, the antigrain imageresize filter is controlled by the parameter filternorm.\n\n\nalpha\nNoneType\nNone\nThe alpha blending value, between 0 (transparent) and 1 (opaque).If alpha is an array, the alpha blending values are applied pixelby pixel, and alpha must have the same shape as X.\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\nPlace the [0, 0] index of the array in the upper left or lowerleft corner of the Axes. The convention (the default) ‘upper’ istypically used for matrices and images.Note that the vertical axis points upward for ‘lower’but downward for ‘upper’.See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\nextent\nNoneType\nNone\nThe bounding box in data coordinates that the image will fill.These values may be unitful and match the units of the Axes.The image is stretched individually along x and y to fill the box.The default extent is determined by the following conditions.Pixels have unit size in data coordinates. Their centers are oninteger coordinates, and their center coordinates range from 0 tocolumns-1 horizontally and from 0 to rows-1 vertically.Note that the direction of the vertical axis and thus the defaultvalues for top and bottom depend on origin:- For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).- For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\ninterpolation_stage\nNoneType\nNone\nIf ‘data’, interpolationis carried out on the data provided by the user. If ‘rgba’, theinterpolation is carried out after the colormapping has beenapplied (visual interpolation).\n\n\nfilternorm\nbool\nTrue\nA parameter for the antigrain image resize filter (see theantigrain documentation). If filternorm is set, the filternormalizes integer values and corrects the rounding errors. Itdoesn’t do anything with the source floating point values, itcorrects only integers according to the rule of 1.0 which meansthat any sum of pixel weights must be equal to 1.0. So, thefilter function must produce a graph of the proper shape.\n\n\nfilterrad\nfloat\n4.0\nThe filter radius for filters that have a radius parameter, i.e.when interpolation is one of: ‘sinc’, ‘lanczos’ or ‘blackman’.\n\n\nresample\nNoneType\nNone\nWhen True, use a full resampling method. When False, onlyresample when the output image is larger than the input image.\n\n\nurl\nNoneType\nNone\nSet the url of the created .AxesImage. See .Artist.set_url.\n\n\ndata\nNoneType\nNone\n\n\n\n\n\n\nExported source\n@fc.delegates(plt.Axes.imshow)\ndef show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n    \"Show a PIL or PyTorch image on `ax`.\"\n    if fc.hasattrs(im, ('cpu','permute','detach')):\n        im = im.detach().cpu()\n        if len(im.shape)==3 and im.shape[0]&lt;5: im=im.permute(1,2,0)\n    elif not isinstance(im,np.ndarray): im=np.array(im)\n    if im.shape[-1]==1: im=im[...,0]\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.set_xticks([]) \n    ax.set_yticks([]) \n    if noframe: ax.axis('off')\n    return ax\n\n\n\nhelp(show_image)\n\nHelp on function show_image in module __main__:\n\nshow_image(im, ax=None, figsize=None, title=None, noframe=True, *, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, interpolation_stage=None, filternorm=True, filterrad=4.0, resample=None, url=None, data=None)\n    Show a PIL or PyTorch image on `ax`.\n\n\n\n\nshow_image(img, figsize=(2,2));\n\n\n\n\n\n\n\n\n\nfig,axs = plt.subplots(1,2)\nshow_image(img, axs[0])\nshow_image(xb[1], axs[1]);\n\n\n\n\n\n\n\n\n\n\nExported source\n@fc.delegates(plt.subplots, keep=True)\ndef subplots(\n    nrows:int=1, # Number of rows in returned axes grid\n    ncols:int=1, # Number of columns in returned axes grid\n    figsize:tuple=None, # Width, height in inches of the returned figure\n    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n    suptitle:str=None, # Title to be set to returned figure\n    **kwargs\n): # fig and axs\n    \"A figure and set of subplots to display images of `imsize` inches\"\n    if figsize is None: figsize=(ncols*imsize, nrows*imsize)\n    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if suptitle is not None: fig.suptitle(suptitle)\n    if nrows*ncols==1: ax = np.array([ax])\n    return fig,ax\n\n\n\n\nExported source\nfrom nbdev.showdoc import show_doc\n\n\n\nsource\n\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None, sharex=False, sharey=False, squeeze=True,\n           width_ratios=None, height_ratios=None, subplot_kw=None,\n           gridspec_kw=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nNoneType\nNone\n\n\n\nheight_ratios\nNoneType\nNone\n\n\n\nsubplot_kw\nNoneType\nNone\n\n\n\ngridspec_kw\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nfig,axs = subplots(3,3, imsize=1)\nimgs = xb[:8]\nfor ax,img in zip(axs.flat,imgs): show_image(img, ax)\n\n\n\n\n\n\n\n\n–checking\n\nsource\n\n\nget_grid\n\n get_grid (n:int, nrows:int=None, ncols:int=None, title:str=None,\n           weight:str='bold', size:int=14, **kwargs)\n\nReturn a grid of n axes, rows by cols\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\nNumber of axes\n\n\nnrows\nint\nNone\nNumber of rows, defaulting to int(math.sqrt(n))\n\n\nncols\nint\nNone\nNumber of columns, defaulting to ceil(n/rows)\n\n\ntitle\nstr\nNone\nIf passed, title set to the figure\n\n\nweight\nstr\nbold\nTitle font weight\n\n\nsize\nint\n14\nTitle font size\n\n\nkwargs\n\n\n\n\n\n\n\n\nExported source\ndef get_grid(\n    n:int, # Number of axes\n    nrows:int=None, # Number of rows, defaulting to `int(math.sqrt(n))`\n    ncols:int=None, # Number of columns, defaulting to `ceil(n/rows)`\n    title:str=None, # If passed, title set to the figure\n    weight:str='bold', # Title font weight\n    size:int=14, # Title font size\n    **kwargs,\n): # fig and axs\n    \"Return a grid of `n` axes, `rows` by `cols`\"\n    if nrows: ncols = ncols or int(np.floor(n/nrows))\n    elif ncols: nrows = nrows or int(np.ceil(n/ncols))\n    else:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.floor(n/nrows))\n    fig,axs = subplots(nrows, ncols, **kwargs)\n    for i in range(n, nrows*ncols): axs.flat[i].set_axis_off()\n    if title is not None: fig.suptitle(title, weight=weight, size=size)\n    return fig,axs\n\n\n\nfig,axs = get_grid(8, nrows=3, imsize=1)\nfor ax,img in zip(axs.flat,imgs): show_image(img, ax)\n\n\n\n\n\n\n\n\n\nsource\n\n\nshow_images\n\n show_images (ims:list, nrows:int|None=None, ncols:int|None=None,\n              titles:list|None=None, **kwargs)\n\nShow all images ims as subplots with rows using titles\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nims\nlist\n\nImages to show\n\n\nnrows\nint | None\nNone\nNumber of rows in grid\n\n\nncols\nint | None\nNone\nNumber of columns in grid (auto-calculated if None)\n\n\ntitles\nlist | None\nNone\nOptional list of titles for each image\n\n\nkwargs\n\n\n\n\n\n\n\n\nExported source\ndef show_images(ims:list, # Images to show\n                nrows:int|None=None, # Number of rows in grid\n                ncols:int|None=None, # Number of columns in grid (auto-calculated if None)\n                titles:list|None=None, # Optional list of titles for each image\n                **kwargs):\n    \"Show all images `ims` as subplots with `rows` using `titles`\"\n    axs = get_grid(len(ims), nrows, ncols, **kwargs)[1].flat\n    for im,t,ax in zip_longest(ims, titles or [], axs): show_image(im, ax=ax, title=t)\n\n\n\nyb = b['label']\nlbls = yb[:8]\n\n\nnames = \"Top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot\".split()\ntitles = itemgetter(*lbls)(names)\n' '.join(titles)\n\n'Boot Top Top Dress Top Pullover Sneaker Pullover'\n\n\n\nshow_images(imgs, imsize=1.7, titles=titles)\n\n\n\n\n\n\n\n\n\nsource\n\n\nDataLoaders\n\n DataLoaders (*dls)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls[:2]\n\n    @classmethod\n    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n        f = collate_dict(dd['train'])\n        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f, **kwargs))",
    "crumbs": [
      "Blog",
      "Hugging Face Datasets"
    ]
  },
  {
    "objectID": "convolutions.html",
    "href": "convolutions.html",
    "title": "Convolutions",
    "section": "",
    "text": "Other forms of AI - multi level perceptron - convolution - transformer net works\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nimport pandas as pd,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor\n\nfrom torch.utils.data import DataLoader\nfrom typing import Mapping\nmpl.rcParams['image.cmap'] = 'gray'\npath_data = Path('Data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\nIn the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that.\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use a convolution. A convolution requires nothing more than multiplication, and addition.",
    "crumbs": [
      "Blog",
      "Convolutions"
    ]
  },
  {
    "objectID": "convolutions.html#creating-the-cnn",
    "href": "convolutions.html#creating-the-cnn",
    "title": "Convolutions",
    "section": "Creating the CNN",
    "text": "Creating the CNN\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\n\nsource\n\nconv\n\n conv (ni, nf, ks=3, stride=2, act=True)\n\n\n\nExported source\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n\nRefactoring parts of your neural networks like this makes it much less likely you’ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\n\nsimple_cnn = nn.Sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,16),           #2x2\n    conv(16,10, act=False), #1x1\n    nn.Flatten(),\n)\n\n\ndimensions = [o.shape for o in model.parameters()]\n\nparameters_per_layer = [np.product(o.shape) for o in model.parameters()]\n\nparameters_total = tensor([np.product(o.shape) for o in model.parameters()]).sum()\n\nmodel, dimensions, parameters_per_layer, parameters_total\n\n(Sequential(\n   (0): Linear(in_features=784, out_features=50, bias=True)\n   (1): ReLU()\n   (2): Linear(in_features=50, out_features=10, bias=True)\n ),\n [torch.Size([50, 784]),\n  torch.Size([50]),\n  torch.Size([10, 50]),\n  torch.Size([10])],\n [39200, 50, 500, 10],\n tensor(39760))\n\n\n\ndimensions = [o.shape for o in simple_cnn.parameters()]\n\nparameters_per_layer = [np.product(o.shape) for o in simple_cnn.parameters()]\n\nparameters_total = tensor([np.product(o.shape) for o in simple_cnn.parameters()]).sum()\n\nsimple_cnn, dimensions, parameters_per_layer, parameters_total\n\n(Sequential(\n   (0): Sequential(\n     (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n     (1): ReLU()\n   )\n   (1): Sequential(\n     (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n     (1): ReLU()\n   )\n   (2): Sequential(\n     (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n     (1): ReLU()\n   )\n   (3): Sequential(\n     (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n     (1): ReLU()\n   )\n   (4): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (5): Flatten(start_dim=1, end_dim=-1)\n ),\n [torch.Size([4, 1, 3, 3]),\n  torch.Size([4]),\n  torch.Size([8, 4, 3, 3]),\n  torch.Size([8]),\n  torch.Size([16, 8, 3, 3]),\n  torch.Size([16]),\n  torch.Size([16, 16, 3, 3]),\n  torch.Size([16]),\n  torch.Size([10, 16, 3, 3]),\n  torch.Size([10])],\n [36, 4, 288, 8, 1152, 16, 2304, 16, 1440, 10],\n tensor(5274))\n\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\n\nx_imgs = x_train.view(-1,1,28,28)\nxv_imgs = x_valid.view(-1,1,28,28)\ntrain_ds,valid_ds = Dataset(x_imgs, y_train),Dataset(xv_imgs, y_valid)\n\n\nsource\n\n\ncollate_device\n\n collate_device (b)\n\n\n\nExported source\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n\n\n\nsource\n\n\nto_device\n\n to_device (x, device='cpu')\n\n\nfrom torch import optim\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.5696531332492828 0.8189000003814697\n1 0.17475426919460296 0.9438000001907348\n2 0.1348926905155182 0.9603000007629394\n3 0.11516531736850738 0.9650000008583068\n4 0.19701933751106263 0.9389000008583069\n\n\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.08551515686511993 0.9752999995231628\n1 0.09710506019592285 0.9714999994277954\n2 0.08652983202934265 0.9754999995231628\n3 0.08377773129940033 0.9766999995231629\n4 0.08503483123779297 0.9750999995231628\n\n\n\n\nUnderstanding Convolution Arithmetic\nIn an input of size 64x1x28x28 the axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order (aka “channels-last”). Channels-last is faster for many models, so recently it’s become more common to see this as an option in PyTorch too.\nWe have 1 input channel, 4 output channels, and a 3×3 kernel.\n\nsimple_cnn\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (1): Sequential(\n    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): Flatten(start_dim=1, end_dim=-1)\n)\n\n\n\nsimple_cnn[0][0]\n\nConv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n\n\nconv1 = simple_cnn[0][0]\nconv1.weight.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\n\nconv1.bias.shape\n\ntorch.Size([4])\n\n\nThe receptive field is the area of an image that is involved in the calculation of a layer. conv-example.xlsx shows the calculation of two stride-2 convolutional layers using an MNIST digit. Here’s what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.\n\nThe blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Click trace precedents again:\n\nIn this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the receptive field\nThe deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer.",
    "crumbs": [
      "Blog",
      "Convolutions"
    ]
  },
  {
    "objectID": "convolutions.html#color-images",
    "href": "convolutions.html#color-images",
    "title": "Convolutions",
    "section": "Color Images",
    "text": "Color Images\nA colour picture is a rank-3 tensor:\n\nfrom torchvision.io import read_image\n\n\nim = read_image('images/grizzly.jpg')\nim.shape\n\ntorch.Size([3, 1000, 846])\n\n\n\nshow_image(im.permute(1,2,0));\n\n\n\n\n\n\n\n\n\n_,axs = plt.subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds','Greens','Blues')): show_image(255-bear, ax=ax, cmap=color)\n\n\n\n\n\n\n\n\n\nThese are then all added together, to produce a single number, for each grid location, for each output feature.\n\nWe have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels.",
    "crumbs": [
      "Blog",
      "Convolutions"
    ]
  },
  {
    "objectID": "stable_diffusion.html",
    "href": "stable_diffusion.html",
    "title": "Stable Diffusion with 🤗 Diffusers",
    "section": "",
    "text": "Pedro Cuenca, Patrick von Platen, Suraj Patil, Jeremy Howard\nChances are you’ll have seen examples in Twitter (and elsewhere) of images generated by typing a short description of the scene you want to create. This is the culmination of years of work in generative models. This notebook introduces Stable Diffusion, the highest-quality open source text to image model as of now. It’s also small enough to run in consumer GPUs rather than in a datacenter. We use the 🤗 Hugging Face 🧨 Diffusers library, which is currently our recommended library for using diffusion models.\nAs we’ll see during the course, understanding state-of-the-art generative models requires a deep understanding of many of the fundamental blocks in modern machine learning models. This notebook shows what Stable Diffusion can do and a glimpse of its main components.\nIf you open this notebook in Colab, or if you get type errors when generating your first image, please uncomment and run the following cell.\n# !pip install -Uq diffusers transformers fastcore",
    "crumbs": [
      "Blog",
      "Stable Diffusion with 🤗 Diffusers"
    ]
  },
  {
    "objectID": "stable_diffusion.html#using-stable-diffusion",
    "href": "stable_diffusion.html#using-stable-diffusion",
    "title": "Stable Diffusion with 🤗 Diffusers",
    "section": "Using Stable Diffusion",
    "text": "Using Stable Diffusion\nTo run Stable Diffusion on your computer you have to accept the model license. It’s an open CreativeML OpenRail-M license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The model card provides more details. If you do accept the license, you need to be a registered user in 🤗 Hugging Face Hub and use an access token for the code to work. You have two options to provide your access token:\n\nUse the huggingface-cli login command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer.\nOr use notebook_login() in a notebook, which does the same thing.\n\n\nimport logging\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\nlogging.disable(logging.WARNING)\n\ntorch.manual_seed(1)\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n\n\nStable Diffusion Pipeline\nStableDiffusionPipeline is an end-to-end diffusion inference pipeline that allows you to start generating images with just a few lines of code. Many Hugging Face libraries (along with other libraries such as scikit-learn) use the concept of a “pipeline” to indicate a sequence of steps that when combined complete some task. We’ll look at the individual steps of the pipeline later – for now though, let’s just use it to see what it can do.\nWhen we say “inference” we’re referring to using an existing model to generate samples (in this case, images), as opposed to “training” (or fine-tuning) models using new data.\nWe use from_pretrained to create the pipeline and download the pretrained weights. We indicate that we want to use the fp16 (half-precision) version of the weights, and we tell diffusers to expect the weights in that format. This allows us to perform much faster inference with almost no discernible difference in quality. The string passed to from_pretrained in this case (CompVis/stable-diffusion-v1-4) is the repo id of a pretrained pipeline hosted on Hugging Face Hub; it can also be a path to a directory containing pipeline weights. The weights for all the models in the pipeline will be downloaded and cached the first time you run this cell.\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\n\n\n\n\n# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16).to(\"cuda\")\n\nThe weights are cached in your home directory by default.\n\n!ls ~/.cache/huggingface/hub\n\nmodels--CompVis--stable-diffusion-v1-4         tmpbcmkgd67\nmodels--microsoft--deberta-v3-small        tmpfwxfw6s8\nmodels--openai--clip-vit-large-patch14         tmpgu008ffm\nmodels--pcuenq--jh_dreambooth_1000         tmphitm4qcc\nmodels--stabilityai--sd-vae-ft-ema         tmpkk9wq4ly\nmodels--timm--convnext_small.in12k_ft_in1k     tmpswqhersp\nmodels--timm--convnextv2_tiny.fcmae_ft_in22k_in1k  tmptew2r7yz\nmodels--timm--resnet18.a1_in1k             tmp_wkryxpx\nmodels--timm--resnet26.bt_in1k             version_diffusers_cache.txt\nmodels--timm--resnet26d.bt_in1k            version.txt\nmodels--timm--resnetv2_50.a1h_in1k\n\n\nWe are now ready to use the pipeline to start creating images.\nIf your GPU is not big enough to use pipe, run pipe.enable_attention_slicing()\nAs described in the docs:\n&gt; When this option is enabled, the attention module will split the input tensor in slices, to compute attention in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n#pipe.enable_attention_slicing()\n\n\nprompt = \"a photograph of an astronaut riding a horse\"\n\n\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1024)\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\nYou will have noticed that running the pipeline shows a progress bar with a certain number of steps. This is because Stable Diffusion is based on a progressive denoising algorithm that is able to create a convincing image starting from pure random noise. Models in this family are known as diffusion models. Here’s an example of the process (from random noise at top to progressively improved images towards the bottom) of a model drawing handwritten digits, which we’ll build from scratch ourselves later in the course.\n\n\n\nimage.png\n\n\n\ntorch.manual_seed(1024)\npipe(prompt, num_inference_steps=3).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1024)\npipe(prompt, num_inference_steps=16).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifier-Free Guidance\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nClassifier-Free Guidance is a method to increase the adherence of the output to the conditioning signal we used (the text).\nRoughly speaking, the larger the guidance the more the model tries to represent the text prompt. However, large values tend to produce less diversity. The default is 7.5, which represents a good compromise between variety and fidelity. This blog post goes into deeper details on how it works.\nWe can generate multiple images for the same prompt by simply passing a list of prompts instead of a string.\n\nnum_rows,num_cols = 4,4\nprompts = [prompt] * num_cols\n\n\nimages = concat(pipe(prompts, guidance_scale=g).images for g in [1.1,3,7,14])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_grid(images, rows=num_rows, cols=num_cols)\n\n\n\n\n\n\n\n\n\n\nNegative prompts\nNegative prompting refers to the use of another prompt (instead of a completely unconditioned generation), and scaling the difference between generations of that prompt and the conditioned generation.\n\ntorch.manual_seed(1000)\nprompt = \"Labrador in the style of Vermeer\"\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\npipe(prompt, negative_prompt=\"blue\").images[0]\n\n\n\n\n\n\n\n\n\n\n\nBy using the negative prompt we move more towards the direction of the positive prompt, effectively reducing the importance of the negative prompt in our composition.\n\n\nImage to Image\nEven though Stable Diffusion was trained to generate images, and optionally drive the generation using text conditioning, we can use the raw image diffusion process for other tasks.\nFor example, instead of starting from pure noise, we can start from an image an add a certain amount of noise to it. We are replacing the initial steps of the denoising and pretending our image is what the algorithm came up with. Then we continue the diffusion process from that state as usual.\nThis usually preserves the composition although details may change a lot. It’s great for sketches!\nThese operations (provide an initial image, add some noise to it and run diffusion from there) can be automatically performed by a special image to image pipeline: StableDiffusionImg2ImgPipeline. This is the source code for its __call__ method.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom fastdownload import FastDownload\n\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:267: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'`. This behavior is deprecated and will be removed in diffusers v1. One should use `variant='fp16'` instead. However, it appears that CompVis/stable-diffusion-v1-4 currently does not have the required variant filenames in the 'main' branch. \n The Diffusers team and community would be very grateful if you could open an issue: https://github.com/huggingface/diffusers/issues/new with the title 'CompVis/stable-diffusion-v1-4 is missing fp16 files' so that the correct variant file can be added.\n  warnings.warn(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\n\n\n\n\n\nWe’ll use as an example the following sketch created by user VigilanteRogue81.\n\nfile = './Data/grizzly0.jpg'\ninit_image = Image.open(file)\ninit_image\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt = \"Bear in a dense forest, photorealistic 4K\"\nimages = pipe(prompt=prompt, num_images_per_prompt=1, image=init_image, strength=0.8, num_inference_steps=50).images\n\n\n\n\n\nimage_grid(images, rows=1, cols=1)\n\n\n\n\n\n\n\n\nWhen we get a composition we like we can use it as the next seed for another prompt and further change the results. For example, let’s take the third image above and try to use it to generate something in the style of Van Gogh.\n\ninit_image = images[0]\n\n\ntorch.manual_seed(1000)\nprompt = \"Oil painting of bear in a forest by Van Gogh\"\nimages = pipe(prompt=prompt, num_images_per_prompt=1, image=init_image, strength=1, num_inference_steps=70).images\nimage_grid(images, rows=1, cols=1)\n\n\n\n\n\n\n\n\n\n\n\nCreative people use different tools in a process of iterative refinement to come up with the ideas they have in mind. Here’s a list with some suggestions to get started.\n\n\nFine-tuning\nHow we made the text-to-pokemon model at Lambda\n\nGirl with a pearl earring, Cute Obama creature, Donald Trump, Boris Johnson, Totoro, Hello Kitty\n\n\nTextual Inversion\nTextual inversion is a process where you can quickly “teach” a new word to the text model and train its embeddings close to some visual representation. This is achieved by adding a new token to the vocabulary, freezing the weights of all the models (except the text encoder), and train with a few representative images.\nThis is a schematic representation of the process by the authors of the paper.\n\n\n\nTextual Inversion diagram\n\n\n\nYou can train your own tokens with photos you provide using this training script or Google Colab notebook. There’s also a Colab notebook for inference, but we’ll show below the steps we have to follow to add a trained token to the vocabulary and make it work the pre-trained Stable Diffusion model.\nWe’ll try an example using embeddings trained for this style.\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16) \npipe = pipe.to(\"cuda\")\n\n\n\n\n\nembeds_url = \"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin\"\nembeds_path = FastDownload().download(embeds_url)\nembeds_dict = torch.load(str(embeds_path), map_location=\"cpu\")\n\nThe embeddings for the new token are stored in a small PyTorch pickled dictionary, whose key is the new text token that was trained. Since the encoder of our pipeline does not know about this term, we need to manually append it.\n\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\nnew_token, embeds = next(iter(embeds_dict.items()))\nembeds = embeds.to(text_encoder.dtype)\nnew_token\n\n'&lt;watercolor-portrait&gt;'\n\n\nWe add the new token to the tokenizer and the trained embeddings to the embeddings table.\n\nassert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\"\n\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\nnew_token_id = tokenizer.convert_tokens_to_ids(new_token)\ntext_encoder.get_input_embeddings().weight.data[new_token_id] = embeds\n\nWe can now run inference and refer to the style as if it was another word in the dictionary.\n\ntorch.manual_seed(1000)\nimage = pipe(\"Woman reading in the style of &lt;watercolor-portrait&gt;\").images[0]\nimage\n\n\n\n\n\n\n\n\n\n\n\n\n\nDreambooth\nDreambooth is a kind of fine-tuning that attempts to introduce new subjects by providing just a few images of the new subject. The goal is similar to that of Textual Inversion, but the process is different. Instead of creating a new token as Textual Inversion does, we select an existing token in the vocabulary (usually a rarely used one), and fine-tune the model for a few hundred steps to bring that token close to the images we provide. This is a regular fine-tuning process in which all modules are unfrozen.\nFor example, we fine-tuned a model with a prompt like \"photo of a sks person\", using the rare sks token to qualify the term person, and using photos of Jeremy as the targets. Let’s see how it works.\n\npipe = StableDiffusionPipeline.from_pretrained(\"pcuenq/jh_dreambooth_1000\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\n\n\n\n\ntorch.manual_seed(1000)\n\nprompt = \"Painting of sks person in the style of Paul Signac\"\nimages = pipe(prompt, num_images_per_prompt=4).images\nimage_grid(images, 1, 4)\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning with Dreambooth is finicky and sensitive to hyperparameters, as we are essentially asking the model to overfit the prompt to the supplied images. In some situations we observe problems such as catastrophic forgetting of the associated term (\"person\" in this case). The authors applied a technique called “prior preservation” to try to avoid it by performing a special regularization using other examples of the term, in addition to the ones we provided. The cool thing about this idea is that those examples can be easily generated by the pre-trained Stable Diffusion model itself! We did not use that method in the model we trained for the previous example.\nOther ideas that may work include: use EMA so that the final weights preserve some of the previous knowledge, use progressive learning rates for fine-tuning, or combine the best of Textual Inversion with Dreambooth. These could make for some interesting projects to try out!\nIf you want to train your own Dreambooth model, you can use this script or this Colab notebook.\n\n\nWhat is Stable Diffusion\nThere are three main components in latent diffusion.\n\nAn autoencoder (VAE).\nA U-Net.\nA text-encoder, e.g. CLIP’s Text Encoder.\n\nThe output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n\nPNDM scheduler (used by default)\nDDIM scheduler\nK-LMS scheduler\n\n\n\nLatents and callbacks\nStable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models.\nGeneral diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image. For a more detailed overview of how they work, check this colab.\nDiffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.\nLatent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\nThe Stable Diffusion pipeline can send intermediate latents to a callback function we provide. By running these latents through the image decoder (the vae component of the pipeline), we can see how the denoising process progresses and the image unfolds.\n\nvae = pipe.vae\nimages = []\n\ndef latents_callback(i, t, latents):\n    latents = 1 / 0.18215 * latents\n    image = vae.decode(latents).sample[0]\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.cpu().permute(1, 2, 0).numpy()\n    images.extend(pipe.numpy_to_pil(image))\n\nprompt = \"Portrait painting of Donald trump looking happy.\"\ntorch.manual_seed(9000)\nfinal_image = pipe(prompt, callback=latents_callback, callback_steps=12).images[0]\nimages.append(final_image)\nimage_grid(images, rows=1, cols=len(images))\n\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:736: FutureWarning: `callback` is deprecated and will be removed in version 1.0.0. Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\n  deprecate(\n/home/thekkel/mambaforge/envs/cfast/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:742: FutureWarning: `callback_steps` is deprecated and will be removed in version 1.0.0. Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\n  deprecate(\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is latent diffusion fast and efficient?\nSince the U-Net of latent diffusion models operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models. For example, the autoencoder used in Stable Diffusion has a reduction factor of 8 but uses 4 channels instead of 3. This means that an image of shape (3, 512, 512) becomes (4, 64, 64) in latent space, which requires 8 × 8 × 3/4 = 48 times less memory.\nThis is why it’s possible to generate 512 × 512 images so quickly, even on 16GB Colab GPUs!\n\ndel pipe",
    "crumbs": [
      "Blog",
      "Stable Diffusion with 🤗 Diffusers"
    ]
  },
  {
    "objectID": "stable_diffusion.html#looking-inside-the-pipeline",
    "href": "stable_diffusion.html#looking-inside-the-pipeline",
    "title": "Stable Diffusion with 🤗 Diffusers",
    "section": "Looking inside the pipeline",
    "text": "Looking inside the pipeline\nThe inference pipeline is just a small piece of code that plugs the components together and performs the inference loop. This is all there it to is.\nWe’ll go through the process of loading and plugging the pieces to see how we could have written it ourselves. We’ll start by loading all the modules that we need from their pretrained weights.\nFirst, we need the text encoder and the tokenizer. These come from the text portion of a standard CLIP model, so we’ll use the weights released by Open AI.\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\nNext we’ll load the vae and the unet. These are distinct models whose weights are stored inside folders of the Stable Diffusion repository. We can use the subfolder argument to refer to these locations.\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\n# Here we use a different VAE to the original release, which has been fine-tuned for more steps\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nTo make things a bit different, we’ll use another scheduler. The standard pipeline uses the PNDM Scheduler, but we’ll use Katherine Crowson’s excellent K-LMS scheduler.\nWe need to be careful to use the same noising schedule that was used during training. The schedule is defined by the number of noising steps and the amount of noise added at each step, which is derived from the beta parameters.\nIn the case of the k-LMS scheduler, this is how the betas evolve during the 1000 steps of the noising process used during training:\n\nbeta_start,beta_end = 0.00085,0.012\nplt.plot(torch.linspace(beta_start**0.5, beta_end**0.5, 1000) ** 2)\nplt.xlabel('Timestep')\nplt.ylabel('β');\n\n\n\n\n\n\n\n\n\nfrom diffusers import LMSDiscreteScheduler\n\n\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\nWe now define the parameters we’ll use for generation.\nIn contrast with the previous examples, we set num_inference_steps to 70 to get an even more defined image.\n\nprompt = [\"a photograph of an astronaut riding a horse\"]\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\nWe tokenize the prompt. The model requires the same number of tokens for every prompt, so padding is used to ensure we meet the required length.\n\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_input['input_ids']\n\ntensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\n\ntokenizer.decode(49407)\n\n'&lt;|endoftext|&gt;'\n\n\nThe attention mask uses zero to represent tokens we are not interested in. These are all of the padding tokens.\n\ntext_input['attention_mask']\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])\n\n\nThe text encoder gives us the embeddings for the text prompt we used.\n\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe also get the embeddings required to perform unconditional generation, which is achieved with an empty string: the model is free to go in whichever direction it wants as long as it results in a reasonably-looking image. These embeddings will be applied to apply classifier-free guidance.\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nuncond_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nFor classifier-free guidance, we need to do two forward passes. One with the conditioned input (text_embeddings), and another with the unconditional embeddings (uncond_embeddings). In practice, we can concatenate both into a single batch to avoid doing two forward passes.\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\nTo start the denoising process, we start from pure Gaussian (normal) noise. These are our initial latents.\n\ntorch.manual_seed(100)\nlatents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\nlatents = latents.to(\"cuda\").half()\nlatents.shape\n\n/tmp/ipykernel_60841/2512694209.py:3: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\n\n\ntorch.Size([1, 4, 64, 64])\n\n\n4×64×64 is the input shape. The decoder will later transform this latent representation into a 3×512×512 image after the denoising process is complete.\nNext, we initialize the scheduler with our chosen num_inference_steps. This will prepare the internal state to be used during denoising.\n\nscheduler.set_timesteps(num_inference_steps)\n\nWe scale the initial noise by the standard deviation required by the scheduler. This value will depend on the particular scheduler we use.\n\nlatents = latents * scheduler.init_noise_sigma\n\nWe are ready to write the denoising loop. The timesteps go from 999 to 0 (1000 steps that were used during training) following a particular schedule.\n\nscheduler.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3044, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9131, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0869, 260.6087, 246.1304, 231.6522, 217.1739, 202.6956,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3044, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000])\n\n\n\nscheduler.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7477,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\n\nplt.plot(scheduler.timesteps, scheduler.sigmas[:-1]);\n\n\n\n\n\n\n\n\n\nfrom tqdm.auto import tqdm\n\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    input = torch.cat([latents] * 2)\n    input = scheduler.scale_model_input(input, t)\n\n    # predict the noise residual\n    with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    pred_uncond, pred_text = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n    # compute the \"previous\" noisy sample\n    latents = scheduler.step(pred, t, latents).prev_sample\n\n\n\n\nAfter this process complets our latents contain the denoised representation of the image. We use the vae decoder to convert it back to pixel space.\n\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\nAnd finally, let’s convert the image to PIL so we can display it.\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)\n\n\n\n\n\n\n\n\n\nJust the code\n\nprompts = [\n    'a photograph of an astronaut riding a horse',\n    'an oil painting of an astronaut riding a horse in the style of grant wood'\n]\n\n\ntext_input = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\n\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"] * len(prompts), padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nemb = torch.cat([uncond_embeddings, text_embeddings])\n\n\ntorch.manual_seed(100)\ng = guidance_scale\n\n\nlatents = torch.randn((len(prompts), unet.in_channels, height//8, width//8))\nscheduler.set_timesteps(num_inference_steps)\nlatents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n\n/tmp/ipykernel_60841/2293748509.py:2: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((len(prompts), unet.in_channels, height//8, width//8))\n\n\n\nfor i,ts in enumerate(tqdm(scheduler.timesteps)):\n    inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n    with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n    pred = u + g*(t-u)\n    latents = scheduler.step(pred, ts, latents).prev_sample\n\n\n\n\n\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\nres = (image / 2 + 0.5).clamp(0, 1)\n\n\nimage = res[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)\n\n\n\n\n\n\n\n\n\nimage = res[1].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)\n\n\n\n\n\n\n\n\n\n\nPut it in functions\n\ndef text_enc(prompts, maxlen=None):\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\ndef mk_img(t):\n    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\n\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nimages = mk_samples(prompts)\n\n/tmp/ipykernel_60841/2156907493.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\nfrom IPython.display import display\n\n\nfor img in images: display(mk_img(img))",
    "crumbs": [
      "Blog",
      "Stable Diffusion with 🤗 Diffusers"
    ]
  },
  {
    "objectID": "tabular.html",
    "href": "tabular.html",
    "title": "Tabular Modeling Deep Dive",
    "section": "",
    "text": "Tabular modeling takes data in the form of a table (like a spreadsheet or CSV). The objective is to predict the value in one column based on the values in the other columns. In this chapter we will not only look at deep learning but also more general machine learning techniques like random forests, as they can give better results depending on your problem.\nWe will look at how we should preprocess and clean the data as well as how to interpret the result of our models after training, but first, we will see how we can feed columns that contain categories into a model that expects numbers by using embeddings.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#categorical-embeddings",
    "href": "tabular.html#categorical-embeddings",
    "title": "Tabular Modeling Deep Dive",
    "section": "Categorical Embeddings",
    "text": "Categorical Embeddings\nIn tabular data some columns may contain numerical data, like “age,” while others contain string values, like “sex.” The numerical data can be directly fed to the model (with some optional preprocessing), but the other columns need to be converted to numbers. Since the values in those correspond to different categories, we often call this type of variables categorical variables. The first type are called continuous variables.\n\njargon: Continuous and Categorical Variables: Continuous variables are numerical data, such as “age,” that can be directly fed to the model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning (even if they’re stored as numbers).\n\nAt the end of 2015, the Rossmann sales competition ran on Kaggle. Competitors were given a wide range of information about various stores in Germany, and were tasked with trying to predict sales on a number of days. The goal was to help the company to manage stock properly and be able to satisfy demand without holding unnecessary inventory. The official training set provided a lot of information about the stores. It was also permitted for competitors to use additional data, as long as that data was made public and available to all participants.\nOne of the gold medalists used deep learning, in one of the earliest known examples of a state-of-the-art deep learning tabular model. Their method involved far less feature engineering, based on domain knowledge, than those of the other gold medalists. The paper, “Entity Embeddings of Categorical Variables” describes their approach. In an online-only chapter on the book’s website we show how to replicate it from scratch and attain the same accuracy shown in the paper. In the abstract of the paper the authors (Cheng Guo and Felix Berkhahn) say:\n\n: Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables… [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit… As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.\n\nWe have already noticed all of these points when we built our collaborative filtering model. We can clearly see that these insights go far beyond just collaborative filtering, however.\nThe paper also points out that (as we discussed in the last chapter) an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer. The authors used the diagram in &lt;&gt; to show this equivalence. Note that “dense layer” is a term with the same meaning as “linear layer,” and the one-hot encoding layers represent inputs.\n\nThe insight is important because we already know how to train linear layers, so this shows that from the point of view of the architecture and our training algorithm the embedding layer is just another layer. We also saw this in practice in the last chapter, when we built a collaborative filtering neural network that looks exactly like this diagram.\nWhere we analyzed the embedding weights for movie reviews, the authors of the entity embeddings paper analyzed the embedding weights for their sales prediction model. What they found was quite amazing, and illustrates their second key insight. This is that the embedding transforms the categorical variables into inputs that are both continuous and meaningful.\nThe images in &lt;&gt; illustrate these ideas. They are based on the approaches used in the paper, along with some analysis we have added.\n\nOn the left is a plot of the embedding matrix for the possible values of the State category. For a categorical variable we call the possible values of the variable its “levels” (or “categories” or “classes”), so here one level is “Berlin,” another is “Hamburg,” etc. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data, yet the model itself learned where they must be, based only on the behavior of store sales!\nDo you remember how we talked about distance between embeddings? The authors of the paper plotted the distance between store embeddings against the actual geographic distance between the stores (see &lt;&gt;). They found that they matched very closely!\n\nWe’ve even tried plotting the embeddings for days of the week and months of the year, and found that days and months that are near each other on the calendar ended up close as embeddings too, as shown in &lt;&gt;.\n\nWhat stands out in these two examples is that we provide the model fundamentally categorical data about discrete entities (e.g., German states or days of the week), and then the model learns an embedding for these entities that defines a continuous notion of distance between them. Because the embedding distance was learned based on real patterns in the data, that distance tends to match up with our intuitions.\nIn addition, it is valuable in its own right that embeddings are continuous, because models are better at understanding continuous variables. This is unsurprising considering models are built of many continuous parameter weights and continuous activation values, which are updated via gradient descent (a learning algorithm for finding the minimums of continuous functions).\nAnother benefit is that we can combine our continuous embedding values with truly continuous input data in a straightforward manner: we just concatenate the variables, and feed the concatenation into our first dense layer. In other words, the raw categorical data is transformed by an embedding layer before it interacts with the raw continuous input data. This is how fastai and Guo and Berkhahn handle tabular models containing continuous and categorical variables.\nAn example using this concatenation approach is how Google does its recommendations on Google Play, as explained in the paper “Wide & Deep Learning for Recommender Systems”. &lt;&gt; illustrates.\n\nInterestingly, the Google team actually combined both approaches we saw in the previous chapter: the dot product (which they call cross product) and neural network approaches.\nLet’s pause for a moment. So far, the solution to all of our modeling problems has been: train a deep learning model. And indeed, that is a pretty good rule of thumb for complex unstructured data like images, sounds, natural language text, and so forth. Deep learning also works very well for collaborative filtering. But it is not always the best starting point for analyzing tabular data.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#beyond-deep-learning",
    "href": "tabular.html#beyond-deep-learning",
    "title": "Tabular Modeling Deep Dive",
    "section": "Beyond Deep Learning",
    "text": "Beyond Deep Learning\nMost machine learning courses will throw dozens of different algorithms at you, with a brief technical description of the math behind them and maybe a toy example. You’re left confused by the enormous range of techniques shown and have little practical understanding of how to apply them.\nThe good news is that modern machine learning can be distilled down to a couple of key techniques that are widely applicable. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n\nEnsembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies)\nMultilayered neural networks learned with SGD (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language)\n\nAlthough deep learning is nearly always clearly superior for unstructured data, these two approaches tend to give quite similar results for many kinds of structured data. But ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning. They have also been popular for quite a lot longer than deep learning, so there is a more mature ecosystem of tooling and documentation around them.\nMost importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles. There are tools and methods for answering the pertinent questions, like: Which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation?\nTherefore, ensembles of decision trees are our first approach for analyzing a new tabular dataset.\nThe exception to this guideline is when the dataset meets one of these conditions:\n\nThere are some high-cardinality categorical variables that are very important (“cardinality” refers to the number of discrete levels representing categories, so a high-cardinality categorical variable is something like a zip code, which can take on thousands of possible levels).\nThere are some columns that contain data that would be best understood with a neural network, such as plain text data.\n\nIn practice, when we deal with datasets that meet these exceptional conditions, we always try both decision tree ensembles and deep learning to see which works best. It is likely that deep learning will be a useful approach in our example of collaborative filtering, as we have at least two high-cardinality categorical variables: the users and the movies. But in practice things tend to be less cut-and-dried, and there will often be a mixture of high- and low-cardinality categorical variables and continuous variables.\nEither way, it’s clear that we are going to need to add decision tree ensembles to our modeling toolbox!\nUp to now we’ve used PyTorch and fastai for pretty much all of our heavy lifting. But these libraries are mainly designed for algorithms that do lots of matrix multiplication and derivatives (that is, stuff like deep learning!). Decision trees don’t depend on these operations at all, so PyTorch isn’t much use.\nInstead, we will be largely relying on a library called scikit-learn (also known as sklearn). Scikit-learn is a popular library for creating machine learning models, using approaches that are not covered by deep learning. In addition, we’ll need to do some tabular data processing and querying, so we’ll want to use the Pandas library. Finally, we’ll also need NumPy, since that’s the main numeric programming library that both sklearn and Pandas rely on.\nWe don’t have time to do a deep dive into all these libraries in this book, so we’ll just be touching on some of the main parts of each. For a far more in depth discussion, we strongly suggest Wes McKinney’s Python for Data Analysis (O’Reilly). Wes is the creator of Pandas, so you can be sure that the information is accurate!\nFirst, let’s gather the data we will use.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#the-dataset",
    "href": "tabular.html#the-dataset",
    "title": "Tabular Modeling Deep Dive",
    "section": "The Dataset",
    "text": "The Dataset\nThe dataset we use in this chapter is from the Blue Book for Bulldozers Kaggle competition, which has the following description: “The goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.”\nThis is a very common type of dataset and prediction problem, similar to what you may see in your project or workplace. The dataset is available for download on Kaggle, a website that hosts data science competitions.\n\nKaggle Competitions\nKaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills. There is nothing like getting hands-on practice and receiving real-time feedback to help you improve your skills.\nKaggle provides:\n\nInteresting datasets\nFeedback on how you’re doing\nA leaderboard to see what’s good, what’s possible, and what’s state-of-the-art\nBlog posts by winning contestants sharing useful tips and techniques\n\nUntil now all our datasets have been available to download through fastai’s integrated dataset system. However, the dataset we will be using in this chapter is only available from Kaggle. Therefore, you will need to register on the site, then go to the page for the competition. On that page click “Rules,” then “I Understand and Accept.” (Although the competition has finished, and you will not be entering it, you still have to agree to the rules to be allowed to download the data.)\nThe easiest way to download Kaggle datasets is to use the Kaggle API. You can install this using pip by running this in a notebook cell:\n!pip install kaggle\nYou need an API key to use the Kaggle API; to get one, click on your profile picture on the Kaggle website, and choose My Account, then click Create New API Token. This will save a file called kaggle.json to your PC. You need to copy this key on your GPU server. To do so, open the file you downloaded, copy the contents, and paste them in the following cell in the notebook associated with this chapter (e.g., creds = '{\"username\":\"xxx\",\"key\":\"xxx\"}'):\n\ncreds = ''\n\nThen execute this cell (this only needs to be run once):\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\nelse: \n    print(\"kaggle account already exists\")\n\nkaggle account already exists\n\n\nNow you can download datasets from Kaggle! Pick a path to download the dataset to:\nAnd use the Kaggle API to download the dataset to that path, and extract it:\n\nimport os\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\n\n\nname = 'bluebook-for-bulldozers'\ndatapath = Path('./Data')\nkaggle_competition_download(name, folderpath = datapath)\n\nfile exists\n\n\n\npath = f'{datapath}/{name}'\npath\n\n'Data/bluebook-for-bulldozers'\n\n\nNow that we have downloaded our dataset, let’s take a look at it!\n\n\nLook at the Data\nKaggle provides information about some of the fields of our dataset. The Data explains that the key fields in train.csv are:\n\nSalesID:: The unique identifier of the sale.\nMachineID:: The unique identifier of a machine. A machine can be sold multiple times.\nsaleprice:: What the machine sold for at auction (only provided in train.csv).\nsaledate:: The date of the sale.\n\nIn any sort of data science work, it’s important to look at your data directly to make sure you understand the format, how it’s stored, what types of values it holds, etc. Even if you’ve read a description of the data, the actual data may not be what you expect. We’ll start by reading the training set into a Pandas DataFrame. Generally it’s a good idea to specify low_memory=False unless Pandas actually runs out of memory and returns an error. The low_memory parameter, which is True by default, tells Pandas to only look at a few rows of data at a time to figure out what type of data is in each column. This means that Pandas can actually end up using different data type for different rows, which generally leads to data processing errors or model training problems later.\nLet’s load our data and have a look at the columns:\n\ndf = pd.read_csv(f'{path}/TrainAndValid.csv', low_memory=False)\ndf.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThat’s a lot of columns for us to look at! Try looking through the dataset to get a sense of what kind of information is in each one. We’ll shortly see how to “zero in” on the most interesting bits.\nAt this point, a good next step is to handle ordinal columns. This refers to columns containing strings or similar, but where those strings have a natural ordering. For instance, here are the levels of ProductSize:\n\ndf['ProductSize'].unique()\n\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)\n\n\nWe can tell Pandas about a suitable ordering of these levels like so:\n\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\n\n\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True)\n\n0            NaN\n1         Medium\n2            NaN\n3          Small\n4            NaN\n           ...  \n412693      Mini\n412694      Mini\n412695      Mini\n412696      Mini\n412697      Mini\nName: ProductSize, Length: 412698, dtype: category\nCategories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']\n\n\nThe most important data column is the dependent variable—that is, the one we want to predict. Recall that a model’s metric is a function that reflects how good the predictions are. It’s important to note what metric is being used for a project. Generally, selecting the metric is an important part of the project setup. In many cases, choosing a good metric will require more than just selecting a variable that already exists. It is more like a design process. You should think carefully about which metric, or set of metrics, actually measures the notion of model quality that matters to you. If no variable represents that metric, you should see if you can build the metric from the variables that are available.\nHowever, in this case Kaggle tells us what metric to use: root mean squared log error (RMSLE) between the actual and predicted auction prices. We need do only a small amount of processing to use this: we take the log of the prices, so that rmse of that value will give us what we ultimately need:\n\ndep_var = 'SalePrice'\n\n\ndf[dep_var] = np.log(df[dep_var])\n\nWe are now ready to explore our first machine learning algorithm for tabular data: decision trees.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#decision-trees",
    "href": "tabular.html#decision-trees",
    "title": "Tabular Modeling Deep Dive",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision tree ensembles, as the name suggests, rely on decision trees. So let’s start there! A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch, as shown in &lt;&gt;. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.\n\nThis sequence of questions is now a procedure for taking any data item, whether an item from the training set or a new one, and assigning that item to a group. Namely, after asking and answering the questions, we can say the item belongs to the same group as all the other training data items that yielded the same set of answers to the questions. But what good is this? The goal of our model is to predict values for items, not to assign them into groups from the training dataset. The value is that we can now assign a prediction value for each of these groups—for regression, we take the target mean of the items in the group.\nLet’s consider how we find the right questions to ask. Of course, we wouldn’t want to have to create all these questions ourselves—that’s what computers are for! The basic steps to train a decision tree can be written down very easily:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it.\n\nAlthough this is an easy enough algorithm to implement yourself (and it is a good exercise to do so), we can save some time by using the implementation built into sklearn.\nFirst, however, we need to do a little data preparation.\n\nA: Here’s a productive question to ponder. If you consider that the procedure for defining a decision tree essentially chooses one sequence of splitting questions about variables, you might ask yourself, how do we know this procedure chooses the correct sequence? The rule is to choose the splitting question that produces the best split (i.e., that most accurately separates the items into two distinct categories), and then to apply the same rule to the groups that split produces, and so on. This is known in computer science as a “greedy” approach. Can you imagine a scenario in which asking a “less powerful” splitting question would enable a better split down the road (or should I say down the trunk!) and lead to a better result overall?\n\n\nHandling Dates\nThe first piece of data preparation we need to do is to enrich our representation of dates. The fundamental basis of the decision tree that we just described is bisection— dividing a group into two. We look at the ordinal variables and divide up the dataset based on whether the variable’s value is greater (or lower) than a threshold, and we look at the categorical variables and divide up the dataset based on whether the variable’s level is a particular level. So this algorithm has a way of dividing up the dataset based on both ordinal and categorical data.\nBut how does this apply to a common data type, the date? You might want to treat a date as an ordinal value, because it is meaningful to say that one date is greater than another. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others in a way that that is often relevant to the systems we are modeling.\nIn order to help our algorithm handle dates intelligently, we’d like our model to know more than whether a date is more recent or less recent than another. We might want our model to make decisions based on that date’s day of the week, on whether a day is a holiday, on what month it is in, and so forth. To do this, we replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful.\nfastai comes with a function that will do this for us—we just have to pass a column name that contains dates:\n\ndf = add_datepart(df, 'saledate')\n\nLet’s do the same for the test set while we’re there:\n\ndf_test = pd.read_csv(f'{path}/Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\nWe can see that there are now lots of new columns in our DataFrame:\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\nThis is a good first step, but we will need to do a bit more cleaning. For this, we will use fastai objects called TabularPandas and TabularProc.\n\n\nUsing TabularPandas and TabularProc\nA second piece of preparatory processing is to be sure we can handle strings and missing data. Out of the box, sklearn cannot do either. Instead we will use fastai’s class TabularPandas, which wraps a Pandas DataFrame and provides a few conveniences. To populate a TabularPandas, we will use two TabularProcs, Categorify and FillMissing. A TabularProc is like a regular Transform, except that:\n\nIt returns the exact same object that’s passed to it, after modifying the object in place.\nIt runs the transform once, when data is first passed in, rather than lazily as the data is accessed.\n\nCategorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing. These two transforms are needed for nearly every tabular dataset you will use, so this is a good starting point for your data processing:\n\nprocs = [Categorify, FillMissing]\n\nTabularPandas will also handle splitting the dataset into training and validation sets for us. However we need to be very careful about our validation set. We want to design it so that it is like the test set Kaggle will use to judge the contest.\nRecall the distinction between a validation set and a test set, as discussed in &lt;&gt;. A validation set is data we hold back from training in order to ensure that the training process does not overfit on the training data. A test set is data that is held back even more deeply, from us ourselves, in order to ensure that we don’t overfit on the validation data, as we explore various model architectures and hyperparameters.\nWe don’t get to see the test set. But we do want to define our validation data so that it has the same sort of relationship to the training data as the test set will have.\nIn some cases, just randomly choosing a subset of your data points will do that. This is not one of those cases, because it is a time series.\nIf you look at the date range represented in the test set, you will discover that it covers a six-month period from May 2012, which is later in time than any date in the training set. This is a good design, because the competition sponsor will want to ensure that a model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time than the training set. The Kaggle training data ends in April 2012, so we will define a narrower training dataset which consists only of the Kaggle training data from before November 2011, and we’ll define a validation set consisting of data from after November 2011.\nTo do this we use np.where, a useful function that returns (as the first element of a tuple) the indices of all True values:\n\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\nTabularPandas needs to be told which columns are continuous and which are categorical. We can handle that automatically using the helper function cont_cat_split:\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nA TabularPandas behaves a lot like a fastai Datasets object, including providing train and valid attributes:\n\nlen(to.train),len(to.valid)\n\n(404710, 7988)\n\n\nWe can see that the data is still displayed as strings for categories (we only show a few columns here because the full table is too big to fit on a page):\n\nto.show(3)\n\n\n\n\n\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nsaleIs_month_end\nsaleIs_month_start\nsaleIs_quarter_end\nsaleIs_quarter_start\nsaleIs_year_end\nsaleIs_year_start\nauctioneerID_na\nMachineHoursCurrentMeter_na\nSalesID\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nsaleYear\nsaleMonth\nsaleWeek\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\nSalePrice\n\n\n\n\n0\nLow\n521D\n521\nD\n#na#\n#na#\n#na#\nWheel Loader - 110.0 to 120.0 Horsepower\nAlabama\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139246\n999089\n3157\n121\n3.0\n2004\n68.0\n2006\n11\n46\n16\n3\n320\n1.163635e+09\n11.097410\n\n\n1\nLow\n950FII\n950\nF\nII\n#na#\nMedium\nWheel Loader - 150.0 to 175.0 Horsepower\nNorth Carolina\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\n23.5\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139248\n117657\n77\n121\n3.0\n1996\n4640.0\n2004\n3\n13\n26\n4\n86\n1.080259e+09\n10.950807\n\n\n2\nHigh\n226\n226\n#na#\n#na#\n#na#\n#na#\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\nNew York\nSSL\nSkid Steer Loaders\n#na#\nOROPS\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nAuxiliary\n#na#\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nStandard\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139249\n434808\n7009\n121\n3.0\n2001\n2838.0\n2004\n2\n9\n26\n3\n57\n1.077754e+09\n9.210340\n\n\n\n\n\n\nto1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure'], [], y_names=dep_var, splits=splits)\nto1.show(3)\n\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\nSalePrice\n\n\n\n\n0\nAlabama\nWL\n#na#\nEROPS w AC\n11.097410\n\n\n1\nNorth Carolina\nWL\n#na#\nEROPS w AC\n10.950807\n\n\n2\nNew York\nSSL\n#na#\nOROPS\n9.210340\n\n\n\n\n\nHowever, the underlying items are all numeric:\n\nto.items.head(3)\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nModelID\n...\nsaleIs_year_start\nsaleElapsed\nauctioneerID_na\nMachineHoursCurrentMeter_na\n\n\n\n\n0\n1139246\n11.097410\n999089\n3157\n...\n1\n1.163635e+09\n1\n1\n\n\n1\n1139248\n10.950807\n117657\n77\n...\n1\n1.080259e+09\n1\n1\n\n\n2\n1139249\n9.210340\n434808\n7009\n...\n1\n1.077754e+09\n1\n1\n\n\n\n\n3 rows × 67 columns\n\n\n\n\nto1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)\n\n\n\n\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\n\n\n\n\n0\n1\n6\n0\n3\n\n\n1\n33\n6\n0\n3\n\n\n2\n32\n3\n0\n6\n\n\n\n\n\n\n\nThe conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column, so there’s no particular meaning to the numbers in categorical columns after conversion. The exception is if you first convert a column to a Pandas ordered category (as we did for ProductSize earlier), in which case the ordering you chose is used. We can see the mapping by looking at the classes attribute:\n\nto.classes['ProductSize']\n\n['#na#', 'Compact', 'Large', 'Large / Medium', 'Medium', 'Mini', 'Small']\n\n\nSince it takes a minute or so to process the data to get to this point, we should save it—that way in the future we can continue our work from here without rerunning the previous steps. fastai provides a save method that uses Python’s pickle system to save nearly any Python object:\n\nsave_pickle(f'{path}/to.pkl',to)\n\nTo read this back later, you would type:\nto = (path/'to.pkl').load()\nNow that all this preprocessing is done, we are ready to create a decision tree.\n\n\nCreating the Decision Tree\nTo begin, we define our independent and dependent variables:\n\nto = load_pickle(f'{path}/to.pkl')\n\n\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\nNow that our data is all numeric, and there are no missing values, we can create a decision tree:\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs.values, y);\n\nTo keep it simple, we’ve told sklearn to just create four leaf nodes. To see what it’s learned, we can display the tree:\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\n\n\n\n\nUnderstanding this picture is one of the best ways to understand decision trees, so we will start at the top and explain each part step by step.\nThe top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see m_rmse, or a root mean squared error, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group—that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the coupler_system column.\nMoving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where coupler_system was greater than 0.5.\nThe bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where coupler_system was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about coupler_system predicts an average value of 9.21 versus 10.1.\nReturning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on coupler_system and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly.\nWe can show the same information using Terence Parr’s powerful dtreeviz library:\n\nimport dtreeviz\n\n\nsamp_idx = np.random.permutation(len(y))[:500]\nm1 = dtreeviz.model(m, X_train=xs.iloc[samp_idx], y_train=y.iloc[samp_idx], feature_names=xs.columns, target_name=dep_var);\nm1.view();\n\nThis shows a chart of the distribution of the data for each split point. We can clearly see that there’s a problem with our YearMade data: there are bulldozers made in the year 1000, apparently! Presumably this is actually just a missing value code (a value that doesn’t otherwise appear in the data and that is used as a placeholder in cases where a value is missing). For modeling purposes, 1000 is fine, but as you can see this outlier makes visualization of the values we are interested in more difficult. So, let’s replace it with 1950:\n\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\nThat change makes the split much clearer in the tree visualization, even although it doesn’t actually change the result of the model in any significant way. This is a great example of how resilient decision trees are to data issues!\n\nxs.columns\n\nIndex(['UsageBand', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls',\n       'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end',\n       'saleIs_quarter_start', 'saleIs_year_end', 'saleIs_year_start',\n       'auctioneerID_na', 'MachineHoursCurrentMeter_na', 'SalesID',\n       'MachineID', 'ModelID', 'datasource', 'auctioneerID', 'YearMade',\n       'MachineHoursCurrentMeter', 'saleYear', 'saleMonth', 'saleWeek',\n       'saleDay', 'saleDayofweek', 'saleDayofyear', 'saleElapsed'],\n      dtype='object')\n\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs.values, y)\n\nm1 = dtreeviz.model(m, X_train=xs.iloc[samp_idx], y_train=y.iloc[samp_idx], feature_names=xs.columns, target_name=dep_var, class_names=[\"perish\", \"survive\"])\n\nm1.view()\n\n\n\n\n\n\n\n\nLet’s now have the decision tree algorithm build a bigger tree. Here, we are not passing in any stopping criteria such as max_leaf_nodes:\n\nm = DecisionTreeRegressor()\nm.fit(xs.values, y);\n\nWe’ll create a little function to check the root mean squared error of our model (m_rmse), since that’s how the competition was judged:\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nm_rmse(m, xs.values, y)\n\n0.0\n\n\nSo, our model is perfect, right? Not so fast… remember we really need to check the validation set, to ensure we’re not overfitting:\n\nm_rmse(m, valid_xs.values, valid_y)\n\n0.332239\n\n\nOops—it looks like we might be overfitting pretty badly. Here’s why:\n\nm.get_n_leaves(), len(xs)\n\n(324338, 404710)\n\n\nWe’ve got nearly as many leaf nodes as data points! That seems a little over-enthusiastic. Indeed, sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 auction records:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs.values, to.train.y)\nm_rmse(m, xs.values, y), m_rmse(m, valid_xs.values, valid_y)\n\n(0.243049, 0.308857)\n\n\nThat looks much better. Let’s check the number of leaves again:\n\nm.get_n_leaves()\n\n12432\n\n\nMuch more reasonable!\n\nA: Here’s my intuition for an overfitting decision tree with more leaf nodes than data items. Consider the game Twenty Questions. In that game, the chooser secretly imagines an object (like, “our television set”), and the guesser gets to pose 20 yes or no questions to try to guess what the object is (like “Is it bigger than a breadbox?”). The guesser is not trying to predict a numerical value, but just to identify a particular object out of the set of all imaginable objects. When your decision tree has more leaves than there are possible objects in your domain, then it is essentially a well-trained guesser. It has learned the sequence of questions needed to identify a particular data item in the training set, and it is “predicting” only by describing that item’s value. This is a way of memorizing the training set—i.e., of overfitting.\n\nBuilding a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees).\nSo how do we get the best of both worlds? We’ll show you right after we handle an important missing detail: how to handle categorical variables.\n\n\nCategorical Variables\nIn the previous chapter, when working with deep learning networks, we dealt with categorical variables by one-hot encoding them and feeding them to an embedding layer. The embedding layer helped the model to discover the meaning of the different levels of these variables (the levels of a categorical variable do not have an intrinsic meaning, unless we manually specify an ordering using Pandas). In a decision tree, we don’t have embeddings layers—so how can these untreated categorical variables do anything useful in a decision tree? For instance, how could something like a product code be used?\nThe short answer is: it just works! Think about a situation where there is one product code that is far more expensive at auction than any other one. In that case, any binary split will result in that one product code being in some group, and that group will be more expensive than the other group. Therefore, our simple decision tree building algorithm will choose that split. Later during training the algorithm will be able to further split the subgroup that contains the expensive product code, and over time, the tree will home in on that one expensive product.\nIt is also possible to use one-hot encoding to replace a single categorical variable with multiple one-hot-encoded columns, where each column represents a possible level of the variable. Pandas has a get_dummies method which does just that.\nHowever, there is not really any evidence that such an approach improves the end result. So, we generally avoid it where possible, because it does end up making your dataset harder to work with. In 2019 this issue was explored in the paper “Splitting on Categorical Predictors in Random Forests” by Marvin Wright and Inke König, which said:\n\n: The standard approach for nominal predictors is to consider all \\(2^{k-1} − 1\\) 2-partitions of the k predictor categories. However, this exponential relationship produces a large number of potential splits to be evaluated, increasing computational complexity and restricting the possible number of categories in most implementations. For binary classification and regression, it was shown that ordering the predictor categories in each split leads to exactly the same splits as the standard approach. This reduces computational complexity because only k − 1 splits have to be considered for a nominal predictor with k categories.\n\nNow that you understand how decisions tree work, it’s time for the best-of-both-worlds solution: random forests.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#random-forests",
    "href": "tabular.html#random-forests",
    "title": "Tabular Modeling Deep Dive",
    "section": "Random Forests",
    "text": "Random Forests\nIn 1994 Berkeley professor Leo Breiman, one year after his retirement, published a small technical report called “Bagging Predictors”, which turned out to be one of the most influential ideas in modern machine learning. The report began:\n\n: Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions… The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.\n\nHere is the procedure that Breiman is proposing:\n\nRandomly choose a subset of the rows of your data (i.e., “bootstrap replicates of your learning set”).\nTrain a model using this subset.\nSave that model, and then return to step 1 a few times.\nThis will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model’s predictions.\n\nThis procedure is known as “bagging.” It is based on a deep and important insight: although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors, therefore, is: zero! So if we take the average of all of the models’ predictions, then we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. This is an extraordinary result—it means that we can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of the data, and averaging its predictions.\nIn 2001 Leo Breiman went on to demonstrate that this approach to building models, when applied to decision tree building algorithms, was particularly powerful. He went even further than just randomly choosing rows for each model’s training, but also randomly selected from a subset of columns when choosing each split in each decision tree. He called this method the random forest. Today it is, perhaps, the most widely used and practically important machine learning method.\nIn essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. Bagging is a particular approach to “ensembling,” or combining the results of multiple models together. To see how it works in practice, let’s get started on creating our own random forest!\n\n# pip install —pre -f https://sklearn-nightly.scdn8.secure.raxcdn.com scikit-learn —U\n\n\nCreating a Random Forest\nWe can create a random forest just like we created a decision tree, except now, we are also specifying parameters that indicate how many trees should be in the forest, how we should subset the data items (the rows), and how we should subset the fields (the columns).\nIn the following function definition n_estimators defines the number of trees we want, max_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point (where 0.5 means “take half the total number of columns”). We can also specify when to stop splitting the tree nodes, effectively limiting the depth of the tree, by including the same min_samples_leaf parameter we used in the last section. Finally, we pass n_jobs=-1 to tell sklearn to use all our CPUs to build the trees in parallel. By creating a little function for this, we can more quickly try different variations in the rest of this chapter:\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\n\nm = rf(xs.values, y);\n\nOur validation RMSE is now much improved over our last result produced by the DecisionTreeRegressor, which made just one tree using all the available data:\n\nm_rmse(m, xs.values, y), m_rmse(m, valid_xs.values, valid_y)\n\n(0.171371, 0.233223)\n\n\nOne of the most important properties of random forests is that they aren’t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train—the more trees you have, the more accurate the model will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4 both tend to work well, although sklearn’s defaults work well too.\nThe sklearn docs show an example of the effects of different max_features choices, with increasing numbers of trees. In the plot, the blue plot line uses the fewest features and the green line uses the most (it uses all the features). As you can see in &lt;&gt;, the models with the lowest error result from using a subset of features but with a larger number of trees.\n\nTo see the impact of n_estimators, let’s get the predictions from each individual tree in our forest (these are in the estimators_ attribute):\n\npreds = np.stack([t.predict(valid_xs.values) for t in m.estimators_])\n\nAs you can see, preds.mean(0) gives the same results as our random forest:\n\nr_mse(preds.mean(0), valid_y)\n\n0.233223\n\n\nLet’s see what happens to the RMSE as we add more and more trees. As you can see, the improvement levels off quite a bit after around 30 trees:\n\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\n\n\n\n\nThe performance on our validation set is worse than on our training set. But is that because we’re overfitting, or because the validation set covers a different time period, or a bit of both? With the existing information we’ve seen, we can’t tell. However, random forests have a very clever trick called out-of-bag (OOB) error that can help us with this (and more!).\n\n\nOut-of-Bag Error\nRecall that in a random forest, each tree is trained on a different subset of the training data. The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row’s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set.\n\nA: My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were not selected for that tree’s training.\n\nThis is particularly beneficial in cases where we have only a small amount of training data, as it allows us to see whether our model generalizes without removing items to create a validation set. The OOB predictions are available in the oob_prediction_ attribute. Note that we compare them to the training labels, since this is being calculated on trees using the training set.\n\nr_mse(m.oob_prediction_, y)\n\n0.211234\n\n\nWe can see that our OOB error is much lower than our validation set error. This means that something else is causing that error, in addition to normal generalization error. We’ll discuss the reasons for this later in this chapter.\nThis is one way to interpret our model’s predictions—let’s focus on more of those now.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#model-interpretation",
    "href": "tabular.html#model-interpretation",
    "title": "Tabular Modeling Deep Dive",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nFor tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary, as we vary these columns?\n\nAs we will see, random forests are particularly well suited to answering these questions. Let’s start with the first one!\n\nTree Variance for Prediction Confidence\nWe saw how the model averages the individual tree’s predictions to get an overall prediction—that is, an estimate of the value. But how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions across the trees, instead of just the mean. This tells us the relative confidence of predictions. In general, we would want to be more cautious of using the results for rows where trees give very different results (higher standard deviations), compared to cases where they are more consistent (lower standard deviations).\nIn the earlier section on creating a random forest, we saw how to get predictions over the validation set, using a Python list comprehension to do this for each tree in the forest:\n\npreds = np.stack([t.predict(valid_xs.values) for t in m.estimators_])\n\n\npreds.shape\n\n(40, 7988)\n\n\nNow we have a prediction for every tree and every auction (40 trees and 7,988 auctions) in the validation set.\nUsing this we can get the standard deviation of the predictions over all the trees, for each auction:\n\npreds_std = preds.std(0)\n\nHere are the standard deviations for the predictions for the first five auctions—that is, the first five rows of the validation set:\n\npreds_std[:5]\n\narray([0.2000169 , 0.08355874, 0.113672  , 0.2747    , 0.12065141])\n\n\nAs you can see, the confidence in the predictions varies widely. For some auctions, there is a low standard deviation because the trees agree. For others it’s higher, as the trees don’t agree. This is information that would be useful in a production setting; for instance, if you were using this model to decide what items to bid on at auction, a low-confidence prediction might cause you to look more carefully at an item before you made a bid.\n\n\nFeature Importance\nIt’s not normally enough just to know that a model can make accurate predictions—we also want to know how it’s making predictions. feature importance gives us insight into this. We can get these directly from sklearn’s random forest by looking in the feature_importances_ attribute. Here’s a simple function we can use to pop them into a DataFrame and sort them:\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\nThe feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) YearMade and ProductSize being at the top of the list:\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n57\nYearMade\n0.166375\n\n\n30\nCoupler_System\n0.113599\n\n\n6\nProductSize\n0.103802\n\n\n7\nfiProductClassDesc\n0.078686\n\n\n3\nfiSecondaryDesc\n0.054542\n\n\n54\nModelID\n0.052919\n\n\n65\nsaleElapsed\n0.050521\n\n\n31\nGrouser_Tracks\n0.041514\n\n\n12\nEnclosure\n0.039451\n\n\n32\nHydraulics_Flow\n0.035355\n\n\n\n\n\n\n\nA plot of the feature importances shows the relative importances more clearly:\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\n\n\nThe way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1.\n\n\nRemoving Low-Importance Variables\nIt seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than 0.005:\n\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n\n22\n\n\nWe can retrain our model using just this subset of the columns:\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\n\nm = rf(xs_imp, y)\n\nAnd here’s the result:\n\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n(0.180965, 0.231633)\n\n\nOur accuracy is about the same, but we have far fewer columns to study:\n\nlen(xs.columns), len(xs_imp.columns)\n\n(66, 22)\n\n\nWe’ve found that generally the first step to improving a model is simplifying it—78 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain.\nThis also makes our feature importance plot easier to interpret. Let’s look at it again:\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\n\n\nOne thing that makes this harder to interpret is that there seem to be some variables with very similar meanings: for example, ProductGroup and ProductGroupDesc. Let’s try to remove any redundent features.\n\n\nRemoving Redundant Features\nLet’s start with:\n\ncluster_columns(xs_imp)\n\n\n\n\n\n\n\n\nIn this chart, the pairs of columns that are most similar are the ones that were merged together early, far from the “root” of the tree at the left. Unsurprisingly, the fields ProductGroup and ProductGroupDesc were merged quite early, as were saleYear and saleElapsed and fiModelDesc and fiBaseModel. These might be so closely correlated they are practically synonyms for each other.\n\nnote: Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. (Feel free to skip over this minor detail though, since it’s not going to come up again in the book!)\n\nLet’s try removing some of these closely related features to see if the model can be simplified without impacting the accuracy. First, we create a function that quickly trains a random forest and returns the OOB score, by using a lower max_samples and higher min_samples_leaf. The OOB score is a number returned by sklearn that ranges between 1.0 for a perfect model and 0.0 for a random model. (In statistics it’s called R^2, although the details aren’t important for this explanation.) We don’t need it to be very accurate—we’re just going to use it to compare different models, based on removing some of the possibly redundant columns:\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nHere’s our baseline:\n\nget_oob(xs_imp)\n\n0.8760739540611289\n\n\nNow we try removing each of our potentially redundant variables, one at a time:\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n\n{'saleYear': 0.8742959821922331,\n 'saleElapsed': 0.8698149904307536,\n 'ProductGroupDesc': 0.8755334280543031,\n 'fiModelDesc': 0.8731600875380836,\n 'fiBaseModel': 0.8743137539613115,\n 'Hydraulics_Flow': 0.8761516062768563,\n 'Grouser_Tracks': 0.875639118713338,\n 'Coupler_System': 0.8753447081392794}\n\n\nNow let’s try dropping multiple variables. We’ll drop one from each of the tightly aligned pairs we noticed earlier. Let’s see what that does:\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8742868087085606\n\n\nLooking good! This is really not much worse than the model with all the fields. Let’s create DataFrames without these columns, and save them:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\nsave_pickle(f'{path}/xs_final.pkl', xs_final)\nsave_pickle(f'{path}/valid_xs_final.pkl', valid_xs_final)\n\nWe can load them back later with:\n\nxs_final = load_pickle(f'{path}/xs_final.pkl')\nvalid_xs_final = load_pickle(f'{path}/valid_xs_final.pkl')\n\nNow we can check our RMSE again, to confirm that the accuracy hasn’t substantially changed.\n\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n(0.18273, 0.233207)\n\n\nBy focusing on the most important variables, and removing some redundant ones, we’ve greatly simplified our model. Now, let’s see how those variables affect our predictions using partial dependence plots.\n\n\nPartial Dependence\nAs we’ve seen, the two most important predictors are ProductSize and YearMade. We’d like to understand the relationship between these predictors and sale price. It’s a good idea to first check the count of values per category (provided by the Pandas value_counts method), to see how common each category is:\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\n\n\n\n\n\n\nThe largrest group is #na#, which is the label fastai applies to missing values.\nLet’s do the same thing for YearMade. Since this is a numeric feature, we’ll need to draw a histogram, which groups the year values into a few discrete bins:\n\nax = valid_xs_final['YearMade'].hist()\n\n\n\n\n\n\n\n\nOther than the special value 1950 which we used for coding missing year values, most of the data is from after 1990.\nNow we’re ready to look at partial dependence plots. Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?\nFor instance, how does YearMade impact sale price, all other things being equal?\nTo answer this question, we can’t just take the average sale price for each YearMade. The problem with that approach is that many other things vary from year to year as well, such as which products are sold, how many products have air-conditioning, inflation, and so forth. So, merely averaging over all the auctions that have the same YearMade would also capture the effect of how every other field also changed along with YearMade and how that overall change affected price.\nInstead, what we do is replace every single value in the YearMade column with 1950, and then calculate the predicted sale price for every auction, and take the average over all auctions. Then we do the same for 1951, 1952, and so forth until our final year of 2011. This isolates the effect of only YearMade (even if it does so by averaging over some imagined records where we assign a YearMade value that might never actually exist alongside some other values).\n\nA: If you are philosophically minded it is somewhat dizzying to contemplate the different kinds of hypotheticality that we are juggling to make this calculation. First, there’s the fact that every prediction is hypothetical, because we are not noting empirical data. Second, there’s the point that we’re not merely interested in asking how sale price would change if we changed YearMade and everything else along with it. Rather, we’re very specifically asking, how sale price would change in a hypothetical world where only YearMade changed. Phew! It is impressive that we can ask such questions. I recommend Judea Pearl and Dana Mackenzie’s recent book on causality, The Book of Why (Basic Books), if you’re interested in more deeply exploring formalisms for analyzing these subtleties.\n\nWith these averages, we can then plot each of these years on the x-axis, and each of the predictions on the y-axis. This, finally, is a partial dependence plot. Let’s take a look:\n\n?PartialDependenceDisplay\n\nObject `PartialDependenceDisplay` not found.\n\n\nLooking first of all at the YearMade plot, and specifically at the section covering the years after 1990 (since as we noted this is where we have the most data), we can see a nearly linear relationship between year and price. Remember that our dependent variable is after taking the logarithm, so this means that in practice there is an exponential increase in price. This is what we would expect: depreciation is generally recognized as being a multiplicative factor over time, so, for a given sale date, varying year made ought to show an exponential relationship with sale price.\nThe ProductSize partial plot is a bit concerning. It shows that the final group, which we saw is for missing values, has the lowest price. To use this insight in practice, we would want to find out why it’s missing so often, and what that means. Missing values can sometimes be useful predictors—it entirely depends on what causes them to be missing. Sometimes, however, they can indicate data leakage.\n\n\nData Leakage\nIn the paper “Leakage in Data Mining: Formulation, Detection, and Avoidance”, Shachar Kaufman, Saharon Rosset, and Claudia Perlich describe leakage as:\n\n: The introduction of information about the target of a data mining problem, which should not be legitimately available to mine from. A trivial example of leakage would be a model that uses the target itself as an input, thus concluding for example that ‘it rains on rainy days’. In practice, the introduction of this illegitimate information is unintentional, and facilitated by the data collection, aggregation and preparation process.\n\nThey give as an example:\n\n: A real-life business intelligence project at IBM where potential customers for certain products were identified, among other things, based on keywords found on their websites. This turned out to be leakage since the website content used for training had been sampled at the point in time where the potential customer has already become a customer, and where the website contained traces of the IBM products purchased, such as the word ‘Websphere’ (e.g., in a press release about the purchase or a specific product feature the client uses).\n\nData leakage is subtle and can take many forms. In particular, missing values often represent data leakage.\nFor instance, Jeremy competed in a Kaggle competition designed to predict which researchers would end up receiving research grants. The information was provided by a university and included thousands of examples of research projects, along with information about the researchers involved and data on whether or not each grant was eventually accepted. The university hoped to be able to use the models developed in this competition to rank which grant applications were most likely to succeed, so it could prioritize its processing.\nJeremy used a random forest to model the data, and then used feature importance to find out which features were most predictive. He noticed three surprising things:\n\nThe model was able to correctly predict who would receive grants over 95% of the time.\nApparently meaningless identifier columns were the most important predictors.\nThe day of week and day of year columns were also highly predictive; for instance, the vast majority of grant applications dated on a Sunday were accepted, and many accepted grant applications were dated on January 1.\n\nFor the identifier columns, one partial dependence plot per column showed that when the information was missing the application was almost always rejected. It turned out that in practice, the university only filled out much of this information after a grant application was accepted. Often, for applications that were not accepted, it was just left blank. Therefore, this information was not something that was actually available at the time that the application was received, and it would not be available for a predictive model—it was data leakage.\nIn the same way, the final processing of successful applications was often done automatically as a batch at the end of the week, or the end of the year. It was this final processing date which ended up in the data, so again, this information, while predictive, was not actually available at the time that the application was received.\nThis example showcases the most practical and simple approaches to identifying data leakage, which are to build a model and then:\n\nCheck whether the accuracy of the model is too good to be true.\nLook for important predictors that don’t make sense in practice.\nLook for partial dependence plot results that don’t make sense in practice.\n\nThinking back to our bear detector, this mirrors the advice that we provided in &lt;&gt;—it is often a good idea to build a model first and then do your data cleaning, rather than vice versa. The model can help you identify potentially problematic data issues.\nIt can also help you identify which factors influence specific predictions, with tree interpreters.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#extrapolation-and-neural-networks",
    "href": "tabular.html#extrapolation-and-neural-networks",
    "title": "Tabular Modeling Deep Dive",
    "section": "Extrapolation and Neural Networks",
    "text": "Extrapolation and Neural Networks\nA problem with random forests, like all machine learning or deep learning algorithms, is that they don’t always generalize well to new data. We will see in which situations neural networks generalize better, but first, let’s look at the extrapolation problem that random forests have.\n\nThe Extrapolation Problem\n\nnp.random.seed(42)\n\nLet’s consider the simple task of making predictions from 40 data points showing a slightly noisy linear relationship:\n\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\n\n\n\n\n\n\nAlthough we only have a single independent variable, sklearn expects a matrix of independent variables, not a single vector. So we have to turn our vector into a matrix with one column. In other words, we have to change the shape from [40] to [40,1]. One way to do that is with the unsqueeze method, which adds a new unit axis to a tensor at the requested dimension:\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n\n(torch.Size([40]), torch.Size([40, 1]))\n\n\nA more flexible approach is to slice an array or tensor with the special value None, which introduces an additional unit axis at that location:\n\nx_lin[:,None].shape\n\ntorch.Size([40, 1])\n\n\nWe can now create a random forest for this data. We’ll use only the first 30 rows to train the model:\n\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n\nThen we’ll test the model on the full dataset. The blue dots are the training data, and the red dots are the predictions:\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\n\n\n\n\nWe have a big problem! Our predictions outside of the domain that our training data covered are all too low. Why do you suppose this is?\nRemember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low.\nBut the problem extends beyond time variables. Random forests are not able to extrapolate outside of the types of data they have seen, in a more general sense. That’s why we need to make sure our validation set does not contain out-of-domain data.\n\n\nFinding Out-of-Domain Data\nSometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There’s actually an easy way to figure this out, which is to use a random forest!\nBut in this case we don’t use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let’s combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance:\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n6\nsaleElapsed\n0.910266\n\n\n11\nSalesID\n0.073707\n\n\n14\nMachineID\n0.012246\n\n\n0\nYearMade\n0.000813\n\n\n9\nfiModelDesc\n0.000535\n\n\n5\nModelID\n0.000471\n\n\n\n\n\n\n\nThis shows that there are three columns that differ significantly between the training and validation sets: saleElapsed, SalesID, and MachineID. It’s fairly obvious why this is the case for saleElapsed: it’s the number of days between the start of the dataset and each row, so it directly encodes the date. The difference in SalesID suggests that identifiers for auction sales might increment over time. MachineID suggests something similar might be happening for individual items sold in those auctions.\nLet’s get a baseline of the original random forest model’s RMSE, then see what the effect is of removing each of these columns in turn:\n\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.231001\nSalesID 0.230214\nsaleElapsed 0.235865\nMachineID 0.231447\n\n\nIt looks like we should be able to remove SalesID and MachineID without losing any accuracy. Let’s check:\n\ntime_vars = ['SalesID','MachineID', 'saleElapsed']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n0.275148\n\n\nRemoving these variables has slightly improved the model’s accuracy; but more importantly, it should make it more resilient over time, and easier to maintain and understand. We recommend that for all datasets you try building a model where your dependent variable is is_valid, like we did here. It can often uncover subtle domain shift issues that you may otherwise miss.\nOne thing that might help in our case is to simply avoid using old data. Often, old data shows relationships that just aren’t valid any more. Let’s try just using the most recent few years of the data:\n\nxs['saleYear'].hist();\n\n\n\n\n\n\n\n\nHere’s the result of training on this subset:\n\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n(0.243451, 0.241082)\n\n\nIt’s a tiny bit better, which shows that you shouldn’t always just use your entire dataset; sometimes a subset can be better.\nLet’s see if using a neural network helps.\n\n\nUsing a Neural Network\nWe can use the same approach to build a neural network model. Let’s first replicate the steps we took to set up the TabularPandas object:\n\n?add_datepart\n\n\nSignature: add_datepart(df, field_name, prefix=None, drop=True, time=False)\nDocstring: Helper function that adds columns relevant to a date in the column `field_name` of `df`.\nFile:      ~/mambaforge/envs/cfast/lib/python3.11/site-packages/fastai/tabular/core.py\nType:      function\n\n\n\n\ndf_nn = pd.read_csv(f'{path}/./TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn['saledate'] = pd.to_datetime(df_nn['saledate'])\n\nWe can leverage the work we did to trim unwanted columns in the random forest by using the same set of columns for our neural network:\n\nxs_final_time.columns\n\nIndex(['YearMade', 'Coupler_System', 'ProductSize', 'fiProductClassDesc',\n       'fiSecondaryDesc', 'ModelID', 'Enclosure', 'Hydraulics_Flow',\n       'fiModelDesc', 'fiModelDescriptor', 'Hydraulics', 'ProductGroup',\n       'Drive_System', 'Tire_Size', 'Track_Type'],\n      dtype='object')\n\n\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\nCategorical columns are handled very differently in neural networks, compared to decision tree approaches. As we saw in &lt;&gt;, in a neural net a great way to handle categorical variables is by using embeddings. To create embeddings, fastai needs to determine which columns should be treated as categorical variables. It does this by comparing the number of distinct levels in the variable to the value of the max_card parameter. If it’s lower, fastai will treat the variable as categorical. Embedding sizes larger than 10,000 should generally only be used after you’ve tested whether there are better ways to group the variable, so we’ll use 9,000 as our max_card:\n\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nIn this case, there’s one variable that we absolutely do not want to treat as categorical: the saleElapsed variable. A categorical variable cannot, by definition, extrapolate outside the range of values that it has seen, but we want to be able to predict auction sale prices in the future. Let’s verify that cont_cat_split did the correct thing.\n\ncont_nn\n\n[]\n\n\nLet’s take a look at the cardinality of each of the categorical variables that we have chosen so far:\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nCoupler_System           2\nProductSize              6\nfiProductClassDesc      74\nfiSecondaryDesc        177\nModelID               5281\nEnclosure                6\nHydraulics_Flow          3\nfiModelDesc           5059\nfiModelDescriptor      140\nHydraulics              12\nProductGroup             6\nDrive_System             4\nTire_Size               17\nTrack_Type               2\ndtype: int64\n\n\nThe fact that there are two variables pertaining to the “model” of the equipment, both with similar very high cardinalities, suggests that they may contain similar, redundant information. Note that we would not necessarily see this when analyzing redundant features, since that relies on similar variables being sorted in the same order (that is, they need to have similarly named levels). Having a column with 5,000 levels means needing 5,000 columns in our embedding matrix, which would be nice to avoid if possible. Let’s see what the impact of removing one of these model columns has on the random forest:\n\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.243052, 0.241177)\n\n\nThere’s minimal impact, so we will remove it as a predictor for our neural network:\n\ncat_nn.remove('fiModelDescriptor')\n\nWe can create our TabularPandas object in the same way as when we created our random forest, with one very important addition: normalization. A random forest does not need any normalization—the tree building procedure cares only about the order of values in a variable, not at all about how they are scaled. But as we have seen, a neural network definitely does care about this. Therefore, we add the Normalize processor when we build our TabularPandas object:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/fastai/tabular/core.py:279: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n\n\nTabular models and data don’t generally require much GPU RAM, so we can use larger batch sizes:\n\ndls = to_nn.dataloaders(1024)\n\nAs we’ve discussed, it’s a good idea to set y_range for regression models, so let’s find the min and max of our dependent variable:\n\ny = to_nn.train.y\ny.min(),y.max()\n\n(8.465899, 11.863583)\n\n\nWe can now create the Learner to create this tabular model. As usual, we use the application-specific learner function, to take advantage of its application-customized defaults. We set the loss function to MSE, since that’s what this competition uses.\nBy default, for tabular data fastai creates a neural network with two hidden layers, with 200 and 100 activations, respectively. This works quite well for small datasets, but here we’ve got quite a large dataset, so we increase the layer sizes to 500 and 250:\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0002754228771664202)\n\n\n\n\n\n\n\n\n\nThere’s no need to use fine_tune, so we’ll train with fit_one_cycle for a few epochs and see how it looks:\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.104603\n0.080641\n00:04\n\n\n1\n0.094649\n0.074813\n00:04\n\n\n2\n0.090272\n0.074764\n00:04\n\n\n3\n0.086864\n0.072307\n00:04\n\n\n4\n0.084164\n0.072127\n00:04\n\n\n\n\n\nWe can use our r_mse function to compare the result to the random forest result we got earlier:\n\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n\n\n\n\n0.268565\n\n\nIt’s quite a bit better than the random forest (although it took longer to train, and it’s fussier about hyperparameter tuning).\nBefore we move on, let’s save our model in case we want to come back to it again later:\n\nlearn.save('nn')\n\nPath('models/nn.pth')\n\n\n\n\nSidebar: fastai’s Tabular Classes\nIn fastai, a tabular model is simply a model that takes columns of continuous or categorical data, and predicts a category (a classification model) or a continuous value (a regression model). Categorical independent variables are passed through an embedding, and concatenated, as we saw in the neural net we used for collaborative filtering, and then continuous variables are concatenated as well.\nThe model created in tabular_learner is an object of class TabularModel. Take a look at the source for tabular_learner now (remember, that’s tabular_learner?? in Jupyter). You’ll see that like collab_learner, it first calls get_emb_sz to calculate appropriate embedding sizes (you can override these by using the emb_szs parameter, which is a dictionary containing any column names you want to set sizes for manually), and it sets a few other defaults. Other than that, it just creates the TabularModel, and passes that to TabularLearner (note that TabularLearner is identical to Learner, except for a customized predict method).\nThat means that really all the work is happening in TabularModel, so take a look at the source for that now. With the exception of the BatchNorm1d and Dropout layers (which we’ll be learning about shortly), you now have the knowledge required to understand this whole class. Take a look at the discussion of EmbeddingNN at the end of the last chapter. Recall that it passed n_cont=0 to TabularModel. We now can see why that was: because there are zero continuous variables (in fastai the n_ prefix means “number of,” and cont is an abbreviation for “continuous”).\n\n\nEnd sidebar\nAnother thing that can help with generalization is to use several models and average their predictions—a technique, as mentioned earlier, known as ensembling.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#ensembling",
    "href": "tabular.html#ensembling",
    "title": "Tabular Modeling Deep Dive",
    "section": "Ensembling",
    "text": "Ensembling\nThink back to the original reasoning behind why random forests work so well: each tree has errors, but those errors are not correlated with each other, so the average of those errors should tend towards zero once there are enough trees. Similar reasoning could be used to consider averaging the predictions of models trained using different algorithms.\nIn our case, we have two very different models, trained using very different algorithms: a random forest, and a neural network. It would be reasonable to expect that the kinds of errors that each one makes would be quite different. Therefore, we might expect that the average of their predictions would be better than either one’s individual predictions.\nAs we saw earlier, a random forest is itself an ensemble. But we can then include a random forest in another ensemble—an ensemble of the random forest and the neural network! While ensembling won’t make the difference between a successful and an unsuccessful modeling process, it can certainly add a nice little boost to any models that you have built.\nOne minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types: PyTorch gives us a rank-2 tensor (i.e, a column matrix), whereas NumPy gives us a rank-1 array (a vector). squeeze removes any unit axes from a tensor, and to_np converts it into a NumPy array:\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\nThis gives us a better result than either model achieved on its own:\n\nr_mse(ens_preds,valid_y)\n\n0.246987\n\n\nIn fact, this result is better than any score shown on the Kaggle leaderboard. It’s not directly comparable, however, because the Kaggle leaderboard uses a separate dataset that we do not have access to. Kaggle does not allow us to submit to this old competition to find out how we would have done, but our results certainly look very encouraging!\n\nBoosting\nSo far our approach to ensembling has been to use bagging, which involves combining many models (each trained on a different data subset) together by averaging them. As we saw, when this is applied to decision trees, this is called a random forest.\nThere is another important approach to ensembling, called boosting, where we add models instead of averaging them. Here is how boosting works:\n\nTrain a small model that underfits your dataset.\nCalculate the predictions in the training set for this model.\nSubtract the predictions from the targets; these are called the “residuals” and represent the error for each point in the training set.\nGo back to step 1, but instead of using the original targets, use the residuals as the targets for the training.\nContinue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse.\n\nUsing this approach, each new tree will be attempting to fit the error of all of the previous trees combined. Because we are continually creating new residuals, by subtracting the predictions of each new tree from the residuals from the previous tree, the residuals will get smaller and smaller.\nTo make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. There are many models following this basic approach, and many names for the same models. Gradient boosting machines (GBMs) and gradient boosted decision trees (GBDTs) are the terms you’re most likely to come across, or you may see the names of specific libraries implementing these; at the time of writing, XGBoost is the most popular.\nNote that, unlike with random forests, with this approach there is nothing to stop us from overfitting. Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set.\nWe are not going to go into detail on how to train a gradient boosted tree ensemble here, because the field is moving rapidly, and any guidance we give will almost certainly be outdated by the time you read this. As we write this, sklearn has just added a HistGradientBoostingRegressor class that provides excellent performance. There are many hyperparameters to tweak for this class, and for all gradient boosted tree methods we have seen. Unlike random forests, gradient boosted trees are extremely sensitive to the choices of these hyperparameters; in practice, most people use a loop that tries a range of different hyperparameters to find the ones that work best.\nOne more technique that has gotten great results is to use embeddings learned by a neural net in a machine learning model.\n\n\nCombining Embeddings with Other Methods\nThe abstract of the entity embedding paper we mentioned at the start of this chapter states: “the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead”. It includes the very interesting table in &lt;&gt;.\n\nThis is showing the mean average percent error (MAPE) compared among four different modeling techniques, three of which we have already seen, along with k-nearest neighbors (KNN), which is a very simple baseline method. The first numeric column contains the results of using the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings instead of the raw categories.\nThis is a really important result, because it shows that you can get much of the performance improvement of a neural network without actually having to use a neural network at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble.\nThese embeddings need not even be necessarily learned separately for each model or task in an organization. Instead, once a set of embeddings are learned for some column for some task, they could be stored in a central place, and reused across multiple models. In fact, we know from private communication with other practitioners at large companies that this is already happening in many places.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#conclusion-our-advice-for-tabular-modeling",
    "href": "tabular.html#conclusion-our-advice-for-tabular-modeling",
    "title": "Tabular Modeling Deep Dive",
    "section": "Conclusion: Our Advice for Tabular Modeling",
    "text": "Conclusion: Our Advice for Tabular Modeling\nWe have dicussed two approaches to tabular modeling: decision tree ensembles and neural networks. We’ve also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises:\n\nRandom forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.\nGradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.\nNeural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.\n\nWe suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it’s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.\nFrom that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better.",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "tabular.html#questionnaire",
    "href": "tabular.html#questionnaire",
    "title": "Tabular Modeling Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a continuous variable?\nWhat is a categorical variable?\nProvide two of the words that are used for the possible values of a categorical variable.\nWhat is a “dense layer”?\nHow do entity embeddings reduce memory usage and speed up neural networks?\nWhat kinds of datasets are entity embeddings especially useful for?\nWhat are the two main families of machine learning algorithms?\nWhy do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\nSummarize what a decision tree algorithm does.\nWhy is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\nShould you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\nWhat is pickle and what is it useful for?\nHow are mse, samples, and values calculated in the decision tree drawn in this chapter?\nHow do we deal with outliers, before building a decision tree?\nHow do we handle categorical variables in a decision tree?\nWhat is bagging?\nWhat is the difference between max_samples and max_features when creating a random forest?\nIf you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\nIn the section “Creating a Random Forest”, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest?\nWhat is “out-of-bag-error”?\nMake a list of reasons why a model’s validation set error might be worse than the OOB error. How could you test your hypotheses?\nExplain why random forests are well suited to answering each of the following question:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors?\nHow do predictions vary as we vary these columns?\n\nWhat’s the purpose of removing unimportant variables?\nWhat’s a good type of plot for showing tree interpreter results?\nWhat is the “extrapolation problem”?\nHow can you tell if your test or validation set is distributed in a different way than your training set?\nWhy do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\nWhat is “boosting”?\nHow could we use embeddings with a random forest? Would we expect this to help?\nWhy might we not always use a neural net for tabular modeling?\n\n\nFurther Research\n\nPick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard.\nImplement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise.\nUse the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw.\nExplain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers).",
    "crumbs": [
      "Blog",
      "Tabular Modeling Deep Dive"
    ]
  },
  {
    "objectID": "imgnet_tiny-200.html",
    "href": "imgnet_tiny-200.html",
    "title": "Tiny Imagenet",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES']='1'\nimport shutil,timm,os,torch,random,datasets,math\nimport fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\nfrom glob import glob\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.training import *\nfrom fastprogress import progress_bar\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "imgnet_tiny-200.html#data-processing",
    "href": "imgnet_tiny-200.html#data-processing",
    "title": "Tiny Imagenet",
    "section": "Data processing",
    "text": "Data processing\n\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'tiny-imagenet-200'\n\n\nurl = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\nif not path.exists():\n    path_zip = fc.urlsave(url, path_data)\n    shutil.unpack_archive('data/tiny-imagenet-200.zip', 'data')\n\n\nbs = 512\n\n\nclass TinyDS:\n    def __init__(self, path):\n        self.path = Path(path)\n        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return self.files[i],Path(self.files[i]).parent.parent.name\n\n\ntds = TinyDS(path/'train')\n\n\npath_anno = path/'val'/'val_annotations.txt'\nanno = dict(o.split('\\t')[:2] for o in path_anno.read_text().splitlines())\n\n\nclass TinyValDS(TinyDS):\n    def __getitem__(self, i): return self.files[i],anno[os.path.basename(self.files[i])]\n\n\nvds = TinyValDS(path/'val')\n\n\nclass TfmDS:\n    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n    def __len__(self): return len(self.ds)\n    def __getitem__(self, i):\n        x,y = self.ds[i]\n        return self.tfmx(x),self.tfmy(y)\n\n\nid2str = (path/'wnids.txt').read_text().splitlines()\nstr2id = {v:k for k,v in enumerate(id2str)}\n\n\nxmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))\n\n\ndef tfmy(y): return tensor(str2id[y])\n\n\ndef denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clip(0,1)\n\n\nall_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\nsynsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}\n\n\ndef tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\n\n\nnfs = (32,64,128,256,512,1024)\n\n\nopt_func = partial(optim.AdamW, eps=1e-5)\n\n\nnbks=(3,3,2,2,1)\n\n\ndef conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True):\n    layers = []\n    if norm: layers.append(norm(ni))\n    if act : layers.append(act())\n    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n    return nn.Sequential(*layers)\n\ndef _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n    return nn.Sequential(conv(ni, nf, stride=1     , act=act, norm=norm, ks=ks),\n                         conv(nf, nf, stride=stride, act=act, norm=norm, ks=ks))\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n        super().__init__()\n        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n\ndef res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n    return nn.Sequential(*[\n        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n        for i in range(n_bk)])\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n\n\naug_tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     T.TrivialAugmentWide())\n\nnorm_tfm = T.Normalize(xmean, xstd)\nerase_tfm = RandErase()\n\n\nfrom PIL import Image\n\n\ndef tfmx(x, aug=False):\n    x = Image.open(x).convert('RGB')\n    if aug: x = aug_tfms(x)\n    x = TF.to_tensor(x)\n    x = norm_tfm(x)\n    if aug: x = erase_tfm(x[None])[0]\n    return x\n\n\ntfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\ndef get_model(): return get_dropmodel(drop=0.1)\n\n\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=[SingleBatchCB(), DeviceCB()])\nlearn.fit(1)\nxb,yb = learn.batch\nshow_images(denorm(xb.cpu())[:9], imsize=2.5)\n\n\n\n\n\n\n\n\n\nepochs = 200\nlr = 0.1\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nlearn = Learner(get_model(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.025\n5.057\n0\ntrain\n\n\n0.043\n4.807\n0\neval\n\n\n0.049\n4.768\n1\ntrain\n\n\n0.065\n4.521\n1\neval\n\n\n0.079\n4.500\n2\ntrain\n\n\n0.111\n4.182\n2\neval\n\n\n0.116\n4.207\n3\ntrain\n\n\n0.159\n3.827\n3\neval\n\n\n0.152\n3.950\n4\ntrain\n\n\n0.193\n3.615\n4\neval\n\n\n0.185\n3.721\n5\ntrain\n\n\n0.242\n3.316\n5\neval\n\n\n0.220\n3.516\n6\ntrain\n\n\n0.286\n3.062\n6\neval\n\n\n0.246\n3.361\n7\ntrain\n\n\n0.306\n2.962\n7\neval\n\n\n0.270\n3.209\n8\ntrain\n\n\n0.309\n3.030\n8\neval\n\n\n0.293\n3.089\n9\ntrain\n\n\n0.344\n2.780\n9\neval\n\n\n0.312\n2.971\n10\ntrain\n\n\n0.355\n2.753\n10\neval\n\n\n0.335\n2.863\n11\ntrain\n\n\n0.384\n2.627\n11\neval\n\n\n0.350\n2.779\n12\ntrain\n\n\n0.381\n2.646\n12\neval\n\n\n0.365\n2.699\n13\ntrain\n\n\n0.388\n2.619\n13\neval\n\n\n0.379\n2.624\n14\ntrain\n\n\n0.409\n2.542\n14\neval\n\n\n0.390\n2.571\n15\ntrain\n\n\n0.410\n2.514\n15\neval\n\n\n0.405\n2.504\n16\ntrain\n\n\n0.405\n2.563\n16\neval\n\n\n0.410\n2.461\n17\ntrain\n\n\n0.426\n2.462\n17\neval\n\n\n0.419\n2.422\n18\ntrain\n\n\n0.442\n2.390\n18\neval\n\n\n0.427\n2.378\n19\ntrain\n\n\n0.451\n2.348\n19\neval\n\n\n0.434\n2.346\n20\ntrain\n\n\n0.413\n2.565\n20\neval\n\n\n0.439\n2.311\n21\ntrain\n\n\n0.371\n2.858\n21\neval\n\n\n0.447\n2.283\n22\ntrain\n\n\n0.390\n2.787\n22\neval\n\n\n0.449\n2.278\n23\ntrain\n\n\n0.434\n2.446\n23\neval\n\n\n0.452\n2.255\n24\ntrain\n\n\n0.444\n2.351\n24\neval\n\n\n0.455\n2.235\n25\ntrain\n\n\n0.438\n2.416\n25\neval\n\n\n0.458\n2.231\n26\ntrain\n\n\n0.457\n2.326\n26\neval\n\n\n0.459\n2.215\n27\ntrain\n\n\n0.455\n2.349\n27\neval\n\n\n0.464\n2.204\n28\ntrain\n\n\n0.442\n2.400\n28\neval\n\n\n0.464\n2.200\n29\ntrain\n\n\n0.344\n3.163\n29\neval\n\n\n0.467\n2.188\n30\ntrain\n\n\n0.403\n2.749\n30\neval\n\n\n0.463\n2.205\n31\ntrain\n\n\n0.407\n2.692\n31\neval\n\n\n0.465\n2.196\n32\ntrain\n\n\n0.400\n2.628\n32\neval\n\n\n0.462\n2.206\n33\ntrain\n\n\n0.467\n2.278\n33\neval\n\n\n0.465\n2.202\n34\ntrain\n\n\n0.452\n2.303\n34\neval\n\n\n0.462\n2.203\n35\ntrain\n\n\n0.452\n2.377\n35\neval\n\n\n0.464\n2.200\n36\ntrain\n\n\n0.437\n2.443\n36\neval\n\n\n0.465\n2.207\n37\ntrain\n\n\n0.436\n2.428\n37\neval\n\n\n0.462\n2.200\n38\ntrain\n\n\n0.423\n2.565\n38\neval\n\n\n0.463\n2.210\n39\ntrain\n\n\n0.357\n3.034\n39\neval\n\n\n0.460\n2.221\n40\ntrain\n\n\n0.396\n2.672\n40\neval\n\n\n0.459\n2.225\n41\ntrain\n\n\n0.450\n2.317\n41\neval\n\n\n0.459\n2.230\n42\ntrain\n\n\n0.446\n2.407\n42\neval\n\n\n0.458\n2.237\n43\ntrain\n\n\n0.397\n2.638\n43\neval\n\n\n0.459\n2.236\n44\ntrain\n\n\n0.439\n2.424\n44\neval\n\n\n0.456\n2.243\n45\ntrain\n\n\n0.445\n2.336\n45\neval\n\n\n0.455\n2.253\n46\ntrain\n\n\n0.412\n2.652\n46\neval\n\n\n0.453\n2.254\n47\ntrain\n\n\n0.380\n2.760\n47\neval\n\n\n0.456\n2.248\n48\ntrain\n\n\n0.400\n2.701\n48\neval\n\n\n0.453\n2.256\n49\ntrain\n\n\n0.409\n2.691\n49\neval\n\n\n0.451\n2.264\n50\ntrain\n\n\n0.354\n3.238\n50\neval\n\n\n0.452\n2.267\n51\ntrain\n\n\n0.434\n2.413\n51\neval\n\n\n0.450\n2.265\n52\ntrain\n\n\n0.431\n2.499\n52\neval\n\n\n0.452\n2.260\n53\ntrain\n\n\n0.447\n2.352\n53\neval\n\n\n0.452\n2.267\n54\ntrain\n\n\n0.442\n2.383\n54\neval\n\n\n0.447\n2.271\n55\ntrain\n\n\n0.405\n2.598\n55\neval\n\n\n0.451\n2.266\n56\ntrain\n\n\n0.418\n2.584\n56\neval\n\n\n0.450\n2.268\n57\ntrain\n\n\n0.419\n2.531\n57\neval\n\n\n0.452\n2.263\n58\ntrain\n\n\n0.457\n2.286\n58\neval\n\n\n0.451\n2.262\n59\ntrain\n\n\n0.351\n3.086\n59\neval\n\n\n0.449\n2.270\n60\ntrain\n\n\n0.387\n2.717\n60\neval\n\n\n0.456\n2.243\n61\ntrain\n\n\n0.427\n2.532\n61\neval\n\n\n0.456\n2.241\n62\ntrain\n\n\n0.408\n2.488\n62\neval\n\n\n0.455\n2.249\n63\ntrain\n\n\n0.405\n2.625\n63\neval\n\n\n0.456\n2.239\n64\ntrain\n\n\n0.420\n2.485\n64\neval\n\n\n0.456\n2.237\n65\ntrain\n\n\n0.411\n2.518\n65\neval\n\n\n0.457\n2.239\n66\ntrain\n\n\n0.417\n2.590\n66\neval\n\n\n0.457\n2.230\n67\ntrain\n\n\n0.354\n2.970\n67\neval\n\n\n0.458\n2.229\n68\ntrain\n\n\n0.424\n2.444\n68\neval\n\n\n0.460\n2.219\n69\ntrain\n\n\n0.347\n2.992\n69\neval\n\n\n0.463\n2.212\n70\ntrain\n\n\n0.395\n2.699\n70\neval\n\n\n0.463\n2.207\n71\ntrain\n\n\n0.339\n3.148\n71\neval\n\n\n0.464\n2.200\n72\ntrain\n\n\n0.353\n3.148\n72\neval\n\n\n0.464\n2.201\n73\ntrain\n\n\n0.434\n2.422\n73\neval\n\n\n0.466\n2.198\n74\ntrain\n\n\n0.424\n2.551\n74\neval\n\n\n0.471\n2.179\n75\ntrain\n\n\n0.393\n2.710\n75\neval\n\n\n0.468\n2.183\n76\ntrain\n\n\n0.405\n2.624\n76\neval\n\n\n0.465\n2.194\n77\ntrain\n\n\n0.417\n2.578\n77\neval\n\n\n0.471\n2.172\n78\ntrain\n\n\n0.440\n2.477\n78\neval\n\n\n0.471\n2.164\n79\ntrain\n\n\n0.392\n2.752\n79\neval\n\n\n0.473\n2.161\n80\ntrain\n\n\n0.424\n2.490\n80\neval\n\n\n0.473\n2.164\n81\ntrain\n\n\n0.420\n2.547\n81\neval\n\n\n0.476\n2.147\n82\ntrain\n\n\n0.394\n2.779\n82\neval\n\n\n0.475\n2.155\n83\ntrain\n\n\n0.400\n2.747\n83\neval\n\n\n0.477\n2.138\n84\ntrain\n\n\n0.397\n2.684\n84\neval\n\n\n0.481\n2.134\n85\ntrain\n\n\n0.442\n2.428\n85\neval\n\n\n0.479\n2.122\n86\ntrain\n\n\n0.371\n2.966\n86\neval\n\n\n0.482\n2.123\n87\ntrain\n\n\n0.410\n2.634\n87\neval\n\n\n0.481\n2.121\n88\ntrain\n\n\n0.442\n2.399\n88\neval\n\n\n0.482\n2.111\n89\ntrain\n\n\n0.461\n2.331\n89\neval\n\n\n0.487\n2.092\n90\ntrain\n\n\n0.435\n2.423\n90\neval\n\n\n0.487\n2.092\n91\ntrain\n\n\n0.414\n2.691\n91\neval\n\n\n0.486\n2.089\n92\ntrain\n\n\n0.384\n2.826\n92\neval\n\n\n0.493\n2.062\n93\ntrain\n\n\n0.462\n2.385\n93\neval\n\n\n0.494\n2.056\n94\ntrain\n\n\n0.460\n2.318\n94\neval\n\n\n0.493\n2.056\n95\ntrain\n\n\n0.418\n2.625\n95\neval\n\n\n0.497\n2.045\n96\ntrain\n\n\n0.444\n2.455\n96\neval\n\n\n0.495\n2.043\n97\ntrain\n\n\n0.484\n2.159\n97\neval\n\n\n0.503\n2.016\n98\ntrain\n\n\n0.417\n2.637\n98\neval\n\n\n0.504\n2.016\n99\ntrain\n\n\n0.384\n2.846\n99\neval\n\n\n0.503\n2.016\n100\ntrain\n\n\n0.494\n2.186\n100\neval\n\n\n0.504\n2.008\n101\ntrain\n\n\n0.436\n2.522\n101\neval\n\n\n0.509\n1.988\n102\ntrain\n\n\n0.445\n2.428\n102\neval\n\n\n0.508\n1.986\n103\ntrain\n\n\n0.488\n2.131\n103\neval\n\n\n0.512\n1.973\n104\ntrain\n\n\n0.485\n2.196\n104\neval\n\n\n0.515\n1.959\n105\ntrain\n\n\n0.486\n2.231\n105\neval\n\n\n0.515\n1.953\n106\ntrain\n\n\n0.461\n2.331\n106\neval\n\n\n0.522\n1.924\n107\ntrain\n\n\n0.477\n2.287\n107\neval\n\n\n0.522\n1.920\n108\ntrain\n\n\n0.452\n2.439\n108\neval\n\n\n0.522\n1.921\n109\ntrain\n\n\n0.493\n2.176\n109\neval\n\n\n0.525\n1.909\n110\ntrain\n\n\n0.488\n2.259\n110\neval\n\n\n0.528\n1.894\n111\ntrain\n\n\n0.477\n2.305\n111\neval\n\n\n0.532\n1.879\n112\ntrain\n\n\n0.485\n2.224\n112\neval\n\n\n0.536\n1.862\n113\ntrain\n\n\n0.485\n2.244\n113\neval\n\n\n0.538\n1.850\n114\ntrain\n\n\n0.451\n2.439\n114\neval\n\n\n0.540\n1.842\n115\ntrain\n\n\n0.482\n2.231\n115\neval\n\n\n0.545\n1.824\n116\ntrain\n\n\n0.506\n2.124\n116\neval\n\n\n0.548\n1.803\n117\ntrain\n\n\n0.461\n2.427\n117\neval\n\n\n0.550\n1.797\n118\ntrain\n\n\n0.510\n2.132\n118\neval\n\n\n0.555\n1.774\n119\ntrain\n\n\n0.504\n2.124\n119\neval\n\n\n0.556\n1.767\n120\ntrain\n\n\n0.447\n2.468\n120\neval\n\n\n0.560\n1.748\n121\ntrain\n\n\n0.472\n2.329\n121\neval\n\n\n0.563\n1.736\n122\ntrain\n\n\n0.488\n2.283\n122\neval\n\n\n0.568\n1.714\n123\ntrain\n\n\n0.466\n2.340\n123\neval\n\n\n0.568\n1.710\n124\ntrain\n\n\n0.464\n2.382\n124\neval\n\n\n0.574\n1.686\n125\ntrain\n\n\n0.451\n2.466\n125\neval\n\n\n0.578\n1.668\n126\ntrain\n\n\n0.518\n2.113\n126\neval\n\n\n0.584\n1.647\n127\ntrain\n\n\n0.495\n2.331\n127\neval\n\n\n0.585\n1.635\n128\ntrain\n\n\n0.496\n2.276\n128\neval\n\n\n0.589\n1.619\n129\ntrain\n\n\n0.452\n2.528\n129\neval\n\n\n0.594\n1.596\n130\ntrain\n\n\n0.534\n2.063\n130\neval\n\n\n0.596\n1.587\n131\ntrain\n\n\n0.522\n2.098\n131\neval\n\n\n0.600\n1.561\n132\ntrain\n\n\n0.535\n1.996\n132\neval\n\n\n0.606\n1.536\n133\ntrain\n\n\n0.539\n1.994\n133\neval\n\n\n0.609\n1.528\n134\ntrain\n\n\n0.506\n2.207\n134\neval\n\n\n0.610\n1.513\n135\ntrain\n\n\n0.515\n2.208\n135\neval\n\n\n0.620\n1.477\n136\ntrain\n\n\n0.542\n1.923\n136\neval\n\n\n0.625\n1.460\n137\ntrain\n\n\n0.521\n2.127\n137\neval\n\n\n0.627\n1.438\n138\ntrain\n\n\n0.518\n2.218\n138\neval\n\n\n0.635\n1.417\n139\ntrain\n\n\n0.515\n2.151\n139\neval\n\n\n0.641\n1.387\n140\ntrain\n\n\n0.555\n1.921\n140\neval\n\n\n0.646\n1.364\n141\ntrain\n\n\n0.526\n2.154\n141\neval\n\n\n0.648\n1.355\n142\ntrain\n\n\n0.544\n2.087\n142\neval\n\n\n0.654\n1.325\n143\ntrain\n\n\n0.558\n1.904\n143\neval\n\n\n0.661\n1.300\n144\ntrain\n\n\n0.551\n2.035\n144\neval\n\n\n0.667\n1.281\n145\ntrain\n\n\n0.537\n2.157\n145\neval\n\n\n0.671\n1.253\n146\ntrain\n\n\n0.558\n1.982\n146\neval\n\n\n0.678\n1.233\n147\ntrain\n\n\n0.553\n2.052\n147\neval\n\n\n0.682\n1.216\n148\ntrain\n\n\n0.547\n2.070\n148\neval\n\n\n0.690\n1.180\n149\ntrain\n\n\n0.571\n1.968\n149\neval\n\n\n0.699\n1.144\n150\ntrain\n\n\n0.578\n1.882\n150\neval\n\n\n0.702\n1.136\n151\ntrain\n\n\n0.574\n1.972\n151\neval\n\n\n0.711\n1.098\n152\ntrain\n\n\n0.571\n1.981\n152\neval\n\n\n0.716\n1.073\n153\ntrain\n\n\n0.589\n1.899\n153\neval\n\n\n0.725\n1.044\n154\ntrain\n\n\n0.561\n2.112\n154\neval\n\n\n0.728\n1.036\n155\ntrain\n\n\n0.592\n1.967\n155\neval\n\n\n0.733\n1.005\n156\ntrain\n\n\n0.567\n2.035\n156\neval\n\n\n0.741\n0.978\n157\ntrain\n\n\n0.573\n2.003\n157\neval\n\n\n0.749\n0.947\n158\ntrain\n\n\n0.599\n1.874\n158\neval\n\n\n0.755\n0.929\n159\ntrain\n\n\n0.596\n1.894\n159\neval\n\n\n0.766\n0.892\n160\ntrain\n\n\n0.600\n1.913\n160\neval\n\n\n0.770\n0.866\n161\ntrain\n\n\n0.606\n1.850\n161\neval\n\n\n0.777\n0.839\n162\ntrain\n\n\n0.604\n1.852\n162\neval\n\n\n0.786\n0.814\n163\ntrain\n\n\n0.592\n1.922\n163\neval\n\n\n0.789\n0.800\n164\ntrain\n\n\n0.604\n1.832\n164\neval\n\n\n0.796\n0.769\n165\ntrain\n\n\n0.613\n1.781\n165\neval\n\n\n0.801\n0.747\n166\ntrain\n\n\n0.617\n1.826\n166\neval\n\n\n0.808\n0.731\n167\ntrain\n\n\n0.609\n1.871\n167\neval\n\n\n0.817\n0.695\n168\ntrain\n\n\n0.621\n1.781\n168\neval\n\n\n0.825\n0.669\n169\ntrain\n\n\n0.622\n1.788\n169\neval\n\n\n0.828\n0.656\n170\ntrain\n\n\n0.624\n1.763\n170\neval\n\n\n0.835\n0.631\n171\ntrain\n\n\n0.635\n1.761\n171\neval\n\n\n0.842\n0.605\n172\ntrain\n\n\n0.641\n1.699\n172\neval\n\n\n0.845\n0.589\n173\ntrain\n\n\n0.629\n1.740\n173\neval\n\n\n0.852\n0.566\n174\ntrain\n\n\n0.621\n1.812\n174\neval\n\n\n0.859\n0.542\n175\ntrain\n\n\n0.637\n1.716\n175\neval\n\n\n0.860\n0.533\n176\ntrain\n\n\n0.646\n1.710\n176\neval\n\n\n0.866\n0.509\n177\ntrain\n\n\n0.652\n1.685\n177\neval\n\n\n0.870\n0.495\n178\ntrain\n\n\n0.654\n1.644\n178\neval\n\n\n0.876\n0.472\n179\ntrain\n\n\n0.652\n1.638\n179\neval\n\n\n0.882\n0.452\n180\ntrain\n\n\n0.654\n1.630\n180\neval\n\n\n0.884\n0.443\n181\ntrain\n\n\n0.653\n1.651\n181\neval\n\n\n0.889\n0.424\n182\ntrain\n\n\n0.661\n1.592\n182\neval\n\n\n0.893\n0.412\n183\ntrain\n\n\n0.661\n1.584\n183\neval\n\n\n0.896\n0.403\n184\ntrain\n\n\n0.663\n1.595\n184\neval\n\n\n0.899\n0.391\n185\ntrain\n\n\n0.666\n1.572\n185\neval\n\n\n0.903\n0.372\n186\ntrain\n\n\n0.669\n1.551\n186\neval\n\n\n0.905\n0.366\n187\ntrain\n\n\n0.668\n1.561\n187\neval\n\n\n0.907\n0.354\n188\ntrain\n\n\n0.670\n1.540\n188\neval\n\n\n0.909\n0.346\n189\ntrain\n\n\n0.672\n1.526\n189\neval\n\n\n0.911\n0.342\n190\ntrain\n\n\n0.674\n1.522\n190\neval\n\n\n0.912\n0.335\n191\ntrain\n\n\n0.676\n1.521\n191\neval\n\n\n0.915\n0.327\n192\ntrain\n\n\n0.675\n1.507\n192\neval\n\n\n0.915\n0.323\n193\ntrain\n\n\n0.675\n1.504\n193\neval\n\n\n0.919\n0.312\n194\ntrain\n\n\n0.675\n1.507\n194\neval\n\n\n0.917\n0.318\n195\ntrain\n\n\n0.676\n1.505\n195\neval\n\n\n0.917\n0.315\n196\ntrain\n\n\n0.679\n1.502\n196\neval\n\n\n0.918\n0.311\n197\ntrain\n\n\n0.677\n1.504\n197\neval\n\n\n0.918\n0.315\n198\ntrain\n\n\n0.676\n1.505\n198\neval\n\n\n0.919\n0.309\n199\ntrain\n\n\n0.675\n1.502\n199\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/inettiny-trivaug-200')",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  },
  {
    "objectID": "styletransfer.html",
    "href": "styletransfer.html",
    "title": "Setup",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nimport torchvision.transforms as trans\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\n# Image URLs for demos. Change as desired.\nface_url = \"https://images.pexels.com/photos/2690323/pexels-photo-2690323.jpeg?w=256\"\nspiderweb_url = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"",
    "crumbs": [
      "Blog",
      "Setup"
    ]
  },
  {
    "objectID": "styletransfer.html#viewing-progress",
    "href": "styletransfer.html#viewing-progress",
    "title": "Setup",
    "section": "Viewing progress",
    "text": "Viewing progress\nIt would be great if we could see what is happening over time. You could save individual images and turn them into a video, but for quick feedback we can also log images every few iterations and display them in a grid in after_fit:\n\nclass ImageLogCB(Callback):\n    order = ProgressCB.order + 1\n    def __init__(self, log_every=10): store_attr(); self.images=[]; self.i=0\n    def after_batch(self, learn): \n        if self.i%self.log_every == 0: self.images.append(to_cpu(learn.preds.clip(0, 1)))\n        self.i += 1\n    def after_fit(self, learn): show_images(self.images)\n\n\nmodel = TensorModel(torch.rand_like(content_im))\nlearn = Learner(model, get_dummy_dls(150), loss_fn_mse, \n                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(30)])\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.028\n0\ntrain\n\n\n0.000\n0\neval",
    "crumbs": [
      "Blog",
      "Setup"
    ]
  },
  {
    "objectID": "styletransfer.html#getting-features-from-vgg16",
    "href": "styletransfer.html#getting-features-from-vgg16",
    "title": "Setup",
    "section": "Getting Features from VGG16",
    "text": "Getting Features from VGG16\nWe’re going to peek inside a small CNN and extract the outputs of different layers.\n\nLoad VGG network\n\n\n\nvgg diag\n\n\n\nprint(timm.list_models('*vgg*'))\n\n['repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\n\n\nNormalize Images\nThis model expacts images normalized with the same stats as those used during training, which in this case requires the stats of the ImageNet dataset. Previously we were working with single-channel images, and so normalizing was pretty straightforward. With three channels, we need to think a bit more about shapes and boradcasting rules:\n\nimagenet_mean = tensor([0.485, 0.456, 0.406])\nimagenet_std = tensor([0.229, 0.224, 0.225])\n\n\n# Try 1 (won't work):\n# (content_im - imagenet_mean) / imagenet_std\n\n\nimagenet_mean.shape\n\ntorch.Size([3])\n\n\n\ncontent_im.shape\n\ntorch.Size([3, 256, 256])\n\n\n\nimagenet_mean[:,None,None].shape\n\ntorch.Size([3, 1, 1])\n\n\n\n# Try 2:\ndef normalize(im):\n    imagenet_mean = tensor([0.485, 0.456, 0.406])[:,None,None].to(im.device)\n    imagenet_std = tensor([0.229, 0.224, 0.225])[:,None,None].to(im.device)\n    return (im - imagenet_mean) / imagenet_std\n\n\nnormalize(content_im).min(), normalize(content_im).max()\n\n(tensor(-2.12, device='cuda:0'), tensor(2.64, device='cuda:0'))\n\n\n\nnormalize(content_im).mean(dim=(1, 2))\n\ntensor([-0.97, -0.96, -0.42], device='cuda:0')\n\n\n\n# And with torchvision transforms:\nnormalize = trans.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n\nnormalize(content_im).min(), normalize(content_im).max()\n\n(tensor(-2.12, device='cuda:0'), tensor(2.64, device='cuda:0'))\n\n\n\n\nGet intermediate representations, take 1:\nWe want to feed some data through the network, storing thr outputs of different layers. Here’s one way to do this:\n\ndef calc_features(imgs, target_layers=(18, 25)): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\n\n# Testing it out to see the shapes of the resulting feature maps:\nfeats = calc_features(content_im)\n[f.shape for f in feats]\n\n[torch.Size([512, 32, 32]), torch.Size([512, 16, 16])]\n\n\n\n# Homework: Can you do this using hooks?\n\n\n\nWhat’s the point?\nYou may remember us looking at https://distill.pub/2017/feature-visualization/ and talking about how deep CNNs ‘learn’ to classify images. Early layers tend to capture gradients and textures, while later layers tend towards more complex types of feature. We’re going to exploit this hierarchy for artistic purposes, but being able to choose what kind of feature you’d like to use when comparing images has a number of other useful applications.",
    "crumbs": [
      "Blog",
      "Setup"
    ]
  },
  {
    "objectID": "styletransfer.html#style-transfer",
    "href": "styletransfer.html#style-transfer",
    "title": "Setup",
    "section": "Style Transfer",
    "text": "Style Transfer\n\ndef image_to_tensor(path, rotate = 0, size = (256,256)):\n    from PIL import Image\n    from torchvision import transforms\n    # Define the file path of your image\n    image_path = path\n    # Open the image using PIL\n    image_pil = Image.open(image_path)\n\n    # Define the transformation to convert the image to a PyTorch tensor\n    transform = transforms.ToTensor()\n\n    # Apply the transformation to the image\n    image_tensor = transform(image_pil)\n    image_tensor = image_tensor.rot90(k = rotate, dims =(1,2))\n    \n    # Downsize the image tensor to size\n    downsized_image_tensor = F.interpolate(image_tensor.unsqueeze(0), size=size, mode='bilinear', align_corners=False)\n    downsized_image_tensor = downsized_image_tensor.squeeze(0)\n\n    # Print the shape of the resulting tensor\n    print(\"Tensor shape:\", downsized_image_tensor.shape)\n    return downsized_image_tensor\n\n\noriginal_path = 'Data/galaxy.jpg'\noriginal = image_to_tensor(original_path, 0, size=(512, 512)).to(def_device)\nshow_image(original);\n\nTensor shape: torch.Size([3, 512, 512])\n\n\n\n\n\n\n\n\n\n\nart_path = 'Data/art.jpg'\nart = image_to_tensor(art_path, size=(512, 512)).to(def_device)\nshow_image(art);\n\nTensor shape: torch.Size([3, 512, 512])\n\n\n\n\n\n\n\n\n\n\nmodel = TensorModel(original) # Start from content image\nstyle_loss = StyleLossToTarget(art)\ncontent_loss = ContentLossToTarget(original)\ncbs = [ImageOptCB(), ProgressCB(), MetricsCB(), DeviceCB()]\ndef combined_loss(x):\n    return style_loss(x) + content_loss(x) * 0.5\nlearn = Learner(model, get_dummy_dls(150), combined_loss, lr=3e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(30)])\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n8.123\n0\ntrain\n\n\n4.497\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\noriginal_path = 'Data/original.jpg'\noriginal1 = image_to_tensor(original_path, -1, size=(512, 512)).to(def_device)\nshow_image(original1);\n\nTensor shape: torch.Size([3, 512, 512])\n\n\n\n\n\n\n\n\n\n\nmodel = TensorModel(original1) # Start from content image\nstyle_loss = StyleLossToTarget(art)\ncontent_loss = ContentLossToTarget(original1)\ncbs = [ImageOptCB(), ProgressCB(), MetricsCB(), DeviceCB()]\ndef combined_loss(x):\n    return style_loss(x) + content_loss(x) * 0.5\nlearn = Learner(model, get_dummy_dls(150), combined_loss, lr=3e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(30)])\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n8.568\n0\ntrain\n\n\n5.177\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = TensorModel(content_im) # Start from content image\nstyle_loss = StyleLossToTarget(style_im)\ncontent_loss = ContentLossToTarget(content_im)\ndef combined_loss(x):\n    return style_loss(x) + content_loss(x)\nlearn = Learner(model, get_dummy_dls(150), combined_loss, lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(30)])\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n79.014\n0\ntrain\n\n\n46.737\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(learn.model().clip(0, 1)); # View the final result\n\n\n\n\n\n\n\n\nAnd trying with random starting image, weighting the style loss lower, using different layers:\n\nmodel = TensorModel(torch.rand_like(content_im))\nstyle_loss = StyleLossToTarget(style_im)\ncontent_loss = ContentLossToTarget(content_im, target_layers=(6, 18, 25))\ndef combined_loss(x):\n    return style_loss(x) * 0.2 + content_loss(x)\nlearn = Learner(model, get_dummy_dls(300), combined_loss, lr=5e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(60)])\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n47.008\n0\ntrain\n\n\n38.026\n0\neval",
    "crumbs": [
      "Blog",
      "Setup"
    ]
  },
  {
    "objectID": "styletransfer.html#for-comparison-non-miniai-version",
    "href": "styletransfer.html#for-comparison-non-miniai-version",
    "title": "Setup",
    "section": "For Comparison: non-miniai version",
    "text": "For Comparison: non-miniai version\n\ncontent_im.shape, style_im.shape\n\n(torch.Size([3, 256, 256]), torch.Size([3, 171, 256]))\n\n\n\ncontent_im = download_image(face_url).to(def_device)\ncopy = download_image(face_url).to(def_device)\nprint('content_im.shape:', content_im.shape)\n\ncontent_im.shape: torch.Size([3, 256, 256])\n\n\n\nshow_image(content_im);\n\n\n\n\n\n\n\n\n\n# The image to be optimized\nfrom tqdm import tqdm  \ncontent_im = download_image(face_url).to(def_device)\ncopy = download_image(face_url).to(def_device)\n\nim = torch.rand(3, 256, 256).to(def_device)\nim = copy\nim.requires_grad = True\n\n\n\n# Set up the optimizer\nopt = torch.optim.Adam([im], lr=5e-2)\n\n# Define the loss function\nstyle_loss = StyleLossToTarget(style_im)\ncontent_loss = ContentLossToTarget(content_im)\n\ndef combined_loss(x):\n    return style_loss(x) * 0.7 + content_loss(x)\n\n# Optimization loop\nfor i in tqdm(range(10)):\n    loss = combined_loss(im)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n# Show the result\nshow_image(im.clip(0, 1));\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00&lt;00:00, 17.57it/s]\n\n\n\n\n\n\n\n\n\n\n# Show the result\nshow_image(content_im.clip(0, 1));\n\n\n\n\n\n\n\n\nWhat do you think are some pros and cons? How would this look once we start displaying progress, testing different configurations and so on?",
    "crumbs": [
      "Blog",
      "Setup"
    ]
  },
  {
    "objectID": "diffusion-attn-cond.html",
    "href": "diffusion-attn-cond.html",
    "title": "Diffusion unet",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES']='1'\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nbs = 512\ndsd = load_dataset(name)\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\ndl = dls.train\n(xt,t),eps = b = next(iter(dl))",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion-attn-cond.html#train",
    "href": "diffusion-attn-cond.html#train",
    "title": "Diffusion unet",
    "section": "Train",
    "text": "Train\nBased on Diffusers\n\n# This version is giving poor results - use the cell below instead\nclass SelfAttention(nn.Module):\n    def __init__(self, ni, attn_chans):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(ni, ni//attn_chans, batch_first=True)\n        self.norm = nn.BatchNorm2d(ni)\n\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x = self.norm(x).view(n, c, -1).transpose(1, 2)\n        x = self.attn(x, x, x, need_weights=False)[0]\n        return x.transpose(1,2).reshape(n,c,h,w)\n\n\nlr = 1e-2\nepochs = 25\nopt_func = partial(optim.Adam, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\nmodel = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.150\n0\ntrain\n\n\n0.086\n0\neval\n\n\n0.069\n1\ntrain\n\n\n0.171\n1\neval\n\n\n0.057\n2\ntrain\n\n\n0.071\n2\neval\n\n\n0.050\n3\ntrain\n\n\n0.055\n3\neval\n\n\n0.045\n4\ntrain\n\n\n0.050\n4\neval\n\n\n0.043\n5\ntrain\n\n\n0.073\n5\neval\n\n\n0.041\n6\ntrain\n\n\n0.044\n6\neval\n\n\n0.039\n7\ntrain\n\n\n0.044\n7\neval\n\n\n0.038\n8\ntrain\n\n\n0.043\n8\neval\n\n\n0.038\n9\ntrain\n\n\n0.058\n9\neval\n\n\n0.038\n10\ntrain\n\n\n0.044\n10\neval\n\n\n0.036\n11\ntrain\n\n\n0.042\n11\neval\n\n\n0.035\n12\ntrain\n\n\n0.038\n12\neval\n\n\n0.035\n13\ntrain\n\n\n0.039\n13\neval\n\n\n0.034\n14\ntrain\n\n\n0.036\n14\neval\n\n\n0.034\n15\ntrain\n\n\n0.036\n15\neval\n\n\n0.034\n16\ntrain\n\n\n0.034\n16\neval\n\n\n0.034\n17\ntrain\n\n\n0.035\n17\neval\n\n\n0.033\n18\ntrain\n\n\n0.033\n18\neval\n\n\n0.033\n19\ntrain\n\n\n0.033\n19\neval\n\n\n0.033\n20\ntrain\n\n\n0.033\n20\neval\n\n\n0.033\n21\ntrain\n\n\n0.032\n21\neval\n\n\n0.032\n22\ntrain\n\n\n0.034\n22\neval\n\n\n0.032\n23\ntrain\n\n\n0.032\n23\neval\n\n\n0.032\n24\ntrain\n\n\n0.033\n24\neval",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion-attn-cond.html#sampling",
    "href": "diffusion-attn-cond.html#sampling",
    "title": "Diffusion unet",
    "section": "Sampling",
    "text": "Sampling\n\nfrom miniai.fid import ImageEval\n\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\nbs = 2048\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\nie = ImageEval(cmodel, dls, cbs=[DeviceCB()])\n\n\nsz = (2048,1,32,32)\n\n\n# set_seed(42)\npreds = sample(ddim_step, model, sz, steps=100, eta=1.)\ns = (preds[-1]*2)\ns.min(),s.max(),s.shape\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:53&lt;00:00]\n    \n    \n\n\n(tensor(-1.0918), tensor(1.4292), torch.Size([2048, 1, 32, 32]))\n\n\n\nshow_images(s[:25].clamp(-1,1), imsize=1.5)\n\n\n\n\n\n\n\n\n\nie.fid(s),ie.kid(s),s.shape\n\n(4.058064770194278, 0.010895456187427044, torch.Size([2048, 1, 32, 32]))\n\n\n\npreds = sample(ddim_step, model, sz, steps=100, eta=1.)\nie.fid(preds[-1]*2)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:53&lt;00:00]\n    \n    \n\n\n5.320260029850715\n\n\n\npreds = sample(ddim_step, model, sz, steps=50, eta=1.)\nie.fid(preds[-1]*2)\n\n\n\n\n\n\n    \n      \n      100.00% [50/50 00:26&lt;00:00]\n    \n    \n\n\n5.243807277315682\n\n\n\npreds = sample(ddim_step, model, sz, steps=50, eta=1.)\nie.fid(preds[-1]*2)\n\n\n\n\n\n\n    \n      \n      100.00% [50/50 00:26&lt;00:00]\n    \n    \n\n\n4.963977301033992",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "diffusion-attn-cond.html#conditional-model",
    "href": "diffusion-attn-cond.html#conditional-model",
    "title": "Diffusion unet",
    "section": "Conditional model",
    "text": "Conditional model\n\ndef collate_ddpm(b):\n    b = default_collate(b)\n    (xt,t),eps = noisify(b[xl])\n    return (xt,t,b[yl]),eps\n\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\ndl = dls.train\n(xt,t,c),eps = b = next(iter(dl))\n\n\nclass CondUNetModel(nn.Module):\n    def __init__( self, n_classes, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n        super().__init__()\n        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n        self.n_temb = nf = nfs[0]\n        n_emb = nf*4\n        self.cond_emb = nn.Embedding(n_classes, n_emb)\n        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n                                     lin(n_emb, n_emb))\n        self.downs = nn.ModuleList()\n        for i in range(len(nfs)):\n            ni = nf\n            nf = nfs[i]\n            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n\n        rev_nfs = list(reversed(nfs))\n        nf = rev_nfs[0]\n        self.ups = nn.ModuleList()\n        for i in range(len(nfs)):\n            prev_nf = nf\n            nf = rev_nfs[i]\n            ni = rev_nfs[min(i+1, len(nfs)-1)]\n            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n\n    def forward(self, inp):\n        x,t,c = inp\n        temb = timestep_embedding(t, self.n_temb)\n        cemb = self.cond_emb(c)\n        emb = self.emb_mlp(temb) + cemb\n        x = self.conv_in(x)\n        saved = [x]\n        for block in self.downs: x = block(x, emb)\n        saved += [p for o in self.downs for p in o.saved]\n        x = self.mid_block(x, emb)\n        for block in self.ups: x = block(x, emb, saved)\n        return self.conv_out(x)\n\n\nlr = 1e-2\nepochs = 25\nopt_func = partial(optim.Adam, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\nmodel = CondUNetModel(10, in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.178\n0\ntrain\n\n\n0.099\n0\neval\n\n\n0.072\n1\ntrain\n\n\n0.066\n1\neval\n\n\n0.053\n2\ntrain\n\n\n0.053\n2\neval\n\n\n0.047\n3\ntrain\n\n\n0.050\n3\neval\n\n\n0.045\n4\ntrain\n\n\n0.045\n4\neval\n\n\n0.042\n5\ntrain\n\n\n0.048\n5\neval\n\n\n0.041\n6\ntrain\n\n\n0.060\n6\neval\n\n\n0.039\n7\ntrain\n\n\n0.042\n7\neval\n\n\n0.037\n8\ntrain\n\n\n0.039\n8\neval\n\n\n0.037\n9\ntrain\n\n\n0.051\n9\neval\n\n\n0.036\n10\ntrain\n\n\n0.039\n10\neval\n\n\n0.035\n11\ntrain\n\n\n0.041\n11\neval\n\n\n0.035\n12\ntrain\n\n\n0.041\n12\neval\n\n\n0.034\n13\ntrain\n\n\n0.035\n13\neval\n\n\n0.034\n14\ntrain\n\n\n0.035\n14\neval\n\n\n0.034\n15\ntrain\n\n\n0.036\n15\neval\n\n\n0.033\n16\ntrain\n\n\n0.037\n16\neval\n\n\n0.033\n17\ntrain\n\n\n0.032\n17\neval\n\n\n0.032\n18\ntrain\n\n\n0.036\n18\neval\n\n\n0.032\n19\ntrain\n\n\n0.033\n19\neval\n\n\n0.032\n20\ntrain\n\n\n0.032\n20\neval\n\n\n0.032\n21\ntrain\n\n\n0.033\n21\neval\n\n\n0.032\n22\ntrain\n\n\n0.033\n22\neval\n\n\n0.031\n23\ntrain\n\n\n0.032\n23\neval\n\n\n0.031\n24\ntrain\n\n\n0.033\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsz = (256,1,32,32)\n\n\nlbls = dsd['train'].features[yl].names\nlbls\n\n['T - shirt / top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n\nset_seed(42)\ncid = 0\npreds = sample(cid, ddim_step, model, sz, steps=100, eta=1.)\ns = (preds[-1]*2)\nshow_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:02&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\nset_seed(42)\ncid = 0\npreds = sample(cid, ddim_step, model, sz, steps=100, eta=0.)\ns = (preds[-1]*2)\nshow_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:02&lt;00:00]",
    "crumbs": [
      "Blog",
      "Diffusion unet"
    ]
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "Foundations",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nimport random",
    "crumbs": [
      "Blog",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#dunder__-thingies",
    "href": "foundations.html#dunder__-thingies",
    "title": "Foundations",
    "section": "__dunder__ thingies",
    "text": "__dunder__ thingies\nAnything that looks like __this__ is, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call __init__. These are defined as part of the python data model.\nFor instance, if python sees +, then it will call the special method __add__. If you try to display an object in Jupyter (or lots of other places in Python) it will call __repr__.\n\nclass SloppyAdder():\n    def __init__(self,o): self.o=o\n    def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01)\n    def __repr__(self): return str(self.o)\n\n\na = SloppyAdder(1)\nb = SloppyAdder(2)\na+b+a\n\n4.02\n\n\nSpecial methods you should probably know about (see data model link above) are:\n\n__getitem__\n__getattr__\n__setattr__\n__del__\n__init__\n__new__\n__enter__\n__exit__\n__len__\n__repr__\n__str__\n\n\n__getattr__ and getattr\n\nclass A: \n    a,b=1,2\n\n\na = A()\n\n\na.b\n\n2\n\n\n\ngetattr(a, 'b')\n\n2\n\n\n\ngetattr(a, 'b' if random.random()&gt;0.5 else 'a')\n\n1\n\n\n\nclass B:\n    a,b=1,2\n    def __getattr__(self, k):\n        if k[0]=='_': raise AttributeError(k)\n        return f'Hello from {k}'\n\n\nb = B()\n\n\nb.a\n\n1\n\n\n\nb.foo\n\n'Hello from foo'",
    "crumbs": [
      "Blog",
      "Foundations"
    ]
  },
  {
    "objectID": "kaggle_tut.html",
    "href": "kaggle_tut.html",
    "title": "Kaggle tutorial",
    "section": "",
    "text": "In Iterate Like a Grandmaster I explained that when working on a Kaggle project:\nHere I’m going to go further, showing the process I used to tackle the Paddy Doctor competition, leading to four submissions in a row which all were (at the time of submission) in 1st place, each one more accurate than the last. You might be surprised to discover that the process of doing this was nearly entirely mechanistic and didn’t involve any consideration of the actual data or evaluation details at all.\nThis notebook is the first in a series showing every step of the process. At the end of this notebook we’ll have a basic submission; by the end of the series you’ll see how I got to the top of the table!:\nAs a special extra, I’m also opening up early a selection of “walkthru” videos that we’ve been preparing for the new upcoming fast.ai course. Each day I do a walkthru with fast.ai fellows and registered students, and we record those sessions. They’ll all be released at the same time as the next course (probably August 2022), but I’m releasing the ones covering this competition right now! Here they are:\nWhen you’re done with this notebook, take a look at part 2 of the series.",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "kaggle_tut.html#getting-set-up",
    "href": "kaggle_tut.html#getting-set-up",
    "title": "Kaggle tutorial",
    "section": "Getting set up",
    "text": "Getting set up\nFirst, we’ll get the data. I’ve just created a new library called fastkaggle which has a few handy features, including getting the data for a competition correctly regardless of whether we’re running on Kaggle or elsewhere. Note you’ll need to first accept the competition rules and join the competition, and you’ll need your kaggle API key file kaggle.json downloaded if you’re running this somewhere other than on Kaggle. setup_comp is the function we use in fastkaggle to grab the data, and install or upgrade our needed python modules when we’re running on Kaggle:\n\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\n\ndatapath = Path('./Data')\nname = 'paddy-disease-classification'\npath = Path(f'{datapath}/{name}')\n\nkaggle_competition_download(name, datapath)\n\nDownloading paddy-disease-classification.zip to Data/paddy-disease-classification\n\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████| 1.02G/1.02G [02:55&lt;00:00, 6.25MB/s]\n\n\n\npath\n\nPosixPath('Data/paddy-disease-classification')\n\n\nNow we can import the stuff we’ll need from fastai, set a seed (for reproducibility – just for the purposes of making this notebook easier to write; I don’t recommend doing that in your own analysis however) and check what’s in the data:\n\nfrom fastai.vision.all import *\nset_seed(42)\n\npath.ls()\n\n(#5) [Path('Data/paddy-disease-classification/paddy-disease-classification.zip'),Path('Data/paddy-disease-classification/train_images'),Path('Data/paddy-disease-classification/train.csv'),Path('Data/paddy-disease-classification/sample_submission.csv'),Path('Data/paddy-disease-classification/test_images')]",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "kaggle_tut.html#looking-at-the-data",
    "href": "kaggle_tut.html#looking-at-the-data",
    "title": "Kaggle tutorial",
    "section": "Looking at the data",
    "text": "Looking at the data\nThe images are in train_images, so let’s grab a list of all of them:\n\ntrn_path = path/'train_images'\nfiles = get_image_files(trn_path)\n\n…and take a look at one:\n\nimg = PILImage.create(files[0])\nprint(img.size)\nimg.to_thumb(128)\n\n(480, 640)\n\n\n\n\n\n\n\n\n\nLooks like the images might be 480x640 – let’s check all their sizes. This is faster if we do it in parallel, so we’ll use fastcore’s parallel for this:\n\nfrom fastcore.parallel import *\n\n\ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files, n_workers=16)\npd.Series(sizes).value_counts()\n\n(480, 640)    10403\n(640, 480)        4\ndtype: int64\n\n\nThey’re nearly all the same size, except for a few. Because of those few, however, we’ll need to make sure we always resize each image to common dimensions first, otherwise fastai won’t be able to create batches. For now, we’ll just squish them to 480x480 images, and then once they’re in batches we do a random resized crop down to a smaller size, along with the other default fastai augmentations provided by aug_transforms. We’ll start out with small resized images, since we want to be able to iterate quickly:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize((480,360), method='squish'),\n    batch_tfms=aug_transforms(size=(128,128), min_scale=0.75))\n\ndls.show_batch(max_n=6)",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "kaggle_tut.html#our-first-model",
    "href": "kaggle_tut.html#our-first-model",
    "title": "Kaggle tutorial",
    "section": "Our first model",
    "text": "Our first model\nLet’s create a model. To pick an architecture, we should look at the options in The best vision models for fine-tuning. I like the looks of resnet26d, which is the fastest resolution-independent model which gets into the top-15 lists there.\n\nimport timm\n\n\n??timm\n\n\nType:        module\nString form: &lt;module 'timm' from '/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/timm/__init__.py'&gt;\nFile:        ~/mambaforge/envs/cfast/lib/python3.11/site-packages/timm/__init__.py\nSource:     \nfrom .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n    is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value\n\n\n\n\ntimm.list_models('convnext*')\n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']\n\n\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.').to_fp16()\n\n\n\n\nLet’s see what the learning rate finder shows:\n\nlearn.lr_find(suggest_funcs=(valley, slide))\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083, slide=0.0014454397605732083)\n\n\n\n\n\n\n\n\n\nlr_find generally recommends rather conservative learning rates, to ensure that your model will train successfully. I generally like to push it a bit higher if I can. Let’s train a few epochs and see how it looks:\n\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.014854\n1.233058\n0.388275\n01:01\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.348343\n0.917006\n0.298895\n01:03\n\n\n1\n1.131407\n0.720297\n0.234503\n01:07\n\n\n2\n0.927136\n0.603111\n0.189332\n01:15\n\n\n3\n0.790826\n0.523225\n0.161941\n01:16\n\n\n4\n0.710640\n0.512586\n0.159058\n01:12\n\n\n\n\n\nWe’re now ready to build our first submission. Let’s take a look at the sample Kaggle provided to see what it needs to look like:\n\narch = 'convnext_small'\n\n\nlearn2 = vision_learner(dls, arch, metrics=error_rate, path='.').to_fp16()\n\n\nlearn2.fine_tune(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.611437\n0.826959\n0.266699\n04:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.785174\n0.426409\n0.135512\n07:11",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "kaggle_tut.html#submitting-to-kaggle",
    "href": "kaggle_tut.html#submitting-to-kaggle",
    "title": "Kaggle tutorial",
    "section": "Submitting to Kaggle",
    "text": "Submitting to Kaggle\n\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nNaN\n\n\n1\n200002.jpg\nNaN\n\n\n2\n200003.jpg\nNaN\n\n\n3\n200004.jpg\nNaN\n\n\n4\n200005.jpg\nNaN\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\nNaN\n\n\n3465\n203466.jpg\nNaN\n\n\n3466\n203467.jpg\nNaN\n\n\n3467\n203468.jpg\nNaN\n\n\n3468\n203469.jpg\nNaN\n\n\n\n\n3469 rows × 2 columns\n\n\n\nOK so we need a CSV containing all the test images, in alphabetical order, and the predicted label for each one. We can create the needed test set using fastai like so:\n\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = dls.test_dl(tst_files)\n\nWe can now get the probabilities of each class, and the index of the most likely class, from this test set (the 2nd thing returned by get_preds are the targets, which are blank for a test set, so we discard them):\n\nprobs,_,idxs = learn2.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n\n\n\n\n\n\n\ntensor([7, 8, 6,  ..., 8, 7, 5])\n\n\nThese need to be mapped to the names of each of these diseases, these names are stored by fastai automatically in the vocab:\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\nWe can create an apply this mapping using pandas:\n\nmapping = dict(enumerate(dls.vocab))\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping)\nresults\n\n0              hispa\n1             normal\n2       downy_mildew\n3              blast\n4              blast\n            ...     \n3464      dead_heart\n3465           hispa\n3466          normal\n3467           hispa\n3468      dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\nKaggle expects the submission as a CSV file, so let’s save it, and check the first few lines:\n\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,downy_mildew\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,hispa\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nLet’s submit this to kaggle. We can do it from the notebook if we’re running on Kaggle, otherwise we can use the API:\n\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial rn26d 128px', name)\n\nNameError: name 'iskaggle' is not defined\n\n\nSuccess! We successfully created a submission.",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "kaggle_tut.html#conclusion",
    "href": "kaggle_tut.html#conclusion",
    "title": "Kaggle tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nOur initial submission is not very good (top 80% of teams) but it only took a minute to train. The important thing is that we have a good starting point to iterate from, and we can do rapid iterations. Every step from loading the data to creating the model to submitting to Kaggle is all automated and runs quickly.\nTherefore, we can now try lots of things quickly and easily and use those experiments to improve our results. In the next notebook, we’ll do exactly that! So if you’re ready, take a look at part 2 of the series.\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. And if you have any questions or comments, please pop them below – I read every comment I receive!",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "kaggle_tut.html#addendum",
    "href": "kaggle_tut.html#addendum",
    "title": "Kaggle tutorial",
    "section": "Addendum",
    "text": "Addendum\nfastkaggle also provides a function that pushes a notebook to Kaggle Notebooks. I wrote this notebook on my own machine, and pushed it to Kaggle from there – here’s the command I used:\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\n\nif not iskaggle:\n    push_notebook('Benson Thekkel', '060_Kaggle_tut',\n                  title='060_Kaggle_tut1',\n                  file='060_Kaggle_tut.ipynb',\n                  competition=name, private=False, gpu=True)\n\nYour kernel title does not resolve to the specified id. This may result in surprising behavior. We suggest making your title something that resolves to the specified id. See https://en.wikipedia.org/wiki/Clean_URL#Slug for more information on how slugs are determined.\nKernel version 1 successfully pushed.  Please check progress at https://www.kaggle.com/code/bensonthekkel/060-kaggle-tut1",
    "crumbs": [
      "Blog",
      "Kaggle tutorial"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html",
    "href": "stable diffusion deep dive.html",
    "title": "Stable Diffusion Deep Dive",
    "section": "",
    "text": "Stable Diffusion is a powerful text-to-image model. There are various websites and tools to make using it as easy as possible. It is also integrated into the Huggingface diffusers library where generating images can be as simple as:\nIn this notebook we’re going to dig into the code behind these easy-to-use interfaces, to see what is going on under the hood. We’ll begin by re-creating the functionality above as a scary chunk of code, and then one by one we’ll inspect the different components and figure out what they do. By the end of this notebook that same sampling loop should feel like something you can tweak and modify as you like.",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#setup-imports",
    "href": "stable diffusion deep dive.html#setup-imports",
    "title": "Stable Diffusion Deep Dive",
    "section": "Setup & Imports",
    "text": "Setup & Imports\nYou’ll need to log into huggingface and accept the terms of the licence for this model - see the model card for details. And when you first run this notebook you need to uncomment the following two cells to install the requirements and log in to huggingface with an access token.\n\n# !pip install -q --upgrade transformers==4.25.1 diffusers ftfy accelerate\n\n\nfrom base64 import b64encode\n\nimport numpy\nimport torch\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\nfrom huggingface_hub import notebook_login\n\n# For video display:\nfrom IPython.display import HTML\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom torch import autocast\nfrom torchvision import transforms as tfms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer, logging\nimport os\n\ntorch.manual_seed(1)\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n# Set device\ntorch_device = \"cpu\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nif \"mps\" == torch_device: os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\"",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#loading-the-models",
    "href": "stable diffusion deep dive.html#loading-the-models",
    "title": "Stable Diffusion Deep Dive",
    "section": "Loading the models",
    "text": "Loading the models\nThis code (and that in the next section) comes from the Huggingface example notebook.\nThis will download and set up the relevant models and components we’ll be using. Let’s just run this for now and move on to the next section to check that it all works before diving deeper.\nIf you’ve loaded a pipeline, you can also access these components using pipe.unet, pipe.vae and so on.\nIn this notebook we aren’t doing any memory-saving tricks - if you find yourself running out of GPU RAM, look at the pipeline code for inspiration with things like attention slicing, switching to half precision (fp16), keeping the VAE on the CPU and other modifications.\n\n# Load the autoencoder model which will be used to decode the latents into image space.\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Load the tokenizer and text encoder to tokenize and encode the text.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# The UNet model for generating the latents.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n\n# The noise scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\n\n# To the GPU we go!\nvae = vae.to(torch_device)\ntext_encoder = text_encoder.to(torch_device)\nunet = unet.to(torch_device);",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#a-diffusion-loop",
    "href": "stable diffusion deep dive.html#a-diffusion-loop",
    "title": "Stable Diffusion Deep Dive",
    "section": "A diffusion loop",
    "text": "A diffusion loop\nIf all you want is to make a picture with some text, you could ignore this notebook and use one of the existing tools (such as DreamStudio) or use the simplified pipeline from huggingface, as documented here.\nWhat we want to do in this notebook is dig a little deeper into how this works, so we’ll start by checking that the example code runs. Again, this is adapted from the HF notebook and looks very similar to what you’ll find if you inspect the __call__() method of the stable diffusion pipeline.\n\n# Prep Scheduler\ndef set_timesteps(scheduler, num_inference_steps):\n    scheduler.set_timesteps(num_inference_steps)\n    scheduler.timesteps = scheduler.timesteps.to(torch.float32) # minor fix to ensure MPS compatibility, fixed in diffusers PR 3925\n\n\n# Some settings\nprompt = [\"A watercolor painting of an otter\"]\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 30            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\n\n# Prep text\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n\nset_timesteps(scheduler,num_inference_steps)\n\n# Prep latents\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n\n# Loop\nwith autocast(\"cpu\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        # Scale the latents (preconditioning):\n        # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -&gt; x_t-1\n        # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\n\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n# Display\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\npil_images[0]\n\n\n\n\n\n\n\n\nIt’s working, but that’s quite a bit of code! Let’s look at the components one by one.",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#the-autoencoder-ae",
    "href": "stable diffusion deep dive.html#the-autoencoder-ae",
    "title": "Stable Diffusion Deep Dive",
    "section": "The Autoencoder (AE)",
    "text": "The Autoencoder (AE)\nThe AE can ‘encode’ an image into some sort of latent representation, and decode this back into an image. I’ve wrapped the code for this into a couple of functions here so we can see what this looks like in action:\n\ndef pil_to_latent(input_im):\n    # Single image -&gt; single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # bath of latents -&gt; list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\nWe’ll use a pic from the web here, but you can load your own instead by uploading it and editing the filename in the next cell.\n\n# Download a demo Image\n!curl --output macaw.jpg 'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 62145  100 62145    0     0  57588      0  0:00:01  0:00:01 --:--:-- 57648\n\n\n\n# Load the image with PIL\ninput_image = Image.open('macaw.jpg').resize((512, 512))\ninput_image\n\n\n\n\n\n\n\n\nEncoding this into the latent space of the AE with the function defined above looks like this:\n\n# Encode to the latent space\nencoded = pil_to_latent(input_image)\nencoded.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\n# Let's visualize the four channels of this latent representation:\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(encoded[0][c].cpu(), cmap='Greys')\n\n\n\n\n\n\n\n\nThis 4x64x64 tensor captures lots of information about the image, hopefully enough that when we feed it through the decoder we get back something very close to our input image:\n\n# Decode this latent representation back into an image\ndecoded = latents_to_pil(encoded)[0]\ndecoded\n\n\n\n\n\n\n\n\nYou’ll see some small differences if you squint! Forcus on the eye if you can’t see anything obvious. This is pretty impressive - that 4x64x64 latent seems to hold a lot more information that a 64px image…\nThis autoencoder has been trained to squish down an image to a smaller representation and then re-create the image back from this compressed version again.\nIn this particular case the compression factor is 48, we start with a 3x512x512(chxhtxwd) image and it get compressed to a latent vector 4x64x64. Each 3x8x8 pixel volume in the input image gets compressed down to just 4 numbers(4x1x1). You can find AEs with a higher compression ratio (eg f16 like some popular VQGAN models) but at some point they begin to introduce artifacts that we don’t want.\nWhy do we even use an autoencoder? We can do diffusion in pixel space - where the model gets all the image data as inputs and produces an output prediction of the same shape. But this means processing a LOT of data, and make high-resolution generation very computationally expensive. Some solutions to this involve doing diffusion at low resolution (64px for eg) and then training a separate model to upscale repeatedly (as with D2/Imagen). But latent diffusion instead does the diffusion process in this ‘latent space’, using the compressed representations from our AE rather than raw images. These representations are information rich, and can be small enough to handle manageably on consumer hardware. Once we’ve generated a new ‘image’ as a latent representation, the autoencoder can take those final latent outputs and turn them into actual pixels.",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#loop-starting-from-noised-version-of-input-aka-image2image",
    "href": "stable diffusion deep dive.html#loop-starting-from-noised-version-of-input-aka-image2image",
    "title": "Stable Diffusion Deep Dive",
    "section": "Loop starting from noised version of input (AKA image2image)",
    "text": "Loop starting from noised version of input (AKA image2image)\nLet’s see what happens when we use our image as a starting point, adding some noise and then doing the final few denoising steps in the loop with a new prompt.\nWe’ll use a similar loop to the first demo, but we’ll skip the first start_step steps.\nTo noise our image we’ll use code like that shown above, using the scheduler to noise it to a level equivalent to step 10 (start_step).\n\n# Settings (same as before except for the new prompt)\nprompt = [\"A colorful dancer, nat geo photo\"]\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 50            # Number of denoising steps\nguidance_scale = 8                  # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\n\n# Prep text (same as before)\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n# Prep Scheduler (setting the number of inference steps)\nset_timesteps(scheduler, num_inference_steps)\n\n# Prep latents (noising appropriately for start_step)\nstart_step = 10\nstart_sigma = scheduler.sigmas[start_step]\nnoise = torch.randn_like(encoded)\nlatents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\nlatents = latents.to(torch_device).float()\n\n# Loop\nfor i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n    if i &gt;= start_step: # &lt;&lt; This is the only modification to the loop we do\n\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -&gt; x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\nlatents_to_pil(latents)[0]\n\n\n\n\n\n\n\n\n\n\n\nYou can see that some colours and structure from the image are kept, but we now have a new picture! The more noise you add and the more steps you do, the further away it gets from the input image.\nThis is how the popular img2img pipeline works. Again, if this is your end goal there are tools to make this easy!\nBut you can see that under the hood this is the same as the generation loop just skipping the first few steps and starting from a noised image rather than pure noise.\nExplore changing how many steps are skipped and see how this affects the amount the image changes from the input.",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#exploring-the-text---embedding-pipeline",
    "href": "stable diffusion deep dive.html#exploring-the-text---embedding-pipeline",
    "title": "Stable Diffusion Deep Dive",
    "section": "Exploring the text -> embedding pipeline",
    "text": "Exploring the text -&gt; embedding pipeline\nWe use a text encoder model to turn our text into a set of ‘embeddings’ which are fed to the diffusion model as conditioning. Let’s follow a piece of text through this process and see how it works.\n\n# Our text prompt\nprompt = 'A picture of a puppy'\n\nWe begin with tokenization:\n\n# Turn the text into a sequnce of tokens:\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_input['input_ids'][0] # View the tokens\n\ntensor([49406,   320,  1674,   539,   320,  6829, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407])\n\n\n\n# See the individual tokens\nfor t in text_input['input_ids'][0][:8]: # We'll just look at the first 7 to save you from a wall of '&lt;|endoftext|&gt;'\n    print(t, tokenizer.decoder.get(int(t)))\n\ntensor(49406) &lt;|startoftext|&gt;\ntensor(320) a&lt;/w&gt;\ntensor(1674) picture&lt;/w&gt;\ntensor(539) of&lt;/w&gt;\ntensor(320) a&lt;/w&gt;\ntensor(6829) puppy&lt;/w&gt;\ntensor(49407) &lt;|endoftext|&gt;\ntensor(49407) &lt;|endoftext|&gt;\n\n\n\n# TODO call out that 6829 is puppy\n\nWe can jump straight to the final (output) embeddings like so:\n\n# Grab the output embeddings\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nprint('Shape:', output_embeddings.shape)\noutput_embeddings\n\nShape: torch.Size([1, 77, 768])\n\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],\n         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],\n         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],\n       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\nWe pass our tokens through the text_encoder and we magically get some numbers we can feed to the model.\nHow are these generated? The tokens are transformed into a set of input embeddings, which are then fed through the transformer model to get the final output embeddings.\nTo get these input embeddings, there are actually two steps - as revealed by inspecting text_encoder.text_model.embeddings:\n\ntext_encoder.text_model.embeddings\n\nCLIPTextEmbeddings(\n  (token_embedding): Embedding(49408, 768)\n  (position_embedding): Embedding(77, 768)\n)\n\n\n\nToken embeddings\nThe token is fed to the token_embedding to transform it into a vector. The function name get_input_embeddings here is misleading since these token embeddings need to be combined with the position embeddings before they are actually used as inputs to the model! Anyway, let’s look at just the token embedding part first\nWe can look at the embedding layer:\n\n# Access the embedding layer\ntoken_emb_layer = text_encoder.text_model.embeddings.token_embedding\ntoken_emb_layer # Vocab size 49408, emb_dim 768\n\nEmbedding(49408, 768)\n\n\nAnd embed a token like so:\n\n# Embed a token - in this case the one for 'puppy'\nembedding = token_emb_layer(torch.tensor(6829, device=torch_device))\nembedding.shape # 768-dim representation\n\ntorch.Size([768])\n\n\nThis single token has been mapped to a 768-dimensional vector - the token embedding.\nWe can do the same with all of the tokens in the prompt to get all the token embeddings:\n\ntoken_embeddings = token_emb_layer(text_input.input_ids.to(torch_device))\nprint(token_embeddings.shape) # batch size 1, 77 tokens, 768 values for each\ntoken_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[ 0.0011,  0.0032,  0.0003,  ..., -0.0018,  0.0003,  0.0019],\n         [ 0.0013, -0.0011, -0.0126,  ..., -0.0124,  0.0120,  0.0080],\n         [ 0.0235, -0.0118,  0.0110,  ...,  0.0049,  0.0078,  0.0160],\n         ...,\n         [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052],\n         [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052],\n         [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052]]],\n       device='cuda:0', grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\nPositional Embeddings\nPositional embeddings tell the model where in a sequence a token is. Much like the token embedding, this is a set of (optionally learnable) parameters. But now instead of dealing with ~50k tokens we just need one for each position (77 total):\n\npos_emb_layer = text_encoder.text_model.embeddings.position_embedding\npos_emb_layer\n\nEmbedding(77, 768)\n\n\nWe can get the positional embedding for each position:\n\nposition_ids = text_encoder.text_model.embeddings.position_ids[:, :77]\nposition_embeddings = pos_emb_layer(position_ids)\nprint(position_embeddings.shape)\nposition_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[ 0.0016,  0.0020,  0.0002,  ..., -0.0013,  0.0008,  0.0015],\n         [ 0.0042,  0.0029,  0.0002,  ...,  0.0010,  0.0015, -0.0012],\n         [ 0.0018,  0.0007, -0.0012,  ..., -0.0029, -0.0009,  0.0026],\n         ...,\n         [ 0.0216,  0.0055, -0.0101,  ..., -0.0065, -0.0029,  0.0037],\n         [ 0.0188,  0.0073, -0.0077,  ..., -0.0025, -0.0009,  0.0057],\n         [ 0.0330,  0.0281,  0.0289,  ...,  0.0160,  0.0102, -0.0310]]],\n       device='cuda:0', grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\nCombining token and position embeddings\nTime to combine the two. How do we do this? Just add them! Other approaches are possible but for this model this is how it is done.\nCombining them in this way gives us the final input embeddings ready to feed through the transformer model:\n\n# And combining them we get the final input embeddings\ninput_embeddings = token_embeddings + position_embeddings\nprint(input_embeddings.shape)\ninput_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[ 2.6770e-03,  5.2133e-03,  4.9323e-04,  ..., -3.1321e-03,\n           1.0659e-03,  3.4316e-03],\n         [ 5.5371e-03,  1.7510e-03, -1.2381e-02,  ..., -1.1410e-02,\n           1.3508e-02,  6.8378e-03],\n         [ 2.5356e-02, -1.1019e-02,  9.7663e-03,  ...,  1.9460e-03,\n           6.8375e-03,  1.8573e-02],\n         ...,\n         [ 2.2781e-02,  1.3262e-02, -1.1241e-02,  ..., -8.0054e-03,\n          -2.0560e-03,  8.9366e-03],\n         [ 2.0026e-02,  1.5015e-02, -8.7638e-03,  ..., -4.0313e-03,\n           1.8487e-05,  1.0885e-02],\n         [ 3.4206e-02,  3.5826e-02,  2.7768e-02,  ...,  1.4465e-02,\n           1.1110e-02, -2.5745e-02]]], device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\nWe can check that these are the same as the result we’d get from text_encoder.text_model.embeddings:\n\n# The following combines all the above steps (but doesn't let us fiddle with them!)\ntext_encoder.text_model.embeddings(text_input.input_ids.to(torch_device))\n\ntensor([[[ 2.6770e-03,  5.2133e-03,  4.9323e-04,  ..., -3.1321e-03,\n           1.0659e-03,  3.4316e-03],\n         [ 5.5371e-03,  1.7510e-03, -1.2381e-02,  ..., -1.1410e-02,\n           1.3508e-02,  6.8378e-03],\n         [ 2.5356e-02, -1.1019e-02,  9.7663e-03,  ...,  1.9460e-03,\n           6.8375e-03,  1.8573e-02],\n         ...,\n         [ 2.2781e-02,  1.3262e-02, -1.1241e-02,  ..., -8.0054e-03,\n          -2.0560e-03,  8.9366e-03],\n         [ 2.0026e-02,  1.5015e-02, -8.7638e-03,  ..., -4.0313e-03,\n           1.8487e-05,  1.0885e-02],\n         [ 3.4206e-02,  3.5826e-02,  2.7768e-02,  ...,  1.4465e-02,\n           1.1110e-02, -2.5745e-02]]], device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nFeeding these through the transformer model\n\n\n\ntransformer diagram\n\n\nWe want to mess with these input embeddings (specifically the token embeddings) before we send them through the rest of the model, but first we should check that we know how to do that. I read the code of the text_encoders forward method, and based on that the code for the forward method of the text_model that the text_encoder wraps. To inspect it yourself, type ??text_encoder.text_model.forward and you’ll get the function info and source code - a useful debugging trick!\nAnyway, based on that we can copy in the bits we need to get the so-called ‘last hidden state’ and thus generate our final embeddings:\n\ndef get_output_embeds(input_embeddings):\n    # CLIP's text model uses causal mask, so we prepare it here:\n    bsz, seq_len = input_embeddings.shape[:2]\n    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(bsz, seq_len, dtype=input_embeddings.dtype)\n\n    # Getting the output embeddings involves calling the model with passing output_hidden_states=True\n    # so that it doesn't just return the pooled final predictions:\n    encoder_outputs = text_encoder.text_model.encoder(\n        inputs_embeds=input_embeddings,\n        attention_mask=None, # We aren't using an attention mask so that can be None\n        causal_attention_mask=causal_attention_mask.to(torch_device),\n        output_attentions=None,\n        output_hidden_states=True, # We want the output embs not the final output\n        return_dict=None,\n    )\n\n    # We're interested in the output hidden state only\n    output = encoder_outputs[0]\n\n    # There is a final layer norm we need to pass these through\n    output = text_encoder.text_model.final_layer_norm(output)\n\n    # And now they're ready!\n    return output\n\nout_embs_test = get_output_embeds(input_embeddings) # Feed through the model with our new function\nprint(out_embs_test.shape) # Check the output shape\nout_embs_test # Inspect the output\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],\n         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],\n         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],\n       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\nNote that these match the output_embeddings we saw near the start - we’ve figured out how to split up that one step (“get the text embeddings”) into multiple sub-steps ready for us to modify.\nNow that we have this process in place, we can replace the input embedding of a token with a new one of our choice - which in our final use-case will be something we learn. To demonstrate the concept though, let’s replace the input embedding for ‘puppy’ in the prompt we’ve been playing with with the embedding for token 2368, get a new set of output embeddings based on this, and use these to generate an image to see what we get:\n\nprompt = 'A picture of a puppy'\n\n# Tokenize\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n\n# Get token embeddings\ntoken_embeddings = token_emb_layer(input_ids)\n\n# The new embedding. In this case just the input embedding of token 2368...\nreplacement_token_embedding = text_encoder.get_input_embeddings()(torch.tensor(2368, device=torch_device))\n\n# Insert this into the token embeddings (\ntoken_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\n# Combine with pos embs\ninput_embeddings = token_embeddings + position_embeddings\n\n#  Feed through to get final output embs\nmodified_output_embeddings = get_output_embeds(input_embeddings)\n\nprint(modified_output_embeddings.shape)\nmodified_output_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.6034, -0.5322,  0.0629,  ..., -0.3964,  0.0877, -0.9558],\n         [-0.5936, -0.5407,  0.0731,  ..., -0.3876,  0.0906, -0.9436],\n         [-0.6393, -0.4703,  0.1103,  ..., -0.3904,  0.1351, -0.9726]]],\n       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\nThe first few are the same, the last aren’t. Everything at and after the position of the token we’re replacing will be affected.\nIf all went well, we should see something other than a puppy when we use these to generate an image. And sure enough, we do!\n\n#Generating an image with these modified embeddings\n\ndef generate_with_embs(text_embeddings):\n    height = 512                        # default height of Stable Diffusion\n    width = 512                         # default width of Stable Diffusion\n    num_inference_steps = 30            # Number of denoising steps\n    guidance_scale = 7.5                # Scale for classifier-free guidance\n    generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n    batch_size = 1\n\n    max_length = text_input.input_ids.shape[-1]\n    uncond_input = tokenizer(\n      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n    # Prep Scheduler\n    set_timesteps(scheduler, num_inference_steps)\n\n    # Prep latents\n    latents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n    )\n    latents = latents.to(torch_device)\n    latents = latents * scheduler.init_noise_sigma\n\n    # Loop\n    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -&gt; x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    return latents_to_pil(latents)[0]\n\n\ngenerate_with_embs(modified_output_embeddings)\n\n/tmp/ipykernel_8407/342776162.py:24: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),\n\n\n\n\n\n\n\n\n\n\n\n\nSuprise! Now you know what token 2368 means ;)\nWhat can we do with this? Why did we go to all of this trouble? Well, we’ll see a more compelling use-case shortly but the tl;dr is that once we can access and modify the token embeddings we can do tricks like replacing them with something else. In the example we just did, that was just another token embedding from the model’s vocabulary, equivalent to just editing the prompt. But we can also mix tokens - for example, here’s a half-puppy-half-skunk:\n\n# In case you're wondering how to get the token for a word, or the embedding for a token:\nprompt = 'skunk'\nprint('tokenizer(prompt):', tokenizer(prompt))\nprint('token_emb_layer([token_id]) shape:', token_emb_layer(torch.tensor([8797], device=torch_device)).shape)\n\ntokenizer(prompt): {'input_ids': [49406, 42194, 49407], 'attention_mask': [1, 1, 1]}\ntoken_emb_layer([token_id]) shape: torch.Size([1, 768])\n\n\n\nprompt = 'A picture of a puppy'\n\n# Tokenize\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n\n# Get token embeddings\ntoken_embeddings = token_emb_layer(input_ids)\n\n# The new embedding. Which is now a mixture of the token embeddings for 'puppy' and 'skunk'\npuppy_token_embedding = token_emb_layer(torch.tensor(6829, device=torch_device))\nskunk_token_embedding = token_emb_layer(torch.tensor(42194, device=torch_device))\nreplacement_token_embedding = 0.5*puppy_token_embedding + 0.5*skunk_token_embedding\n\n# Insert this into the token embeddings (\ntoken_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\n# Combine with pos embs\ninput_embeddings = token_embeddings + position_embeddings\n\n#  Feed through to get final output embs\nmodified_output_embeddings = get_output_embeds(input_embeddings)\n\n# Generate an image with these\ngenerate_with_embs(modified_output_embeddings)\n\n/tmp/ipykernel_8407/342776162.py:24: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTextual Inversion\nOK, so we can slip in a modified token embedding, and use this to generate an image. We used the token embedding for ‘cat’ in the above example, but what if instead could ‘learn’ a new token embedding for a specific concept? This is the idea behind ‘Textual Inversion’, in which a few example images are used to create a new token embedding:\n Diagram from the textual inversion blog post - note it doesn’t show the positional embeddings step for simplicity\nWe won’t cover how this training works, but we can try loading one of these new ‘concepts’ from the community-created SD concepts library and see how it fits in with our example above. I’ll use https://huggingface.co/sd-concepts-library/birb-style since it was the first one I made :) Download the learned_embeds.bin file from there and upload the file to wherever this notebook is before running this next cell:\n\nbirb_embed = torch.load('learned_embeds.bin')\nbirb_embed.keys(), birb_embed['&lt;birb-style&gt;'].shape\n\n(dict_keys(['&lt;birb-style&gt;']), torch.Size([768]))\n\n\nWe get a dictionary with a key (the special placeholder I used, ) and the corresponding token embedding. As in the previous example, let’s replace the ‘puppy’ token embedding with this and see what happens:\n\nprompt = 'A mouse in the style of puppy'\n\n# Tokenize\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n\n# Get token embeddings\ntoken_embeddings = token_emb_layer(input_ids)\n\n# The new embedding - our special birb word\nreplacement_token_embedding = birb_embed['&lt;birb-style&gt;'].to(torch_device)\n\n# Insert this into the token embeddings\ntoken_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\n# Combine with pos embs\ninput_embeddings = token_embeddings + position_embeddings\n\n#  Feed through to get final output embs\nmodified_output_embeddings = get_output_embeds(input_embeddings)\n\n# And generate an image with this:\ngenerate_with_embs(modified_output_embeddings)\n\n/tmp/ipykernel_8407/342776162.py:24: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),\n\n\n\n\n\n\n\n\n\n\n\n\nThe token for ‘puppy’ was replaced with one that captures a particular style of painting, but it could just as easily represent a specific object or class of objects.\nAgain, there is a nice inference notebook from hf to make it easy to use the different concepts, that properly handles using the names in prompts (“A &lt;cat-toy&gt; in the style of &lt;birb-style&gt;”) without worrying about all this manual stuff. The goal of this notebook is to pull back the curtain a bit so you know what is going on behind the scenes :)",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#messing-with-embeddings",
    "href": "stable diffusion deep dive.html#messing-with-embeddings",
    "title": "Stable Diffusion Deep Dive",
    "section": "Messing with Embeddings",
    "text": "Messing with Embeddings\nBesides just replacing the token embedding of a single word, there are various other tricks we can try. For example, what if we create a ‘chimera’ by averaging the embeddings of two different prompts?\n\n# Embed two prompts\ntext_input1 = tokenizer([\"A mouse\"], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_input2 = tokenizer([\"A leopard\"], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings1 = text_encoder(text_input1.input_ids.to(torch_device))[0]\n    text_embeddings2 = text_encoder(text_input2.input_ids.to(torch_device))[0]\n\n# Mix them together\nmix_factor = 0.35\nmixed_embeddings = (text_embeddings1*mix_factor + \\\n                   text_embeddings2*(1-mix_factor))\n\n# Generate!\ngenerate_with_embs(mixed_embeddings)\n\n/tmp/ipykernel_8407/342776162.py:24: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#the-unet-and-cfg",
    "href": "stable diffusion deep dive.html#the-unet-and-cfg",
    "title": "Stable Diffusion Deep Dive",
    "section": "The UNET and CFG",
    "text": "The UNET and CFG\nNow it’s time we looked at the actual diffusion model. This is typically a Unet that takes in the noisy latents (x) and predicts the noise. We use a conditional model that also takes in the timestep (t) and our text embedding (aka encoder_hidden_states) as conditioning. Feeding all of these into the model looks like this: noise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\nWe can try it out and see what the output looks like:\n\n# Prep Scheduler\nset_timesteps(scheduler, num_inference_steps)\n\n# What is our timestep\nt = scheduler.timesteps[0]\nsigma = scheduler.sigmas[0]\n\n# A noisy latent\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma\n\n# Text embedding\ntext_input = tokenizer(['A macaw'], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n\n# Run this through the unet to predict the noise residual\nwith torch.no_grad():\n    noise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\nlatents.shape, noise_pred.shape # We get preds in the same shape as the input\n\n/tmp/ipykernel_8407/440475445.py:10: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),\n\n\n(torch.Size([1, 4, 64, 64]), torch.Size([1, 4, 64, 64]))\n\n\nGiven a set of noisy latents, the model predicts the noise component. We can remove this noise from the noisy latents to see what the output image looks like (latents_x0 = latents - sigma * noise_pred). And we can add most of the noise back to this predicted output to get the (slightly less noisy hopefully) input for the next diffusion step. To visualize this let’s generate another image, saving both the predicted output (x0) and the next step (xt-1) after every step:\n\nprompt = 'Oil painting of an otter in a top hat'\nheight = 512\nwidth = 512\nnum_inference_steps = 50\nguidance_scale = 8\ngenerator = torch.manual_seed(32)\nbatch_size = 1\n\n# Make a folder to store results\n!rm -rf steps/\n!mkdir -p steps/\n\n# Prep text\ntext_input = tokenizer([prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n# Prep Scheduler\nset_timesteps(scheduler, num_inference_steps)\n\n# Prep latents\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma\n\n# Loop\nfor i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n    sigma = scheduler.sigmas[i]\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n    # Get the predicted x0:\n    # latents_x0 = latents - sigma * noise_pred # Calculating ourselves\n    latents_x0 = scheduler.step(noise_pred, t, latents).pred_original_sample # Using the scheduler (Diffusers 0.4 and above)\n\n    # compute the previous noisy sample x_t -&gt; x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    # To PIL Images\n    im_t0 = latents_to_pil(latents_x0)[0]\n    im_next = latents_to_pil(latents)[0]\n\n    # Combine the two images and save for later viewing\n    im = Image.new('RGB', (1024, 512))\n    im.paste(im_next, (0, 0))\n    im.paste(im_t0, (512, 0))\n    im.save(f'steps/{i:04}.jpeg')\n\n/tmp/ipykernel_8407/2488592417.py:30: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),\n\n\n\n\n\n\n# Make and show the progress video (change width to 1024 for full res)\n!ffmpeg -v 1 -y -f image2 -framerate 12 -i steps/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\nmp4 = open('out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n&lt;video width=600 controls&gt;\n      &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n&lt;/video&gt;\n\"\"\" % data_url)\n\n\n\n      \n\n\n\nThe version on the right shows the predicted ‘final output’ (x0) at each step, and this is what is usually used for progress videos etc. The version on the left is the ‘next step’. I found it interesteing to compare the two - watching the progress videos only you’d think drastic changes are happening expecially at early stages, but since the changes made per-step are relatively small the actual process is much more gradual.\n\nClassifier Free Guidance\nBy default, the model doesn’t often do what we ask. If we want it to follow the prompt better, we use a hack called CFG. There’s a good explanation in this video (AI coffee break GLIDE).\nIn the code, this comes down to us doing:\nnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\nThis works suprisingly well :) Explore changing the guidance_scale in the code above and see how this affects the results. How high can you push it before the results get worse?",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "stable diffusion deep dive.html#sampling",
    "href": "stable diffusion deep dive.html#sampling",
    "title": "Stable Diffusion Deep Dive",
    "section": "Sampling",
    "text": "Sampling\nThere is still some complexity hidden from us inside latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]. How exactly does the sampler go from the current noisy latents to a slightly less noisy version? Why don’t we just use the model in a single step? Are there other ways to view this?\nThe model tries to predict the noise in an image. For low noise values, we assume it does a pretty good job. For higher noise levels, it has a hard task! So instead of producing a perfect image, the results tend to look like a blurry mess - see the start of the video above for a visual! So, samplers use the model predictions to move a small amount towards the model prediction (removing some of the noise) and then get another prediction based on this marginally-less-rubbish input, and hope that this iteratively improves the result.\nDifferent samplers do this in different ways. You can try to inspect the code for the default LMS sampler with:\n\n# ??scheduler.step\n\nTime to draw some diagrams! (Whiteboard/paper interlude)",
    "crumbs": [
      "Blog",
      "Stable Diffusion Deep Dive"
    ]
  },
  {
    "objectID": "accel_sgd.html",
    "href": "accel_sgd.html",
    "title": "Accelerated SGD",
    "section": "",
    "text": "Exported source\nimport torch\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nExported source\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nExported source\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\nbs = 1024\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nlrf_cbs = [DeviceCB(), LRFinderCB()]",
    "crumbs": [
      "Blog",
      "Accelerated SGD"
    ]
  },
  {
    "objectID": "accel_sgd.html#optimizers",
    "href": "accel_sgd.html#optimizers",
    "title": "Accelerated SGD",
    "section": "Optimizers",
    "text": "Optimizers\n\nSGD\n\nsource\n\n\nSGD\n\n SGD (params, lr, wd=0.0)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass SGD:\n    def __init__(self, params, lr, wd=0.):\n        params = list(params)\n        fc.store_attr()\n        self.i = 0\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params:\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i +=1\n\n    def opt_step(self, p): p -= p.grad * self.lr\n    def reg_step(self, p):\n        if self.wd != 0: p *= 1 - self.lr*self.wd\n\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=0.4, cbs=cbs, opt_func=SGD)\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.772\n0.640\n0\ntrain\n\n\n0.824\n0.477\n0\neval\n\n\n0.845\n0.424\n1\ntrain\n\n\n0.849\n0.419\n1\neval\n\n\n0.865\n0.373\n2\ntrain\n\n\n0.851\n0.403\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the difference between weight decay and L2 regularization:\nweight -= lr*wd*weight\n…vs…\nweight.grad += wd*weight\n\n\nMomentum\n\nxs = torch.linspace(-4, 4, 100)\nys = 1 - (xs/3) ** 2 + torch.randn(100) * 0.1\n\n\n_,axs = plt.subplots(2,2, figsize=(12,8))\nbetas = [0.5,0.7,0.9,0.99]\nfor beta,ax in zip(betas, axs.flatten()):\n    ax.scatter(xs,ys)\n    avg,res = 0,[]\n    for yi in ys:\n        avg = beta*avg + (1-beta)*yi\n        res.append(avg)\n    ax.plot(xs,np.array(res), color='red');\n    ax.set_title(f'beta={beta}')\n\n\n\n\n\n\n\n\n\nsource\n\n\nMomentum\n\n Momentum (params, lr, wd=0.0, mom=0.9)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Momentum(SGD):\n    def __init__(self, params, lr, wd=0., mom=0.9):\n        super().__init__(params, lr=lr, wd=wd)\n        self.mom=mom\n\n    def opt_step(self, p):\n        if not hasattr(p, 'grad_avg'): p.grad_avg = torch.zeros_like(p.grad)\n        p.grad_avg = p.grad_avg*self.mom + p.grad*(1-self.mom)\n        p -= self.lr * p.grad_avg\n\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=1.5, cbs=cbs, opt_func=Momentum)\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.787\n0.597\n0\ntrain\n\n\n0.848\n0.417\n0\neval\n\n\n0.869\n0.360\n1\ntrain\n\n\n0.861\n0.368\n1\neval\n\n\n0.885\n0.315\n2\ntrain\n\n\n0.871\n0.360\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\n\n\n\n\nRMSProp\n\nsource\n\n\nRMSProp\n\n RMSProp (params, lr, wd=0.0, sqr_mom=0.99, eps=1e-05)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass RMSProp(SGD):\n    def __init__(self, params, lr, wd=0., sqr_mom=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.sqr_mom,self.eps = sqr_mom,eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = p.grad**2\n        p.sqr_avg = p.sqr_avg*self.sqr_mom + p.grad**2*(1-self.sqr_mom)\n        p -= self.lr * p.grad/(p.sqr_avg.sqrt() + self.eps)\n\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=RMSProp)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.766\n0.664\n0\ntrain\n\n\n0.821\n0.482\n0\neval\n\n\n0.848\n0.416\n1\ntrain\n\n\n0.846\n0.424\n1\neval\n\n\n0.866\n0.367\n2\ntrain\n\n\n0.851\n0.404\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\n\n\n\n\nAdam\n\nsource\n\n\nAdam\n\n Adam (params, lr, wd=0.0, beta1=0.9, beta2=0.99, eps=1e-05)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'avg'): p.avg = torch.zeros_like(p.grad.data)\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = torch.zeros_like(p.grad.data)\n        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n        unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\n        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\n        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\n\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=Adam)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.791\n0.583\n0\ntrain\n\n\n0.839\n0.428\n0\neval\n\n\n0.870\n0.359\n1\ntrain\n\n\n0.859\n0.379\n1\neval\n\n\n0.886\n0.312\n2\ntrain\n\n\n0.874\n0.345\n2\neval",
    "crumbs": [
      "Blog",
      "Accelerated SGD"
    ]
  },
  {
    "objectID": "accel_sgd.html#schedulers",
    "href": "accel_sgd.html#schedulers",
    "title": "Accelerated SGD",
    "section": "Schedulers",
    "text": "Schedulers\nWe’ve already seen how we can easily write a custom LR-adjusting callback or Learner, or can use the predefined PyTorch schedulers. We’ll use the predefined ones for now since there’s nothing new to learn in implementing them ourselves.\n\ndir(lr_scheduler)\n\n['ChainedScheduler',\n 'ConstantLR',\n 'CosineAnnealingLR',\n 'CosineAnnealingWarmRestarts',\n 'Counter',\n 'CyclicLR',\n 'EPOCH_DEPRECATION_WARNING',\n 'ExponentialLR',\n 'LRScheduler',\n 'LambdaLR',\n 'LinearLR',\n 'MultiStepLR',\n 'MultiplicativeLR',\n 'OneCycleLR',\n 'Optimizer',\n 'PolynomialLR',\n 'ReduceLROnPlateau',\n 'SequentialLR',\n 'StepLR',\n '_LRScheduler',\n '__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_enable_get_lr_call',\n 'bisect_right',\n 'inf',\n 'math',\n 'types',\n 'warnings',\n 'weakref',\n 'wraps']\n\n\n\n' '.join(o for o in dir(lr_scheduler) if o[0].isupper() and o[1].islower())\n\n'ChainedScheduler ConstantLR CosineAnnealingLR CosineAnnealingWarmRestarts Counter CyclicLR ExponentialLR LambdaLR LinearLR MultiStepLR MultiplicativeLR OneCycleLR Optimizer PolynomialLR ReduceLROnPlateau SequentialLR StepLR'\n\n\n\n' '.join(filter(lambda x: x[0].isupper() and x[1].islower(), dir(lr_scheduler)))\n\n'ChainedScheduler ConstantLR CosineAnnealingLR CosineAnnealingWarmRestarts Counter CyclicLR ExponentialLR LambdaLR LinearLR MultiStepLR MultiplicativeLR OneCycleLR Optimizer PolynomialLR ReduceLROnPlateau SequentialLR StepLR'\n\n\n\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, lr=6e-3, cbs=[DeviceCB(), SingleBatchCB()])\nlearn.fit(1)\n\n\nopt = learn.opt\n' '.join(o for o in dir(opt) if o[0]!='_')\n\n'add_param_group defaults load_state_dict param_groups profile_hook_step register_step_post_hook register_step_pre_hook state state_dict step zero_grad'\n\n\n\nopt\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.006\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\n\nparam = next(iter(learn.model.parameters()))\nst = opt.state[param]\n\n\nst\n\n{'momentum_buffer': None}\n\n\n\nlen(opt.param_groups)\n\n1\n\n\n\npg = opt.param_groups[0]\n\n\nlist(pg)\n\n['params',\n 'lr',\n 'momentum',\n 'dampening',\n 'weight_decay',\n 'nesterov',\n 'maximize',\n 'foreach',\n 'differentiable']\n\n\n\nsched = lr_scheduler.CosineAnnealingLR(opt, 100)\n\n\nsched.base_lrs\n\n[0.006]\n\n\n\nsched.get_last_lr()\n\n[0.006]\n\n\n\nsource\n\nsched_lrs\n\n sched_lrs (sched, steps)\n\n\n\nExported source\ndef sched_lrs(sched, steps):\n    lrs = [sched.get_last_lr()]\n    for i in range(steps):\n        sched.optimizer.step()\n        sched.step()\n        lrs.append(sched.get_last_lr())\n    plt.plot(lrs)\n\n\n\nsched_lrs(sched, 110)\n\n\n\n\n\n\n\n\n\n\nScheduler callbacks\n\nsource\n\n\nBaseSchedCB\n\n BaseSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nBatchSchedCB\n\n BatchSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nHasLearnCB\n\n HasLearnCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nRecorderCB\n\n RecorderCB (**d)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\ndef _lr(cb): return cb.pg['lr']\n\n\n\nlen(dls.train)\n\n59\n\n\n\ntmax = 3 * len(dls.train)\nsched = partial(lr_scheduler.CosineAnnealingLR, T_max=tmax)\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nrec = RecorderCB(lr=_lr)\nxtra = [BatchSchedCB(sched),rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=2e-2, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.806\n0.525\n0\ntrain\n\n\n0.848\n0.415\n0\neval\n\n\n0.878\n0.331\n1\ntrain\n\n\n0.879\n0.330\n1\neval\n\n\n0.897\n0.282\n2\ntrain\n\n\n0.881\n0.318\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nrec.plot()\n\n\n\n\n\n\n\n\n\nsource\n\n\nEpochSchedCB\n\n EpochSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass EpochSchedCB(BaseSchedCB):\n    def after_epoch(self, learn): self._step(learn)\n\n\n\nsched = partial(lr_scheduler.CosineAnnealingLR, T_max=3)\nset_seed(42)\nxtra = [EpochSchedCB(sched),rec]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=2e-2, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.806\n0.524\n0\ntrain\n\n\n0.857\n0.382\n0\neval\n\n\n0.879\n0.328\n1\ntrain\n\n\n0.874\n0.336\n1\neval\n\n\n0.898\n0.275\n2\ntrain\n\n\n0.883\n0.309\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nrec.plot()\n\n\n\n\n\n\n\n\n\n\n1cycle training\nPaper by Leslie Smith.\n\ndef _beta1(cb): return cb.pg['betas'][0]\nrec = RecorderCB(lr=_lr, mom=_beta1)\n\n\nset_seed(42)\nlr,epochs = 6e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.767\n0.660\n0\ntrain\n\n\n0.842\n0.462\n0\neval\n\n\n0.855\n0.391\n1\ntrain\n\n\n0.859\n0.396\n1\neval\n\n\n0.884\n0.312\n2\ntrain\n\n\n0.881\n0.327\n2\neval\n\n\n0.906\n0.254\n3\ntrain\n\n\n0.895\n0.284\n3\neval\n\n\n0.920\n0.215\n4\ntrain\n\n\n0.900\n0.271\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nrec.plot()",
    "crumbs": [
      "Blog",
      "Accelerated SGD"
    ]
  },
  {
    "objectID": "augment.html",
    "href": "augment.html",
    "title": "Augmentation",
    "section": "",
    "text": "Exported source\nimport torch,random\nimport fastcore.all as fc\n\nfrom torch import nn\nfrom torch.nn import init\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nExported source\nimport pickle,gzip,math,os,time,shutil\nimport matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom fastcore.test import test_close\nfrom torch import distributions\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nbs = 1024\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n\ndsd = load_dataset(name)\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nset_seed(42)\nlr,epochs = 6e-2,5",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#going-wider",
    "href": "augment.html#going-wider",
    "title": "Augmentation",
    "section": "Going wider",
    "text": "Going wider\n\nsource\n\nget_model\n\n get_model (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(16, 32,\n            64, 128, 256, 512), norm=&lt;class\n            'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n\n\nExported source\ndef get_model(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n    return nn.Sequential(*layers)\n\n\n\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.824\n0.704\n0\ntrain\n\n\n0.859\n0.563\n0\neval\n\n\n0.898\n0.381\n1\ntrain\n\n\n0.872\n0.422\n1\neval\n\n\n0.922\n0.265\n2\ntrain\n\n\n0.907\n0.294\n2\neval\n\n\n0.941\n0.196\n3\ntrain\n\n\n0.928\n0.240\n3\neval\n\n\n0.963\n0.139\n4\ntrain\n\n\n0.933\n0.222\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPooling\n\nsource\n\n\nGlobalAvgPool\n\n GlobalAvgPool (*args, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass GlobalAvgPool(nn.Module):\n    def forward(self, x): return x.mean((-2,-1))\n\n\n\nsource\n\n\nget_model2\n\n get_model2 (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(16, 32,\n             64, 128, 256), norm=&lt;class\n             'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n\n\nExported source\ndef get_model2(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [ResBlock(256, 512, act=act, norm=norm), GlobalAvgPool()]\n    layers += [nn.Linear(512, 10, bias=False), nn.BatchNorm1d(10)]\n    return nn.Sequential(*layers)\n\n\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\nTrainLearner(get_model2(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()\n\nTot params: 4907588; MFLOPS: 33.0\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nResBlock\n(1024, 1, 28, 28)\n(1024, 16, 28, 28)\n6928\n5.3\n\n\nResBlock\n(1024, 16, 28, 28)\n(1024, 32, 14, 14)\n14560\n2.8\n\n\nResBlock\n(1024, 32, 14, 14)\n(1024, 64, 7, 7)\n57792\n2.8\n\n\nResBlock\n(1024, 64, 7, 7)\n(1024, 128, 4, 4)\n230272\n3.7\n\n\nResBlock\n(1024, 128, 4, 4)\n(1024, 256, 2, 2)\n919296\n3.7\n\n\nResBlock\n(1024, 256, 2, 2)\n(1024, 512, 2, 2)\n3673600\n14.7\n\n\nGlobalAvgPool\n(1024, 512, 2, 2)\n(1024, 512)\n0\n0.0\n\n\nLinear\n(1024, 512)\n(1024, 10)\n5120\n0.0\n\n\nBatchNorm1d\n(1024, 10)\n(1024, 10)\n20\n0.0\n\n\n\n\n\n\nset_seed(42)\nmodel = get_model2(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.822\n0.715\n0\ntrain\n\n\n0.857\n0.518\n0\neval\n\n\n0.898\n0.384\n1\ntrain\n\n\n0.881\n0.389\n1\neval\n\n\n0.921\n0.267\n2\ntrain\n\n\n0.906\n0.286\n2\neval\n\n\n0.941\n0.199\n3\ntrain\n\n\n0.925\n0.244\n3\neval\n\n\n0.962\n0.141\n4\ntrain\n\n\n0.929\n0.227\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_model3\n\n get_model3 (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(16, 32,\n             64, 128, 256), norm=&lt;class\n             'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n\n\nExported source\ndef get_model3(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n    return nn.Sequential(*layers)\n\n\n\nTrainLearner(get_model3(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()\n\nTot params: 1231428; MFLOPS: 18.3\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nResBlock\n(1024, 1, 28, 28)\n(1024, 16, 28, 28)\n6928\n5.3\n\n\nResBlock\n(1024, 16, 28, 28)\n(1024, 32, 14, 14)\n14560\n2.8\n\n\nResBlock\n(1024, 32, 14, 14)\n(1024, 64, 7, 7)\n57792\n2.8\n\n\nResBlock\n(1024, 64, 7, 7)\n(1024, 128, 4, 4)\n230272\n3.7\n\n\nResBlock\n(1024, 128, 4, 4)\n(1024, 256, 2, 2)\n919296\n3.7\n\n\nGlobalAvgPool\n(1024, 256, 2, 2)\n(1024, 256)\n0\n0.0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2560\n0.0\n\n\nBatchNorm1d\n(1024, 10)\n(1024, 10)\n20\n0.0\n\n\n\n\n\n\n[o.shape for o in get_model3()[0].parameters()]\n\n\nset_seed(42)\nmodel = get_model3(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.810\n0.758\n0\ntrain\n\n\n0.871\n0.450\n0\neval\n\n\n0.895\n0.401\n1\ntrain\n\n\n0.897\n0.339\n1\neval\n\n\n0.919\n0.276\n2\ntrain\n\n\n0.895\n0.319\n2\neval\n\n\n0.939\n0.207\n3\ntrain\n\n\n0.927\n0.246\n3\neval\n\n\n0.960\n0.152\n4\ntrain\n\n\n0.929\n0.230\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_model4\n\n get_model4 (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(16, 32,\n             64, 128, 256), norm=&lt;class\n             'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n\n\nExported source\ndef get_model4(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n    layers = [conv(1, 16, ks=5, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n    return nn.Sequential(*layers)\n\n\n\n[o.shape for o in get_model4()[0].parameters()]\n\n[torch.Size([16, 1, 5, 5]),\n torch.Size([16]),\n torch.Size([16]),\n torch.Size([16])]\n\n\n\nTrainLearner(get_model4(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()\n\nTot params: 1224948; MFLOPS: 13.3\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nSequential\n(1024, 1, 28, 28)\n(1024, 16, 28, 28)\n448\n0.3\n\n\nResBlock\n(1024, 16, 28, 28)\n(1024, 32, 14, 14)\n14560\n2.8\n\n\nResBlock\n(1024, 32, 14, 14)\n(1024, 64, 7, 7)\n57792\n2.8\n\n\nResBlock\n(1024, 64, 7, 7)\n(1024, 128, 4, 4)\n230272\n3.7\n\n\nResBlock\n(1024, 128, 4, 4)\n(1024, 256, 2, 2)\n919296\n3.7\n\n\nGlobalAvgPool\n(1024, 256, 2, 2)\n(1024, 256)\n0\n0.0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2560\n0.0\n\n\nBatchNorm1d\n(1024, 10)\n(1024, 10)\n20\n0.0\n\n\n\n\n\n\nset_seed(42)\nmodel = get_model4(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.804\n0.782\n0\ntrain\n\n\n0.869\n0.474\n0\neval\n\n\n0.898\n0.393\n1\ntrain\n\n\n0.892\n0.366\n1\neval\n\n\n0.918\n0.277\n2\ntrain\n\n\n0.896\n0.340\n2\neval\n\n\n0.940\n0.202\n3\ntrain\n\n\n0.923\n0.244\n3\neval\n\n\n0.961\n0.148\n4\ntrain\n\n\n0.925\n0.238\n4\neval",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#data-augmentation",
    "href": "augment.html#data-augmentation",
    "title": "Augmentation",
    "section": "Data augmentation",
    "text": "Data augmentation\nAfter 20 epochs without augmentation:\n{'accuracy': '0.999', 'loss': '0.012', 'epoch': 19, 'train': True}\n{'accuracy': '0.924', 'loss': '0.284', 'epoch': 19, 'train': False}\nWith batchnorm, weight decay doesn’t really regularize.\n\n\nExported source\nfrom torchvision import transforms\n\n\n\ndef tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n\ntfms = nn.Sequential(transforms.RandomCrop(28, padding=4),\n                     transforms.RandomHorizontalFlip())\n\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\nmodel = get_model()\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[SingleBatchCB(), augcb])\n\n\nlearn.fit(1)\n\n\nxb,yb = learn.batch\nshow_images(xb[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\nsource\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\n\nExported source\n@fc.patch\n@fc.delegates(show_images)\ndef show_image_batch(self:Learner, max_n=9, cbs=None, **kwargs):\n    self.fit(1, cbs=[SingleBatchCB()]+fc.L(cbs))\n    show_images(self.batch[0][:max_n], **kwargs)\n\n\n\nlearn.show_image_batch(max_n=16, imsize=(1.5))\n\n\n\n\n\n\n\n\n\ntfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n                     transforms.RandomHorizontalFlip())\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n\n\nset_seed(42)\nepochs = 20\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.764\n0.879\n0\ntrain\n\n\n0.824\n0.604\n0\neval\n\n\n0.858\n0.597\n1\ntrain\n\n\n0.869\n0.495\n1\neval\n\n\n0.877\n0.477\n2\ntrain\n\n\n0.823\n0.520\n2\neval\n\n\n0.887\n0.392\n3\ntrain\n\n\n0.874\n0.378\n3\neval\n\n\n0.894\n0.336\n4\ntrain\n\n\n0.877\n0.394\n4\neval\n\n\n0.906\n0.288\n5\ntrain\n\n\n0.904\n0.281\n5\neval\n\n\n0.914\n0.258\n6\ntrain\n\n\n0.858\n0.400\n6\neval\n\n\n0.919\n0.236\n7\ntrain\n\n\n0.914\n0.252\n7\neval\n\n\n0.923\n0.223\n8\ntrain\n\n\n0.919\n0.234\n8\neval\n\n\n0.930\n0.198\n9\ntrain\n\n\n0.922\n0.222\n9\neval\n\n\n0.934\n0.189\n10\ntrain\n\n\n0.922\n0.222\n10\neval\n\n\n0.940\n0.173\n11\ntrain\n\n\n0.930\n0.205\n11\neval\n\n\n0.943\n0.164\n12\ntrain\n\n\n0.927\n0.207\n12\neval\n\n\n0.949\n0.148\n13\ntrain\n\n\n0.932\n0.193\n13\neval\n\n\n0.952\n0.139\n14\ntrain\n\n\n0.937\n0.185\n14\neval\n\n\n0.959\n0.121\n15\ntrain\n\n\n0.939\n0.180\n15\neval\n\n\n0.962\n0.111\n16\ntrain\n\n\n0.939\n0.181\n16\neval\n\n\n0.966\n0.102\n17\ntrain\n\n\n0.941\n0.180\n17\neval\n\n\n0.970\n0.093\n18\ntrain\n\n\n0.943\n0.175\n18\neval\n\n\n0.971\n0.090\n19\ntrain\n\n\n0.944\n0.174\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\nA custom collation function could let you do per-item transformations.\n\nmdl_path = Path('models')\nmdl_path.mkdir(exist_ok=True)\ntorch.save(learn.model, mdl_path/'data_aug.pkl')",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#test-time-augmentation-tta",
    "href": "augment.html#test-time-augmentation-tta",
    "title": "Augmentation",
    "section": "Test time augmentation (TTA)",
    "text": "Test time augmentation (TTA)\n\nsource\n\nCapturePreds\n\n CapturePreds ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass CapturePreds(Callback):\n    def before_fit(self, learn): self.all_inps,self.all_preds,self.all_targs = [],[],[]\n    def after_batch(self, learn):\n        self.all_inps. append(to_cpu(learn.batch[0]))\n        self.all_preds.append(to_cpu(learn.preds))\n        self.all_targs.append(to_cpu(learn.batch[1]))\n    def after_fit(self, learn):\n        self.all_preds,self.all_targs,self.all_inps = map(torch.cat, [self.all_preds,self.all_targs,self.all_inps])\n\n\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\n\nExported source\n@fc.patch\ndef capture_preds(self: Learner, cbs=None, inps=False):\n    cp = CapturePreds()\n    self.fit(1, train=False, cbs=[cp]+fc.L(cbs))\n    res = cp.all_preds,cp.all_targs\n    if inps: res = res+(cp.all_inps,)\n    return res\n\n\n\nap1, at = learn.capture_preds()\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.944\n0.174\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nttacb = BatchTransformCB(partial(tfm_batch, tfm_x=TF.hflip), on_val=True)\nap2, at = learn.capture_preds(cbs=[ttacb])\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.942\n0.175\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nap1.shape,ap2.shape,at.shape\n\n(torch.Size([10000, 10]), torch.Size([10000, 10]), torch.Size([10000]))\n\n\n\nap = torch.stack([ap1,ap2]).mean(0).argmax(1)\n\n\nround((ap==at).float().mean().item(), 3)\n\n0.946",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#random-erase",
    "href": "augment.html#random-erase",
    "title": "Augmentation",
    "section": "Random erase",
    "text": "Random erase\n\nxb,_ = next(iter(dls.train))\nxbt = xb[:16]\n\n\nxm,xs = xbt.mean(),xbt.std()\n\n\nxbt.min(), xbt.max()\n\n(tensor(-0.80), tensor(2.06))\n\n\n\npct = 0.2\n\n\nszx = int(pct*xbt.shape[-2])\nszy = int(pct*xbt.shape[-1])\nstx = int(random.random()*(1-pct)*xbt.shape[-2])\nsty = int(random.random()*(1-pct)*xbt.shape[-1])\nstx,sty,szx,szy\n\n(14, 0, 5, 5)\n\n\n\ninit.normal_(xbt[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs);\n\n\nshow_images(xbt, imsize=1.5)\n\n\n\n\n\n\n\n\n\nxbt.min(), xbt.max()\n\n(tensor(-3.36), tensor(2.56))\n\n\n\n\nExported source\ndef _rand_erase1(x, pct, xm, xs, mn, mx):\n    szx = int(pct*x.shape[-2])\n    szy = int(pct*x.shape[-1])\n    stx = int(random.random()*(1-pct)*x.shape[-2])\n    sty = int(random.random()*(1-pct)*x.shape[-1])\n    init.normal_(x[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs)\n    x.clamp_(mn, mx)\n\n\n\nxb,_ = next(iter(dls.train))\nxbt = xb[:16]\n_rand_erase1(xbt, 0.2, xbt.mean(), xbt.std(), xbt.min(), xbt.max())\nshow_images(xbt, imsize=1.5)\n\n\n\n\n\n\n\n\n\nxbt.mean(),xbt.std(),xbt.min(), xbt.max()\n\n(tensor(0.09), tensor(1.04), tensor(-0.80), tensor(2.06))\n\n\n\nsource\n\nrand_erase\n\n rand_erase (x, pct=0.2, max_num=4)\n\n\n\nExported source\ndef rand_erase(x, pct=0.2, max_num = 4):\n    xm,xs,mn,mx = x.mean(),x.std(),x.min(),x.max()\n    num = random.randint(0, max_num)\n    for i in range(num): _rand_erase1(x, pct, xm, xs, mn, mx)\n#     print(num)\n    return x\n\n\n\nxb,_ = next(iter(dls.train))\nxbt = xb[:16]\nrand_erase(xbt, 0.2, 4)\nshow_images(xbt, imsize=1.5)\n\n\nsource\n\n\nRandErase\n\n RandErase (pct=0.2, max_num=4)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass RandErase(nn.Module):\n    def __init__(self, pct=0.2, max_num=4):\n        super().__init__()\n        self.pct,self.max_num = pct,max_num\n    def forward(self, x): return rand_erase(x, self.pct, self.max_num)\n\n\n\ntfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n                     transforms.RandomHorizontalFlip(),\n                     RandErase())\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n\n\nmodel = get_model()\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\nlearn.fit(1)\nxb,yb = learn.batch\nshow_images(xb[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\nepochs = 20\nlr = 2e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.760\n0.871\n0\ntrain\n\n\n0.813\n0.607\n0\neval\n\n\n0.842\n0.596\n1\ntrain\n\n\n0.845\n0.472\n1\neval\n\n\n0.856\n0.480\n2\ntrain\n\n\n0.856\n0.427\n2\neval\n\n\n0.866\n0.405\n3\ntrain\n\n\n0.856\n0.421\n3\neval\n\n\n0.872\n0.374\n4\ntrain\n\n\n0.822\n0.491\n4\neval\n\n\n0.885\n0.323\n5\ntrain\n\n\n0.880\n0.363\n5\neval\n\n\n0.895\n0.295\n6\ntrain\n\n\n0.852\n0.456\n6\neval\n\n\n0.899\n0.278\n7\ntrain\n\n\n0.869\n0.368\n7\neval\n\n\n0.907\n0.257\n8\ntrain\n\n\n0.901\n0.301\n8\neval\n\n\n0.912\n0.244\n9\ntrain\n\n\n0.910\n0.260\n9\neval\n\n\n0.917\n0.231\n10\ntrain\n\n\n0.916\n0.229\n10\neval\n\n\n0.922\n0.215\n11\ntrain\n\n\n0.921\n0.220\n11\neval\n\n\n0.926\n0.206\n12\ntrain\n\n\n0.928\n0.201\n12\neval\n\n\n0.930\n0.191\n13\ntrain\n\n\n0.924\n0.208\n13\neval\n\n\n0.933\n0.185\n14\ntrain\n\n\n0.921\n0.219\n14\neval\n\n\n0.938\n0.172\n15\ntrain\n\n\n0.929\n0.198\n15\neval\n\n\n0.941\n0.163\n16\ntrain\n\n\n0.936\n0.178\n16\neval\n\n\n0.944\n0.153\n17\ntrain\n\n\n0.939\n0.172\n17\neval\n\n\n0.947\n0.146\n18\ntrain\n\n\n0.940\n0.169\n18\neval\n\n\n0.949\n0.142\n19\ntrain\n\n\n0.939\n0.172\n19\neval",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#random-copy",
    "href": "augment.html#random-copy",
    "title": "Augmentation",
    "section": "Random copy",
    "text": "Random copy\n\nxb,_ = next(iter(dls.train))\nxbt = xb[:16]\n\n\nszx = int(pct*xbt.shape[-2])\nszy = int(pct*xbt.shape[-1])\nstx1 = int(random.random()*(1-pct)*xbt.shape[-2])\nsty1 = int(random.random()*(1-pct)*xbt.shape[-1])\nstx2 = int(random.random()*(1-pct)*xbt.shape[-2])\nsty2 = int(random.random()*(1-pct)*xbt.shape[-1])\nstx1,sty1,stx2,sty2,szx,szy\n\n(9, 18, 3, 7, 5, 5)\n\n\n\nxbt[:,:,stx1:stx1+szx,sty1:sty1+szy] = xbt[:,:,stx2:stx2+szx,sty2:sty2+szy]\n\n\nshow_images(xbt, imsize=1.5)\n\n\n\n\n\n\n\n\n\n\nExported source\ndef _rand_copy1(x, pct):\n    szx = int(pct*x.shape[-2])\n    szy = int(pct*x.shape[-1])\n    stx1 = int(random.random()*(1-pct)*x.shape[-2])\n    sty1 = int(random.random()*(1-pct)*x.shape[-1])\n    stx2 = int(random.random()*(1-pct)*x.shape[-2])\n    sty2 = int(random.random()*(1-pct)*x.shape[-1])\n    x[:,:,stx1:stx1+szx,sty1:sty1+szy] = x[:,:,stx2:stx2+szx,sty2:sty2+szy]\n\n\n\nxb,_ = next(iter(dls.train))\nxbt = xb[:16]\n_rand_copy1(xbt, 0.2)\nshow_images(xbt, imsize=1.5)\n\n\n\n\n\n\n\n\n\nsource\n\nrand_copy\n\n rand_copy (x, pct=0.2, max_num=4)\n\n\n\nExported source\ndef rand_copy(x, pct=0.2, max_num = 4):\n    num = random.randint(0, max_num)\n    for i in range(num): _rand_copy1(x, pct)\n#     print(num)\n    return x\n\n\n\nxb,_ = next(iter(dls.train))\nxbt = xb[:16]\nrand_copy(xbt, 0.2, 4)\nshow_images(xbt, imsize=1.5)\n\n\n\n\n\n\n\n\n\nsource\n\n\nRandCopy\n\n RandCopy (pct=0.2, max_num=4)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass RandCopy(nn.Module):\n    def __init__(self, pct=0.2, max_num=4):\n        super().__init__()\n        self.pct,self.max_num = pct,max_num\n    def forward(self, x): return rand_copy(x, self.pct, self.max_num)\n\n\n\ntfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n                     transforms.RandomHorizontalFlip(),\n                     RandCopy())\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n\n\nmodel = get_model()\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\nlearn.fit(1)\nxb,yb = learn.batch\nshow_images(xb[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\nset_seed(1)\nepochs = 20\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.739\n0.940\n0\ntrain\n\n\n0.810\n0.599\n0\neval\n\n\n0.832\n0.656\n1\ntrain\n\n\n0.842\n0.534\n1\neval\n\n\n0.849\n0.558\n2\ntrain\n\n\n0.838\n0.487\n2\neval\n\n\n0.858\n0.467\n3\ntrain\n\n\n0.827\n0.528\n3\neval\n\n\n0.873\n0.394\n4\ntrain\n\n\n0.860\n0.418\n4\neval\n\n\n0.885\n0.344\n5\ntrain\n\n\n0.868\n0.391\n5\neval\n\n\n0.891\n0.321\n6\ntrain\n\n\n0.885\n0.334\n6\neval\n\n\n0.899\n0.293\n7\ntrain\n\n\n0.906\n0.261\n7\neval\n\n\n0.910\n0.258\n8\ntrain\n\n\n0.913\n0.242\n8\neval\n\n\n0.913\n0.249\n9\ntrain\n\n\n0.897\n0.294\n9\neval\n\n\n0.914\n0.242\n10\ntrain\n\n\n0.921\n0.229\n10\neval\n\n\n0.922\n0.221\n11\ntrain\n\n\n0.923\n0.215\n11\neval\n\n\n0.925\n0.212\n12\ntrain\n\n\n0.927\n0.206\n12\neval\n\n\n0.929\n0.200\n13\ntrain\n\n\n0.925\n0.209\n13\neval\n\n\n0.934\n0.189\n14\ntrain\n\n\n0.918\n0.226\n14\neval\n\n\n0.937\n0.177\n15\ntrain\n\n\n0.933\n0.187\n15\neval\n\n\n0.942\n0.167\n16\ntrain\n\n\n0.937\n0.178\n16\neval\n\n\n0.944\n0.159\n17\ntrain\n\n\n0.939\n0.171\n17\neval\n\n\n0.946\n0.152\n18\ntrain\n\n\n0.939\n0.170\n18\neval\n\n\n0.951\n0.142\n19\ntrain\n\n\n0.940\n0.171\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel2 = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn2 = TrainLearner(model2, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn2.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.745\n0.935\n0\ntrain\n\n\n0.823\n0.573\n0\neval\n\n\n0.838\n0.648\n1\ntrain\n\n\n0.863\n0.455\n1\neval\n\n\n0.853\n0.542\n2\ntrain\n\n\n0.812\n0.598\n2\neval\n\n\n0.860\n0.471\n3\ntrain\n\n\n0.864\n0.408\n3\neval\n\n\n0.871\n0.398\n4\ntrain\n\n\n0.858\n0.438\n4\neval\n\n\n0.884\n0.348\n5\ntrain\n\n\n0.898\n0.295\n5\neval\n\n\n0.896\n0.308\n6\ntrain\n\n\n0.883\n0.345\n6\neval\n\n\n0.901\n0.284\n7\ntrain\n\n\n0.891\n0.298\n7\neval\n\n\n0.899\n0.290\n8\ntrain\n\n\n0.903\n0.284\n8\neval\n\n\n0.916\n0.243\n9\ntrain\n\n\n0.905\n0.271\n9\neval\n\n\n0.914\n0.245\n10\ntrain\n\n\n0.916\n0.243\n10\neval\n\n\n0.919\n0.227\n11\ntrain\n\n\n0.922\n0.227\n11\neval\n\n\n0.925\n0.211\n12\ntrain\n\n\n0.923\n0.220\n12\neval\n\n\n0.930\n0.197\n13\ntrain\n\n\n0.932\n0.198\n13\neval\n\n\n0.934\n0.186\n14\ntrain\n\n\n0.930\n0.201\n14\neval\n\n\n0.938\n0.173\n15\ntrain\n\n\n0.934\n0.194\n15\neval\n\n\n0.943\n0.163\n16\ntrain\n\n\n0.929\n0.205\n16\neval\n\n\n0.943\n0.160\n17\ntrain\n\n\n0.938\n0.183\n17\neval\n\n\n0.946\n0.152\n18\ntrain\n\n\n0.938\n0.183\n18\neval\n\n\n0.947\n0.150\n19\ntrain\n\n\n0.937\n0.185\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmdl_path = Path('models')\ntorch.save(learn.model,  mdl_path/'randcopy1.pkl')\ntorch.save(learn2.model, mdl_path/'randcopy2.pkl')\n\n\ncp1 = CapturePreds()\nlearn.fit(1, train=False, cbs=cp1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.940\n0.171\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ncp2 = CapturePreds()\nlearn2.fit(1, train=False, cbs=cp2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.937\n0.185\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nap = torch.stack([cp1.all_preds,cp2.all_preds]).mean(0).argmax(1)\n\n\nround((ap==cp1.all_targs).float().mean().item(), 3)\n\n0.942",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#dropout",
    "href": "augment.html#dropout",
    "title": "Augmentation",
    "section": "Dropout",
    "text": "Dropout\n\np = 0.1\ndist = distributions.binomial.Binomial(probs=1-p)\ndist.sample((10,))\n\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nsource\n\nDropout\n\n Dropout (p=0.1)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass Dropout(nn.Module):\n    def __init__(self, p=0.1):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        if not self.training: return x\n        dist = distributions.binomial.Binomial(tensor(1.0).to(x.device), probs=1-self.p)\n        return x * dist.sample(x.size()) * 1/(1-self.p)\n\n\n\nsource\n\n\nget_dropmodel\n\n get_dropmodel (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=(16,\n                32, 64, 128, 256, 512), norm=&lt;class\n                'torch.nn.modules.batchnorm.BatchNorm2d'&gt;, drop=0.0)\n\n\n\nExported source\ndef get_dropmodel(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d, drop=0.0):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm), nn.Dropout2d(drop)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [nn.Flatten(), Dropout(drop), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n    return nn.Sequential(*layers)\n\n\n\nset_seed(42)\nepochs=5\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nmodel = get_dropmodel(act_gr, norm=nn.BatchNorm2d, drop=0.1).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.809\n0.745\n0\ntrain\n\n\n0.854\n0.482\n0\neval\n\n\n0.894\n0.392\n1\ntrain\n\n\n0.892\n0.340\n1\neval\n\n\n0.917\n0.277\n2\ntrain\n\n\n0.910\n0.278\n2\neval\n\n\n0.937\n0.208\n3\ntrain\n\n\n0.927\n0.234\n3\neval\n\n\n0.956\n0.155\n4\ntrain\n\n\n0.930\n0.225\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nTTD_CB\n\n TTD_CB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass TTD_CB(Callback):\n    def before_epoch(self, learn):\n        learn.model.apply(lambda m: m.train() if isinstance(m, (nn.Dropout,nn.Dropout2d)) else None)",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "augment.html#augment-2",
    "href": "augment.html#augment-2",
    "title": "Augmentation",
    "section": "Augment 2",
    "text": "Augment 2\n\n\ninplace.._f\n\n inplace.&lt;locals&gt;._f (b)\n\n\n\nExported source\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)*2-1) for o in b[xl]]\n\n\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\n\nset_seed(42)\nepochs = 20\nlr = 1e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.742\n0.932\n0\ntrain\n\n\n0.798\n0.623\n0\neval\n\n\n0.832\n0.663\n1\ntrain\n\n\n0.785\n0.712\n1\neval\n\n\n0.855\n0.535\n2\ntrain\n\n\n0.848\n0.496\n2\neval\n\n\n0.854\n0.486\n3\ntrain\n\n\n0.790\n0.620\n3\neval\n\n\n0.867\n0.412\n4\ntrain\n\n\n0.865\n0.390\n4\neval\n\n\n0.885\n0.344\n5\ntrain\n\n\n0.880\n0.353\n5\neval\n\n\n0.887\n0.335\n6\ntrain\n\n\n0.873\n0.377\n6\neval\n\n\n0.899\n0.292\n7\ntrain\n\n\n0.908\n0.260\n7\neval\n\n\n0.904\n0.274\n8\ntrain\n\n\n0.897\n0.286\n8\neval\n\n\n0.905\n0.266\n9\ntrain\n\n\n0.909\n0.258\n9\neval\n\n\n0.915\n0.240\n10\ntrain\n\n\n0.917\n0.232\n10\neval\n\n\n0.920\n0.227\n11\ntrain\n\n\n0.913\n0.243\n11\neval\n\n\n0.924\n0.214\n12\ntrain\n\n\n0.922\n0.216\n12\neval\n\n\n0.929\n0.202\n13\ntrain\n\n\n0.930\n0.201\n13\neval\n\n\n0.934\n0.185\n14\ntrain\n\n\n0.933\n0.191\n14\neval\n\n\n0.934\n0.183\n15\ntrain\n\n\n0.936\n0.182\n15\neval\n\n\n0.941\n0.166\n16\ntrain\n\n\n0.938\n0.179\n16\neval\n\n\n0.943\n0.163\n17\ntrain\n\n\n0.940\n0.177\n17\neval\n\n\n0.945\n0.158\n18\ntrain\n\n\n0.938\n0.180\n18\neval\n\n\n0.947\n0.152\n19\ntrain\n\n\n0.940\n0.177\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/data_aug2.pkl')",
    "crumbs": [
      "Blog",
      "Augmentation"
    ]
  },
  {
    "objectID": "ddpm_v3.html",
    "href": "ddpm_v3.html",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "",
    "text": "import os\n\n\nimport pickle,gzip,math,os,time,shutil,torch,random,logging\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom functools import partial\n\nfrom fastcore.foundation import L\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\n\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\n\n\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nmpl.rcParams['image.cmap'] = 'gray_r'\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\nbs = 256\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\n\n\nfrom types import SimpleNamespace\n\n\ndef linear_sched(betamin=0.0001,betamax=0.02,n_steps=1000):\n    beta = torch.linspace(betamin, betamax, n_steps)\n    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())\n\n\ndef abar(t, T): return (t/T*math.pi/2).cos()**2\n\n\ndef cos_sched(n_steps=1000):\n    ts = torch.linspace(0, n_steps-1, n_steps)\n    ab = abar(ts,n_steps)\n    alp = ab/abar(ts-1,n_steps)\n    return SimpleNamespace(a=alp, abar=ab, sig=(1-alp).sqrt())\n\n\nlin_abar = linear_sched().abar\ncos_abar = cos_sched().abar\nplt.plot(lin_abar, label='lin')\nplt.plot(cos_abar, label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(lin_abar[1:]-lin_abar[:-1], label='lin')\nplt.plot(cos_abar[1:]-cos_abar[:-1], label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nlin_abar = linear_sched(betamax=0.01).abar\nplt.plot(lin_abar, label='lin')\nplt.plot(cos_abar, label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(lin_abar[1:]-lin_abar[:-1], label='lin')\nplt.plot(cos_abar[1:]-cos_abar[:-1], label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nn_steps = 1000\nlin_abar = linear_sched(betamax=0.01)\nalphabar = lin_abar.abar\nalpha = lin_abar.a\nsigma = lin_abar.sig\n\n\ndef noisify(x0, ᾱ):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    ε = torch.randn(x0.shape, device=device)\n    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n    return (xt, t.to(device)), ε\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\n\n(xt,t),ε = noisify(xb[:25],alphabar)\nt\n\ntensor([ 26, 335, 620, 924, 950, 113, 378,  14, 210, 954, 231, 572, 315, 295, 567, 706, 749, 876,  73, 111, 899, 213, 541, 769, 287])\n\n\n\ntitles = fc.map_ex(t[:25], '{}')\nshow_images(xt[:25], imsize=1.5, titles=titles)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v3.html#imports",
    "href": "ddpm_v3.html#imports",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "",
    "text": "import os\n\n\nimport pickle,gzip,math,os,time,shutil,torch,random,logging\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom functools import partial\n\nfrom fastcore.foundation import L\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\n\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\n\n\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nmpl.rcParams['image.cmap'] = 'gray_r'\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\nbs = 256\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\n\n\nfrom types import SimpleNamespace\n\n\ndef linear_sched(betamin=0.0001,betamax=0.02,n_steps=1000):\n    beta = torch.linspace(betamin, betamax, n_steps)\n    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())\n\n\ndef abar(t, T): return (t/T*math.pi/2).cos()**2\n\n\ndef cos_sched(n_steps=1000):\n    ts = torch.linspace(0, n_steps-1, n_steps)\n    ab = abar(ts,n_steps)\n    alp = ab/abar(ts-1,n_steps)\n    return SimpleNamespace(a=alp, abar=ab, sig=(1-alp).sqrt())\n\n\nlin_abar = linear_sched().abar\ncos_abar = cos_sched().abar\nplt.plot(lin_abar, label='lin')\nplt.plot(cos_abar, label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(lin_abar[1:]-lin_abar[:-1], label='lin')\nplt.plot(cos_abar[1:]-cos_abar[:-1], label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nlin_abar = linear_sched(betamax=0.01).abar\nplt.plot(lin_abar, label='lin')\nplt.plot(cos_abar, label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(lin_abar[1:]-lin_abar[:-1], label='lin')\nplt.plot(cos_abar[1:]-cos_abar[:-1], label='cos')\nplt.legend();\n\n\n\n\n\n\n\n\n\nn_steps = 1000\nlin_abar = linear_sched(betamax=0.01)\nalphabar = lin_abar.abar\nalpha = lin_abar.a\nsigma = lin_abar.sig\n\n\ndef noisify(x0, ᾱ):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    ε = torch.randn(x0.shape, device=device)\n    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n    return (xt, t.to(device)), ε\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\n\n(xt,t),ε = noisify(xb[:25],alphabar)\nt\n\ntensor([ 26, 335, 620, 924, 950, 113, 378,  14, 210, 954, 231, 572, 315, 295, 567, 706, 749, 876,  73, 111, 899, 213, 541, 769, 287])\n\n\n\ntitles = fc.map_ex(t[:25], '{}')\nshow_images(xt[:25], imsize=1.5, titles=titles)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm_v3.html#training",
    "href": "ddpm_v3.html#training",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Training",
    "text": "Training\n\nfrom diffusers import UNet2DModel\n\n\nclass UNet(UNet2DModel):\n    def forward(self, x): return super().forward(*x).sample\n\n\ndef init_ddpm(model):\n    for o in model.down_blocks:\n        for p in o.resnets:\n            p.conv2.weight.data.zero_()\n            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n\n    for o in model.up_blocks:\n        for p in o.resnets: p.conv2.weight.data.zero_()\n\n    model.conv_out.weight.data.zero_()\n\n\ndef collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\ndef dl_ddpm(ds, nw=4): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=nw)\n\n\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\nlr = 1e-2\nepochs = 4\nopt_func = partial(optim.AdamW, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(16, 32, 64, 128), norm_num_groups=8)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.158\n0\ntrain\n\n\n0.038\n0\neval\n\n\n0.032\n1\ntrain\n\n\n0.030\n1\neval\n\n\n0.028\n2\ntrain\n\n\n0.027\n2\neval\n\n\n0.026\n3\ntrain\n\n\n0.026\n3\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmdl_path = Path('models')\n\n\ntorch.save(learn.model, mdl_path/'fashion_ddpm3_25.pkl')\n\n\nmodel = torch.load(mdl_path/'fashion_ddpm3_25.pkl').cuda()\n\n\n@torch.no_grad()\ndef sample(model, sz):\n    ps = next(model.parameters())\n    x_t = torch.randn(sz).to(ps)\n    preds = []\n    for t in reversed(range(n_steps)):\n        t_batch = torch.full((x_t.shape[0],), t, device=ps.device, dtype=torch.long)\n        z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(ps)\n        ᾱ_t1 = alphabar[t-1]  if t &gt; 0 else torch.tensor(1)\n        b̄_t = 1-alphabar[t]\n        b̄_t1 = 1-ᾱ_t1\n        noise = model((x_t, t_batch))\n        x_0_hat = ((x_t - b̄_t.sqrt() * noise)/alphabar[t].sqrt())\n        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n        preds.append(x_t.float().cpu())\n    return preds\n\n\nn_samples = 128\n\n\nsamples = sample(model, (n_samples, 1, 32, 32))\n\nCPU times: user 52.9 s, sys: 4.74 s, total: 57.6 s\nWall time: 57.3 s\n\n\n\ns = (samples[-1]*2)#.clamp(-1,1)\ns.min(),s.max()\n\n(tensor(-1.20), tensor(1.56))\n\n\n\nshow_images(s[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\n@inplace\ndef transformi2(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\ntds2 = dsd.with_transform(transformi2)\ndls2 = DataLoaders.from_dd(tds2, bs, num_workers=fc.defaults.cpus)\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\n\nfrom fastAIcourse.fid import ImageEval\n\n\nie = ImageEval(cmodel, dls2, cbs=[DeviceCB()])\n\n\nie.fid(s)\n\n1753.90234375\n\n\n\ns.min(),s.max()\n\n(tensor(-7.90), tensor(7.56))\n\n\n\nie.fid(xb*2)\n\n13.9842529296875\n\n\n\nSkip sampling\n\n@torch.no_grad()\ndef sample_skip(model, sz):\n    ps = next(model.parameters())\n    x_t = torch.randn(sz).to(ps)\n    preds = []\n    for t in reversed(range(n_steps)):\n        t_batch = torch.full((x_t.shape[0],), t, device=ps.device, dtype=torch.long)\n        z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(ps)\n        ᾱ_t1 = alphabar[t-1]  if t &gt; 0 else torch.tensor(1)\n        b̄_t = 1-alphabar[t]\n        b̄_t1 = 1-ᾱ_t1\n        if t%3==0 or t&lt;50: noise = model((x_t, t_batch))\n        x_0_hat = ((x_t - b̄_t.sqrt() * noise)/alphabar[t].sqrt())\n        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n        preds.append(x_t.cpu().float())\n    return preds\n\n\nsamples = sample_skip(model, (n_samples, 1, 32, 32))\n\nCPU times: user 20.6 s, sys: 1.71 s, total: 22.3 s\nWall time: 22 s\n\n\n\ns = (samples[-1]*2)#.clamp(-1,1)\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nie.fid(s)\n\n60.546875\n\n\n\n@torch.no_grad()\ndef sample2(model, sz):\n    ps = next(model.parameters())\n    x_t = torch.randn(sz).to(ps)\n    sample_at = {t for t in range(n_steps) if (t+101)%((t+101)//100)==0}\n    preds = []\n    for t in reversed(range(n_steps)):\n        t_batch = torch.full((x_t.shape[0],), t, device=ps.device, dtype=torch.long)\n        z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(ps)\n        ᾱ_t1 = alphabar[t-1]  if t &gt; 0 else torch.tensor(1)\n        b̄_t = 1-alphabar[t]\n        b̄_t1 = 1-ᾱ_t1\n        if t in sample_at: noise = model((x_t, t_batch))\n        x_0_hat = ((x_t - b̄_t.sqrt() * noise)/alphabar[t].sqrt())\n        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n        if t in sample_at: preds.append(x_t.float().cpu())\n    return preds\n\n\nsamples = sample2(model, (n_samples, 1, 32, 32))\n\nCPU times: user 17.7 s, sys: 1.76 s, total: 19.4 s\nWall time: 17.9 s\n\n\n\ns = (samples[-1]*2)#.clamp(-1,1)\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nie.fid(s)\n\n64.6614990234375",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html",
    "href": "how-random-forests-really-work.html",
    "title": "How random forests really work",
    "section": "",
    "text": "Previously I’ve shown how to create a linear model and neural net from scratch, and used it to create a solid submission to Kaggle’s Titanic competition. However, for tabular data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it’s more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines.\nIn this notebook, we’re going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition! That might sound like a pretty big stretch, but I think you’ll be surprised to discover how straightforward it actually is.\nWe’ll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html#data-preprocessing",
    "href": "how-random-forests-really-work.html#data-preprocessing",
    "title": "How random forests really work",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nWe’ll create DataFrames from the CSV files just like we did in the “linear model and neural net from scratch” notebook, and do much the same preprocessing (so go back and check that out if you’re not already familiar with the dataset):\n\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\n\n\ndatapath = Path('./Data')\nname = 'titanic'\npath = Path(f'{datapath}/{name}')\n\nkaggle_competition_download(name, datapath)\n\nfile exists\n\n\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n\nOne difference with Random Forests however is that we don’t generally have to create dummy variables like we did for non-numeric columns in the linear models and neural network. Instead, we can just convert those fields to categorical variables, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)\n\nWe’ll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider Pclass a categorical variable. That’s because it’s ordered (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we’ll see, only care about order, not about absolute value.\n\ncats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"\n\nEven although we’ve made the cats columns categorical, they are still shown by Pandas as their original values:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nHowever behind the scenes they’re now stored as integers, with indices that are looked up in the Categories list shown in the output above. We can view the stored values by looking in the cat.codes attribute:\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html#binary-splits",
    "href": "how-random-forests-really-work.html#binary-splits",
    "title": "How random forests really work",
    "section": "Binary splits",
    "text": "Binary splits\nBefore we create a Random Forest or Gradient Boosting Machine, we’ll first need to learn how to create a decision tree, from which both of these models are built.\nAnd to create a decision tree, we’ll first need to create a binary split, since that’s what a decision tree is built from.\nA binary split is where all rows are placed into one of two groups, based on whether they’re above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold 0.5 and the column Sex (since the values in the column are 0 for female and 1 for male). We can use a plot to see how that would split up our data – we’ll use the Seaborn library, which is a layer on top of matplotlib that makes some useful charts easier to create, and more aesthetically pleasing by default:\n\nimport seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\n\n\n\nHere we see that (on the left) if we split the data into males and females, we’d have groups that have very different survival rates: &gt;70% for females, and &lt;20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of around 900) in each group.\nWe could create a very simple “model” which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n(In the previous step we also replaced the categorical variables with their integer codes, since some of the models we’ll be building in a moment require that.)\nNow we can create our independent variables (the x variables) and dependent (the y variable):\n\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\nHere’s the predictions for our extremely simple model, where female is coded as 0:\n\npreds = val_xs.Sex==0\n\nWe’ll use mean absolute error to measure how good this model is:\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work – here’s an example of how we could look at LogFare:\n\ndf_fare = trn_df[trn_df.LogFare&gt;0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\n\n\n\n\nThe boxenplot above shows quantiles of LogFare for each group of Survived==0 and Survived==1. It shows that the average LogFare for passengers that didn’t survive is around 2.5, and for those that did it’s around 3.2. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\nLet’s create a simple model based on this observation:\n\npreds = val_xs.LogFare&gt;2.7\n\n…and test it out:\n\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nThis is quite a bit less accurate than our model that used Sex as the single binary split.\nIdeally, we’d like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We’ll create a score function to do this. Instead of returning the mean absolute error, we’ll calculate a measure of impurity – that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it’s higher, then it means the rows are more different to each other. We’ll then multiply this by the number of rows, since a bigger group as more impact than a smaller group:\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot&lt;=1: return 0\n    return y[side].std()*tot\n\nNow we’ve got that written, we can calculate the score for a split by adding up the scores for the “left hand side” (lhs) and “right hand side” (rhs):\n\ndef score(col, y, split):\n    lhs = col&lt;=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nFor instance, here’s the impurity score for the split on Sex:\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\n…and for LogFare:\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.47180873952099694\n\n\nAs we’d expect from our earlier tests, Sex appears to be a better split.\nTo make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle if you click “Copy and Edit” in the top right to open the notebook editor):\n\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\nTry selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\nWe can do the same thing for the categorical variables:\n\ninteract(nm=cats, split=2)(iscore);\n\n\n\n\nThat works well enough, but it’s rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for age we’d first need to make a list of all the possible split points (i.e all the unique values of that field)…:\n\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\n…and find which index of those values is where score() is the lowest:\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nscores\n\narray([0.48447755, 0.48351588, 0.48158676, 0.48061929, 0.47964987, 0.480937  , 0.48347294, 0.48171397, 0.47987776, 0.47884826,\n       0.47831672, 0.47949847, 0.47957573, 0.48092137, 0.48130659, 0.48200571, 0.48163287, 0.48124801, 0.48151498, 0.48183316,\n       0.48105614, 0.48202484, 0.48178211, 0.48337829, 0.48439618, 0.48501782, 0.48545475, 0.48556795, 0.48550856, 0.48554074,\n       0.48550094, 0.48504976, 0.48480161, 0.48561331, 0.4852559 , 0.48513473, 0.48529147, 0.48530156, 0.48543741, 0.48569729,\n       0.48571309, 0.48571467, 0.4856701 , 0.48563657, 0.48579877, 0.48579767, 0.4858019 , 0.48580095, 0.48580002, 0.48580178,\n       0.48580211, 0.48579777, 0.4857996 , 0.48580236, 0.48579236, 0.48580043, 0.48580303, 0.4858034 , 0.4857613 , 0.4855666 ,\n       0.48579394, 0.48580506, 0.48580434, 0.48580707, 0.48579364, 0.48580788, 0.48581017, 0.48580597, 0.48581077, 0.48576815,\n       0.48580167, 0.48545792, 0.48567909, 0.48542059, 0.48557468, 0.48492654, 0.4852198 , 0.48548666, 0.48590271, 0.48601112,\n       0.48447755, 0.48543732])\n\n\n\nunq[scores.argmin()]\n\n6.0\n\n\nBased on this, it looks like, for instance, that for the Age column, 6 is the optimal cutoff according to our training set.\nWe can write a little function that implements this idea:\n\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\nLet’s try all the columns:\n\ncols = cats+conts\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\nAccording to this, Sex&lt;=0 is the best split we can use.\nWe’ve just re-invented the OneR classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it’s so simple and surprisingly effective, it makes for a great baseline – that is, a starting point that you can use to compare your more sophisticated models to.\nWe found earlier that out OneR rule had an error of around 0.215, so we’ll keep that in mind as we try out more sophisticated approaches.",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html#creating-a-decision-tree",
    "href": "how-random-forests-really-work.html#creating-a-decision-tree",
    "title": "How random forests really work",
    "section": "Creating a decision tree",
    "text": "Creating a decision tree\nHow can we improve our OneR classifier, which predicts survival based only on Sex?\nHow about we take each of our two groups, female and male, and create one more binary split for each of them. That is: fine the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section’s steps, once for males, and once for females.\nFirst, we’ll remove Sex from the list of possible splits (since we’ve already used it, and there’s only one possible split for that binary column), and create our two groups:\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nNow let’s find the single best binary split for males…:\n\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n…and for females:\n\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nWe can see that the best next binary split for males is Age&lt;=6, and for females is Pclass&lt;=2.\nBy adding these rules, we have created a decision tree, where our model will first check whether Sex is female or male, and depending on the result will then check either the above Age or Pclass rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we’ve now created.\nRather than writing that code manually, we can use DecisionTreeClassifier, from sklearn, which does exactly that for us:\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\nOne handy feature or this class is that it provides a function for drawing a tree representing the rules:\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\n\n\n\nWe can see that it’s found exactly the same splits as we did!\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (“samples”) match that set of rules, and shows how many perish or survive (“values”). There’s also something called “gini”. That’s another measure of impurity, and it’s very similar to the score() we created earlier. It’s defined as follows:\n\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n\nWhat this calculates is the probability that, if you pick two rows from a group, you’ll get the same Survived result each time. If the group is all the same, the probability is 1.0, and 0.0 if they’re all different:\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nLet’s see how this model compares to our OneR version:\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\nIt’s a tiny bit worse. Since this is such a small dataset (we’ve only got around 200 rows in our validation set) this small difference isn’t really meaningful. Perhaps we’ll see better results if we create a bigger tree:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=12)\n\n\n\n\n\n\n\n\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\nIt looks like this is an improvement, although again it’s a bit hard to tell with small datasets like this. Let’s try submitting it to Kaggle:\n\ntst_df[cats] = tst_df[cats]\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nWhen I submitted this, I got a score of 0.765, which isn’t as good as our linear models or most of our neural nets, but it’s pretty close to those results.\nHopefully you can now see why we didn’t really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here’s how the first few items of Embarked are labeled:\n\ndf.Embarked.head()\n\n0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']\n\n\n…resulting in these integer codes:\n\ndf.Embarked.cat.codes.head()\n\n0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8\n\n\nSo let’s say we wanted to split into “C” in one group, vs “Q” or “S” in the other group. Then we just have to split on codes &lt;=0 (since C is mapped to category 0). Note that if we wanted to split into “Q” in one group, we’d need to use two binary splits, first to separate “C” from “Q” and “S”, and then a second split to separate “Q” from “S”. For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\nIn practice, I often use dummy variables for &lt;4 levels, and numeric codes for &gt;=4 levels.",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html#the-random-forest",
    "href": "how-random-forests-really-work.html#the-random-forest",
    "title": "How random forests really work",
    "section": "The random forest",
    "text": "The random forest\nWe can’t make the decision tree much bigger than the example above, since some leaf nodes already have only 50 rows in them. That’s not a lot of data to make a prediction.\nSo how could we use bigger trees? One big insight came from Leo Breiman: what if we create lots of bigger trees, and take the average of their predictions? Taking the average prediction of a bunch of models in this way is known as bagging.\nThe idea is that we want each model’s predictions in the averaged ensemble to be uncorrelated with each other model. That way, if we average the predictions, the average will be equal to the true target value – that’s because the average of lots of uncorrelated random errors is zero. That’s quite an amazing insight!\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here’s how we can create a tree on a random subset of the data:\n\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\nNow we can create as many trees as we want:\n\ntrees = [get_tree() for t in range(100)]\n\nOur prediction will be the average of these trees’ predictions:\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nThis is nearly identical to what sklearn’s RandomForestClassifier does. The main extra piece in a “real” random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here’s how we repeat the above process with a random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(200, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y)\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.19282511210762332\n\n\nWe can submit that to Kaggle too:\n\n??subm\n\n\nSignature: subm(preds, suff)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\nFile:      /tmp/ipykernel_1170724/3774548726.py\nType:      function\n\n\n\nI found that gave nearly an identical result as our single tree (which, in turn, was slightly lower than our linear and neural net models in the previous notebook).\nOne particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using feature_importances_:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\n\n\n\n\nWe can see that Sex is by far the most important predictor, with Pclass a distant second, and LogFare and Age behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn’t really need to take the log() of Fare, since random forests only care about order, and log() doesn’t change the order – we only did it to make our graphs earlier easier to read.)\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at chapter 8 of our book.",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html#conclusion",
    "href": "how-random-forests-really-work.html#conclusion",
    "title": "How random forests really work",
    "section": "Conclusion",
    "text": "Conclusion\nSo what can we take away from all this?\nI think the first thing I’d note from this is that, clearly, more complex models aren’t always better. Our “OneR” model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn’t an improvement on the single decision tree at all.\nSo we should always be careful to benchmark simple models, as see if they’re good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there’s no need to guess – it’s so easy to try a few different models, there’s no reason not to give the simpler ones a go too!\nAnother thing I think we can take away is that random forests aren’t actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren’t sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "how-random-forests-really-work.html#tips",
    "href": "how-random-forests-really-work.html#tips",
    "title": "How random forests really work",
    "section": "Tips",
    "text": "Tips\n\nGet standard deviation of the random forest results to get an idea of reliablity\nfrom sklearn. inspection import plot_partial_dependence",
    "crumbs": [
      "Blog",
      "How random forests really work"
    ]
  },
  {
    "objectID": "nca.html",
    "href": "nca.html",
    "title": "Setup 2",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nimport torchvision.transforms as trans\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *",
    "crumbs": [
      "Blog",
      "Setup 2"
    ]
  },
  {
    "objectID": "nca.html#background-neural-cellular-automata",
    "href": "nca.html#background-neural-cellular-automata",
    "title": "Setup 2",
    "section": "Background: Neural Cellular Automata",
    "text": "Background: Neural Cellular Automata\nA cellular automaton is a discrete model that consists of a grid of cells, each of which can be in one of a finite number of states. The cells are updated simultaneously based on a set of rules that determine the state of a cell based on the states of its neighbors.\nCellular automata are often used to model complex systems and can exhibit emergent behavior, meaning that patterns and behaviors emerge from the interactions of the individual cells even though the rules governing their behavior are simple.\nClassic examples such as the famous ‘Game of Life’ have very simple rules and limit states to ‘alive’ or ‘dead’. However, the ideas can be extended to continuous outputs for each cell, and the update ‘rule’ can be a small neural network rather than a hard-coded decision tree - giving us ‘Neural Cellular Automata’.\nHere’s what our NCA will look like:\n\n\n\nimage.png\n\n\nKey references: - Growing Neural Cellular Automata - A delightful paper that was my intro to the topic - Texture Generation with NCA - tiny models making amazing textures, the paper we’re replicating today. - ‘The Future of Artificial Intelligence is Self-Organizing and Self-Assembling’ - More general discussion of this space - Fixing Neural CA Colors with Sliced Optimal Transport - A follow-on video from Alexander Mordvintsev (include code for a different style loss) - Fun with Neural Cellular Automata - My W&B report with lots of examples",
    "crumbs": [
      "Blog",
      "Setup 2"
    ]
  },
  {
    "objectID": "nca.html#goal-match-this-style-with-an-nca",
    "href": "nca.html#goal-match-this-style-with-an-nca",
    "title": "Setup 2",
    "section": "Goal: Match This Style with an NCA",
    "text": "Goal: Match This Style with an NCA\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);",
    "crumbs": [
      "Blog",
      "Setup 2"
    ]
  },
  {
    "objectID": "nca.html#style-loss-see-17a",
    "href": "nca.html#style-loss-see-17a",
    "title": "Setup 2",
    "section": "Style Loss (see 17A):",
    "text": "Style Loss (see 17A):\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = trans.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\n# Batched version of the previous notebook's gram matrix function\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\ntensor(1175.81, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)",
    "crumbs": [
      "Blog",
      "Setup 2"
    ]
  },
  {
    "objectID": "nca.html#defining-the-nca-model",
    "href": "nca.html#defining-the-nca-model",
    "title": "Setup 2",
    "section": "Defining the NCA model",
    "text": "Defining the NCA model\n\nnum_channels = 4\nhidden_n = 8\n\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\n\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\n\nx = make_grids(1)\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\n# Brain: linear layer style\nbrain = nn.Sequential(\n    nn.Linear(num_channels*4, hidden_n),\n    nn.ReLU(),\n    nn.Linear(hidden_n, num_channels, bias=False)\n).to(def_device)\nmodel_inputs_flat = einops.rearrange(model_inputs, 'b c h w -&gt; (b h w) c') # (1*128*128, 16)\nbrain_preds = brain(model_inputs_flat).reshape(x.shape)\nbrain_preds.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\n\n[p.shape for p in brain.parameters()]\n\n[torch.Size([8, 16]), torch.Size([8]), torch.Size([4, 8])]\n\n\n\n# Brain: conv style\nbrain = nn.Sequential(\n    nn.Conv2d(num_channels*4, hidden_n, 1),\n    nn.ReLU(),\n    nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n).to(def_device)\nbrain_preds = brain(model_inputs).reshape(x.shape)\nbrain_preds.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\n\n[p.shape for p in brain.parameters()]\n\n[torch.Size([8, 16, 1, 1]), torch.Size([8]), torch.Size([4, 8, 1, 1])]\n\n\nPutting this into a class, with a few extra features: - Random update: only update ~50% of the cells - to_rgb function to scale and show the first 3 channels as an RGB image - An option to zero out the weights of the second layer. Think: why is this useful?\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our 'brain'\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5",
    "crumbs": [
      "Blog",
      "Setup 2"
    ]
  },
  {
    "objectID": "nca.html#training",
    "href": "nca.html#training",
    "title": "Setup 2",
    "section": "Training",
    "text": "Training\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn\n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1972.588\n0\ntrain\n\n\n25.787\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n    x = model(x)\n    if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168",
    "crumbs": [
      "Blog",
      "Setup 2"
    ]
  },
  {
    "objectID": "backpropagation.html",
    "href": "backpropagation.html",
    "title": "Backpropagation",
    "section": "",
    "text": "n,m = x_train.shape\nc = y_train.max()+1\nn,m,c\n\n(50000, 784, tensor(10))\n\n\n\n# num hidden\nnh = 50\n\n\nw1 = torch.randn(m,nh)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)\nb2 = torch.zeros(1)\n\n\ndef lin(x, w, b): return x@w + b\n\n\nt = lin(x_valid, w1, b1)\nt.shape\n\ntorch.Size([10000, 50])\n\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = relu(t)\nt\n\ntensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n        ...,\n        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])\n\n\n\ndef model(xb):\n    l1 = lin(xb, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\n\nres = model(x_valid)\nres.shape\n\ntorch.Size([10000, 1])\n\n\n\n\n\n(Of course, mse is not a suitable loss function for multi-class classification; we’ll use a better loss function soon. We’ll use mse for now to keep things simple.)\n\nres.shape,y_valid.shape\n\n(torch.Size([10000, 1]), torch.Size([10000]))\n\n\n\n(res-y_valid).shape\n\ntorch.Size([10000, 10000])\n\n\nWe need to get rid of that trailing (,1), in order to use mse.\n\nres[:,0].shape\n\ntorch.Size([10000])\n\n\n\nres.squeeze().shape\n\ntorch.Size([10000])\n\n\n\n(res[:,0]-y_valid).shape\n\ntorch.Size([10000])\n\n\n\ny_train,y_valid = y_train.float(),y_valid.float()\n\npreds = model(x_train)\npreds.shape\n\ntorch.Size([50000, 1])\n\n\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\n\nmse(preds, y_train)\n\ntensor(4308.76)\n\n\n\n\n\n\nfrom sympy import symbols,diff\nx,y = symbols('x y')\ndiff(x**2, x)\n\n\\(\\displaystyle 2 x\\)\n\n\n\ndiff(3*x**2+9, x)\n\n\\(\\displaystyle 6 x\\)",
    "crumbs": [
      "Blog",
      "Backpropagation"
    ]
  },
  {
    "objectID": "backpropagation.html#foundations-version",
    "href": "backpropagation.html#foundations-version",
    "title": "Backpropagation",
    "section": "",
    "text": "n,m = x_train.shape\nc = y_train.max()+1\nn,m,c\n\n(50000, 784, tensor(10))\n\n\n\n# num hidden\nnh = 50\n\n\nw1 = torch.randn(m,nh)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)\nb2 = torch.zeros(1)\n\n\ndef lin(x, w, b): return x@w + b\n\n\nt = lin(x_valid, w1, b1)\nt.shape\n\ntorch.Size([10000, 50])\n\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = relu(t)\nt\n\ntensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n        ...,\n        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])\n\n\n\ndef model(xb):\n    l1 = lin(xb, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\n\nres = model(x_valid)\nres.shape\n\ntorch.Size([10000, 1])\n\n\n\n\n\n(Of course, mse is not a suitable loss function for multi-class classification; we’ll use a better loss function soon. We’ll use mse for now to keep things simple.)\n\nres.shape,y_valid.shape\n\n(torch.Size([10000, 1]), torch.Size([10000]))\n\n\n\n(res-y_valid).shape\n\ntorch.Size([10000, 10000])\n\n\nWe need to get rid of that trailing (,1), in order to use mse.\n\nres[:,0].shape\n\ntorch.Size([10000])\n\n\n\nres.squeeze().shape\n\ntorch.Size([10000])\n\n\n\n(res[:,0]-y_valid).shape\n\ntorch.Size([10000])\n\n\n\ny_train,y_valid = y_train.float(),y_valid.float()\n\npreds = model(x_train)\npreds.shape\n\ntorch.Size([50000, 1])\n\n\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\n\nmse(preds, y_train)\n\ntensor(4308.76)\n\n\n\n\n\n\nfrom sympy import symbols,diff\nx,y = symbols('x y')\ndiff(x**2, x)\n\n\\(\\displaystyle 2 x\\)\n\n\n\ndiff(3*x**2+9, x)\n\n\\(\\displaystyle 6 x\\)",
    "crumbs": [
      "Blog",
      "Backpropagation"
    ]
  },
  {
    "objectID": "backpropagation.html#chain-rule",
    "href": "backpropagation.html#chain-rule",
    "title": "Backpropagation",
    "section": "Chain Rule",
    "text": "Chain Rule\n\\(y = 3x^2 + 9 \\rightarrow u = x^2, y = 3u + 9\\)\n\\(\\dfrac{dy}{dx} = \\dfrac{dy}{du} \\times \\dfrac{du}{dx}\\)\n\\(\\dfrac{d(3x^2 + 9)}{dx} = \\dfrac{d(3u + 9)}{du} \\times \\dfrac{d(x^2)}{dx}\\)\n\\(\\dfrac{d(3x^2 + 9)}{dx} = 3 + 0 \\times 2x\\)\n\\(\\dfrac{dy}{dx} = 6x\\)",
    "crumbs": [
      "Blog",
      "Backpropagation"
    ]
  },
  {
    "objectID": "backpropagation.html#backpropagation",
    "href": "backpropagation.html#backpropagation",
    "title": "Backpropagation",
    "section": "Backpropagation",
    "text": "Backpropagation\nLoss(L_2_, y)\nL_2_(relu)\nrelu(L_1_)\nL_1_(X, W)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"white\")\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 4))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(w1.T, cmap=cmap, vmax=6.3, center=0,\n            square=False, linewidths=0, cbar_kws={\"shrink\": .5})\n\n\n\n\n\n\n\n\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    #import pdb;pdb.set_trace()\n    i, o = inp.unsqueeze(-1), out.g.unsqueeze(1)\n    w.g = (i * o).sum(0)\n    b.g = out.g.sum(0)\n\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1&gt;0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\n\nforward_and_backward(x_train, y_train)\n\n\n# Save for testing against later\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n\n\nprint(w1.grad)\n\nNone\n\n\nWe cheat a little bit and use PyTorch autograd to check our results.\n\ndef mkgrad(x): return x.clone().requires_grad_(True)\nptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))\n\n\ndef forward(inp, targ):\n    l1 = lin(inp, w12, b12)\n    l2 = relu(l1)\n    out = lin(l2, w22, b22)\n    return mse(out, targ)\n\n\nloss = forward(xt2, y_train)\nloss.backward()\n\n\n#for a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)",
    "crumbs": [
      "Blog",
      "Backpropagation"
    ]
  },
  {
    "objectID": "backpropagation.html#refactor-model",
    "href": "backpropagation.html#refactor-model",
    "title": "Backpropagation",
    "section": "Refactor model",
    "text": "Refactor model\n\nLayers as classes\n\nclass A:\n    def __call__(self, x): print(f'hi {x}')\n\na = A()\na('asd')\n\nhi asd\n\n\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g\n\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nModule.forward()\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n\n\nclass Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g\n\n\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n    def forward(self, inp): return inp@self.w + self.b\n    def bwd(self, out, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse(Module):\n    def forward (self, inp, targ): \n        self.diff = inp.squeeze() - targ\n        return (self.diff).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(self.diff).unsqueeze(-1) / targ.shape[0]\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nAutograd\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Linear(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in,n_out).requires_grad_()\n        self.b = torch.zeros(n_out).requires_grad_()\n    def forward(self, inp): return inp@self.w + self.b\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return F.mse_loss(x, targ[:,None])\n\n\nmodel = Model(m, nh, 1)\nloss = model(x_train, y_train)\nloss.backward()\n\n\nl0 = model.layers[0]\nl0.b.grad\n\ntensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])",
    "crumbs": [
      "Blog",
      "Backpropagation"
    ]
  },
  {
    "objectID": "multi-target.html",
    "href": "multi-target.html",
    "title": "Mulit target Example",
    "section": "",
    "text": "# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nThis is part 4 of the Road to the Top series, in which I show the process I used to tackle the Paddy Doctor competition. The first three parts show the process I took that lead to four 1st place submissions. This part onwards discusses various extensions and advanced tricks you might want to consider – but which I don’t promise will necessarily help your score (you’ll need to do your own experiments to figure this out!) If you haven’t read the rest of this series yet, I’d suggest starting at part 1.\nIn this notebook we’re going to build a model that doesn’t just predict what disease the rice paddy has, but also predicts what kind of rice is shown.\nThis might sound like a bad idea. After all, doesn’t that mean that the model has more to do? Mightn’t it get rather distracted from its main task, which is to identify paddy disease?\nPerhaps… But in previous projects I’ve often found the opposite to be true, especially when training for quite a few epochs. By giving the model more signal about what is present in a picture, it may be able to use this information to find more interesting features that predict our target of interest. For instance, perhaps some of the features of disease change between varieties.",
    "crumbs": [
      "Blog",
      "Mulit target Example"
    ]
  },
  {
    "objectID": "multi-target.html#multi-output-dataloader",
    "href": "multi-target.html#multi-output-dataloader",
    "title": "Mulit target Example",
    "section": "Multi-output DataLoader",
    "text": "Multi-output DataLoader\nFirst we’ll repeat the steps we used last time to access the data and ensure all the latest libraries are installed:\n\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\nfrom fastai.vision.all import *\nset_seed(42)\n\n\ndatapath = Path('./Data')\nname = 'paddy-disease-classification'\npath = Path(f'{datapath}/{name}')\n\nkaggle_competition_download(name, datapath)\n\nfile exists\n\n\n\nfrom fastcore.parallel import *\ntrn_path = Path(f'{path}/train_images')\n\nHere’s the CSV that Kaggle provides, showing the variety of rice contained in each image – we’ll make image_id the index of our data frame so that we can look up images directly to grab their variety:\n\ndf = pd.read_csv(f'{path}/train.csv', index_col='image_id')\ndf.head()\n\n\n\n\n\n\n\n\nlabel\nvariety\nage\n\n\nimage_id\n\n\n\n\n\n\n\n100330.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n100365.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n100382.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n100632.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n101918.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n\n\n\n\n\nPandas uses the loc attribute to look up rows by index. Here’s how we can get the variety of image 100330.jpg, for instance:\n\ndf.loc['100330.jpg', 'variety']\n\n'ADT45'\n\n\nOur DataBlock will be using get_image_files to get the list of training images, which returns Path objects. Therefore, to look up an item to get its variety, we’ll need to pass its name. Here’s a function which does just that:\n\ndef get_variety(p): return df.loc[p.name, 'variety']\n\nWe’re now ready to create our DataLoaders. To do this, we’ll use the DataBlock API, which is a flexible and convenient way to plug pieces of a data processing pipeline together:\n\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label,get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(192, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n).dataloaders(trn_path)\n\nHere’s an explanation of each line:\nblocks=(ImageBlock,CategoryBlock,CategoryBlock),\nThe DataBlock will create 3 things from each file: an image (the contents of the file), and 2 categorical variables (the disease and the variety).\nn_inp=1,\nThere is 1 input (the image) – and therefore the other two variables (the two categories) are outputs.\nget_items=get_image_files,\nUse get_image_files to get a list of inputs.\nget_y = [parent_label,get_variety],\nTo create the two outputs for each file, call two functions: parent_label (from fastai) and get_variety (defined above).\nsplitter=RandomSplitter(0.2, seed=42),\nRandomly split the input into 80% train and 20% validation sets.\nitem_tfms=Resize(192, method='squish'),\nbatch_tfms=aug_transforms(size=128, min_scale=0.75)\nThese are the same item and batch transforms we’ve used in previous notebooks.\nLet’s take a look at part of a batch of this data:\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nWe can see that fastai has created both the image input and two categorical outputs that we requested!",
    "crumbs": [
      "Blog",
      "Mulit target Example"
    ]
  },
  {
    "objectID": "multi-target.html#replicating-the-disease-model",
    "href": "multi-target.html#replicating-the-disease-model",
    "title": "Mulit target Example",
    "section": "Replicating the disease model",
    "text": "Replicating the disease model\nNow we’ll replicate the same disease model we’ve made before, but have it work with this new data.\nThe key difference is that our metrics and loss will now receive three things instead of two: the model outputs (i.e. the metric and loss function inputs), and the two targets (disease and variety). Therefore, we need to define slight variations of our metric (error_rate) and loss function (cross_entropy) to pass on just the disease target:\n\ndef disease_err(inp,disease,variety): return error_rate(inp,disease)\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp,disease)\n\nWe’re now ready to create our learner.\nThere’s just one wrinkle to be aware of. Now that our DataLoaders is returning multiple targets, fastai doesn’t know how many outputs our model will need. Therefore we have to pass n_out when we create our Learner – we need 10 outputs, one for each possible disease:\n\narch = 'convnext_small'\nlearn = vision_learner(dls, arch, loss_func=disease_loss, metrics=disease_err, n_out=10).to_fp16()\nlr = 0.01\n\nWhen we train this model we should get similar results to what we’ve seen with similar models before:\n\nlearn.fine_tune(5, lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ndisease_err\ntime\n\n\n\n\n0\n1.433345\n1.037968\n0.308986\n01:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ndisease_err\ntime\n\n\n\n\n0\n0.730947\n0.590886\n0.190774\n01:03\n\n\n1\n0.522005\n0.622835\n0.196540\n01:08\n\n\n2\n0.313315\n0.299247\n0.090822\n01:00\n\n\n3\n0.141513\n0.134452\n0.040846\n01:01\n\n\n4\n0.081356\n0.110794\n0.031235\n01:03",
    "crumbs": [
      "Blog",
      "Mulit target Example"
    ]
  },
  {
    "objectID": "multi-target.html#multi-target-model",
    "href": "multi-target.html#multi-target-model",
    "title": "Mulit target Example",
    "section": "Multi-target model",
    "text": "Multi-target model\nIn order to predict both the probability of each disease, and of each variety, we’ll now need the model to output a tensor of length 20, since there are 10 possible diseases, and 10 possible varieties. We can do this by setting n_out=20:\n\nlearn = vision_learner(dls, arch, n_out=20).to_fp16()\n\nWe can define disease_loss just like we did previously, but with one important change: the input tensor is now length 20, not 10, so it doesn’t match the number of possible diseases. We can pick whatever part of the input we want to be used to predict disease. Let’s use the first 10 values:\n\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease)\n\nThat means we can do the same thing for predicting variety, but use the last 10 values of the input, and set the target to variety instead of disease:\n\ndef variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety)\n\nOur overall loss will then be the sum of these two losses:\n\ndef combine_loss(inp,disease,variety): return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety)\n\nIt would be useful to view the error rate for each of the outputs too, so let’s do the same thing for out metrics:\n\ndef disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease)\ndef variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety)\n\nerr_metrics = (disease_err,variety_err)\n\nIt’s useful to see the loss for each of the outputs too, so we’ll add those as metrics:\n\nall_metrics = err_metrics+(disease_loss,variety_loss)\n\nWe’re now ready to create and train our Learner:\n\nlearn = vision_learner(dls, arch, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16()\n\n\nlearn.fine_tune(5, lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ndisease_err\nvariety_err\ndisease_loss\nvariety_loss\ntime\n\n\n\n\n0\n2.574120\n1.809338\n0.410860\n0.161941\n1.277781\n0.531556\n00:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ndisease_err\nvariety_err\ndisease_loss\nvariety_loss\ntime\n\n\n\n\n0\n1.099624\n0.838923\n0.165786\n0.114849\n0.526330\n0.312593\n00:54\n\n\n1\n0.767783\n0.732510\n0.133590\n0.103316\n0.433259\n0.299251\n00:58\n\n\n2\n0.439430\n0.287075\n0.063431\n0.021144\n0.223693\n0.063382\n00:59\n\n\n3\n0.207751\n0.180888\n0.043729\n0.010091\n0.145213\n0.035676\n01:00\n\n\n4\n0.117015\n0.163008\n0.038443\n0.010572\n0.129077\n0.033932\n01:00",
    "crumbs": [
      "Blog",
      "Mulit target Example"
    ]
  },
  {
    "objectID": "multi-target.html#conclusion",
    "href": "multi-target.html#conclusion",
    "title": "Mulit target Example",
    "section": "Conclusion",
    "text": "Conclusion\nSo, is this useful?\nWell… if you actually want a model that predicts multiple things, then yes, definitely! But as to whether it’s going to help us better predict rice disease, I honestly don’t know. I haven’t come across any research that tackles this important question: when can a multi-target model improve the accuracy of the individual targets compared to a single target model? (That doesn’t mean it doesn’t exist of course – perhaps it does and I haven’t found it yet…)\nI’ve certainly found in previous projects that there are cases where improvements to single targets can be made by using a multi-target model. I’d guess that it’ll be most useful when you’re having problems with overfitting. So try doing this with more epochs, and let me know how you go!\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. And if you have any questions or comments, please pop them below – I read every comment I receive.\n\n# This is what I use to push my notebook from my home PC to Kaggle\nif not iskaggle:\n    push_notebook('jhoward', 'multi-target-road-to-the-top-part-4',\n                  title='Multi-target: Road to the Top, Part 4',\n                  file='11-multitask.ipynb',\n                  competition=comp, private=False, gpu=True)",
    "crumbs": [
      "Blog",
      "Mulit target Example"
    ]
  },
  {
    "objectID": "initializing.html",
    "href": "initializing.html",
    "title": "Initialization",
    "section": "",
    "text": "Exported source\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport sys,gc,traceback\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n@inplace\ndef transformi(b): b[xl] = [TF.to_tensor(o) for o in b[xl]]\n\nbs = 1024\ntds = dsd.with_transform(transformi)\n\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))\nsource",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#glorotxavier-init",
    "href": "initializing.html#glorotxavier-init",
    "title": "Initialization",
    "section": "Glorot/Xavier init",
    "text": "Glorot/Xavier init\n\nx = torch.randn(200, 100)\nx[0:5,0:5]\n\ntensor([[-0.60, -1.38, -1.35,  1.16,  2.13],\n        [ 0.77, -1.04,  0.61, -0.06,  0.34],\n        [-0.71, -0.26,  2.27, -1.28, -0.68],\n        [-0.56, -0.87,  1.25, -2.16, -0.48],\n        [-0.37, -1.65,  0.45, -1.31,  0.25]])\n\n\n\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n\n\nThe result is nans everywhere. So maybe the scale of our matrix was too big, and we need to have smaller weights? But if we use too small weights, we will have the opposite problem—the scale of our activations will go from 1 to 0.1, and after 50 layers we’ll be left with zeros everywhere:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nSo we have to scale our weight matrices exactly right so that the standard deviation of our activations stays at 1. We can compute the exact value to use mathematically, as illustrated by Xavier Glorot and Yoshua Bengio in “Understanding the Difficulty of Training Deep Feedforward Neural Networks”. The right scale for a given layer is \\(1/\\sqrt{n_{in}}\\), where \\(n_{in}\\) represents the number of inputs.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\n\ntensor([[-1.47, -0.81, -0.62,  0.14, -1.07],\n        [ 0.13,  0.97, -0.34, -0.67,  0.88],\n        [-0.31, -0.08,  0.16,  0.63, -0.87],\n        [ 0.01,  0.34, -0.06, -0.32,  0.49],\n        [-1.60,  0.97, -1.06, -1.42, -0.15]])",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#background",
    "href": "initializing.html#background",
    "title": "Initialization",
    "section": "Background",
    "text": "Background\n\nVariance and standard deviation\nVariance is the average of how far away each data point is from the mean. E.g.:\n\nt = torch.tensor([1.,2.,4.,18])\n\n\nm = t.mean(); m\n\ntensor(6.25)\n\n\n\n(t-m).mean()\n\ntensor(0.)\n\n\nOops. We can’t do that. Because by definition the positives and negatives cancel out. So we can fix that in one of (at least) two ways:\n\nVariance\n\n(t-m).pow(2).mean()\n\ntensor(47.19)\n\n\n\n(t-m).pow(2).sqrt().mean()\n\ntensor(5.88)\n\n\n\n\nMean absolute difference\n\n(t-m).abs().mean()\n\ntensor(5.88)\n\n\nBut the first of these is now a totally different scale, since we squared. So let’s undo that at the end.\n\n\n\nStandard deviation (std)\n\n(t-m).pow(2).mean().sqrt()\n\ntensor(6.87)\n\n\nThey’re still different. Why?\nNote that we have one outlier (18). In the version where we square everything, it makes that much bigger than everything else.\n(t-m).pow(2).mean() is refered to as variance. It’s a measure of how spread out the data is, and is particularly sensitive to outliers.\nWhen we take the sqrt of the variance, we get the standard deviation. Since it’s on the same kind of scale as the original data, it’s generally more interpretable. However, since sqrt(1)==1, it doesn’t much matter which we use when talking about unit variance for initializing neural nets.\nThe standard deviation represents if the data stays close to the mean or on the contrary gets values that are far away. It’s computed by the following formula:\n\\[\\sigma = \\sqrt{\\frac{1}{n}\\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \\cdots + (x_{n-1}-m)^{2}\\right]}\\]\nwhere m is the mean and \\(\\sigma\\) (the greek letter sigma) is the standard deviation. Here we have a mean of 0, so it’s just the square root of the mean of x squared.\n(t-m).abs().mean() is referred to as the mean absolute deviation. It isn’t used nearly as much as it deserves to be, because mathematicians don’t like how awkward it is to work with. But that shouldn’t stop us, because we have computers and stuff.\nHere’s a useful thing to note about variance:\n\n(t-m).pow(2).mean(), (t*t).mean() - (m*m)\n\n(tensor(47.19), tensor(47.19))\n\n\nYou can see why these are equal if you want to work thru the algebra. Or not.\nBut, what’s important here is that the latter is generally much easier to work with. In particular, you only have to track two things: the sum of the data, and the sum of squares of the data. Whereas in the first form you actually have to go thru all the data twice (once to calculate the mean, once to calculate the differences).\nLet’s go steal the LaTeX from Wikipedia:\n\\[\\operatorname{E}\\left[X^2 \\right] - \\operatorname{E}[X]^2\\]\n\n\nCovariance\nHere’s how Wikipedia defines covariance:\n\\[\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}\\]\nLet’s see that in code. So now we need two vectors.\n\nt\n\ntensor([ 1.,  2.,  4., 18.])\n\n\n\n# `u` is twice `t`, plus a bit of randomness\nu = t*2\nu *= torch.randn_like(t)/10+0.95\n\nplt.scatter(t, u);\n\n\n\n\n\n\n\n\n\nprod = (t-t.mean())*(u-u.mean()); prod\n\ntensor([ 52.33,  37.08,  12.03, 282.48])\n\n\n\nprod.mean()\n\ntensor(95.98)\n\n\n\nv = torch.randn_like(t)\nplt.scatter(t, v);\n\n\n\n\n\n\n\n\n\n((t-t.mean())*(v-v.mean())).mean()\n\ntensor(-1.28)\n\n\nIt’s generally more conveniently defined like so:\n\\[\\operatorname{E}\\left[X Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right]\\]\n\ncov = (t*v).mean() - t.mean()*v.mean(); cov\n\ntensor(-1.28)\n\n\nFrom now on, you’re not allowed to look at an equation (or especially type it in LaTeX) without also typing it in Python and actually calculating some values. Ideally, you should also plot some values.\nFinally, here is the Pearson correlation coefficient:\n\\[\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y}\\]\n\ncov / (t.std() * v.std())\n\ntensor(-0.41)\n\n\nIt’s just a scaled version of the same thing.\n\nx.std()\n\ntensor(1.13)\n\n\n\n\nXavier init derivation\nWhen we do y = a @ x, the coefficients of y are defined by\n\\[y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \\cdots + a_{i,n-1} x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k} x_{k}\\]\nor in pure python code:\ny[i] = sum([c*d for c,d in zip(a[i], x)])\nor in numpy/pytorch code:\ny[i] = (a[i]*x).sum()\nAt the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way).\n\nx = torch.randn(100)\nx.mean(), x.std()\n\n(tensor(-0.03), tensor(1.05))\n\n\nIf we go back to y = a @ x and assume that we chose weights for a that also have a mean of 0, we can compute the standard deviation of y quite easily. Since it’s random, and we may fall on bad numbers, we repeat the operation 100 times.\n\nmean,sqr = 0.,0.\nfor i in range(100):\n    x = torch.randn(100)\n    a = torch.randn(512, 100)\n    y = a @ x\n    mean += y.mean().item()\n    sqr  += y.pow(2).mean().item()\nmean/100,sqr/100\n\n(0.02379358373582363, 101.66621231079101)\n\n\nNow that looks very close to the dimension of our matrix 100. And that’s no coincidence! When you compute y, you sum 100 product of one element of a by one element of x. So what’s the mean and the standard deviation of such a product? We can show mathematically that as long as the elements in a and the elements in x are independent, the mean is 0 and the std is 1. This can also be seen experimentally:\n\nmean,sqr = 0.,0.\nfor i in range(10000):\n    x = torch.randn(1)\n    a = torch.randn(1)\n    y = a*x\n    mean += y.item()\n    sqr  += y.pow(2).item()\nmean/10000,sqr/10000\n\n(-0.008459147600694087, 0.9879768942045608)\n\n\nThen we sum 100 of those things that have a mean of zero, and a mean of squares of 1, so we get something that has a mean of 0, and mean of square of 100, hence math.sqrt(100) being our magic number. If we scale the weights of the matrix and divide them by this math.sqrt(100), it will give us a y of scale 1, and repeating the product has many times as we want won’t overflow or vanish.",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#kaiminghe-init",
    "href": "initializing.html#kaiminghe-init",
    "title": "Initialization",
    "section": "Kaiming/He init",
    "text": "Kaiming/He init\n(“He” is a Chinese surname and is pronouced like “Her”, not like “Hee”.)\n\nBackground\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\n\n\nExported source\nfrom math import sqrt\n\n\n\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n\n\nsource\n\n\nlin\n\n lin (x, w, b)\n\n\n\nExported source\ndef lin(x, w, b): return x @ w + b\n\n\n\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n\n(tensor(-0.01), tensor(1.00))\n\n\n\nsource\n\n\nrelu\n\n relu (x)\n\n\n\nExported source\ndef relu(x): return x.clamp_min(0.)\n\n\n\nl2 = relu(l1)\nl2.mean(),l2.std()\n\n(tensor(0.39), tensor(0.57))\n\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\n\ntensor([[    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00],\n        [    0.00,     0.00,     0.00,     0.00,     0.00]])\n\n\nIn “Delving Deep into Rectifiers: Surpassing Human-Level Performance” Kaiming He et al. show that we should use the following scale instead: \\(\\sqrt{2 / n_{in}}\\), where \\(n_{in}\\) is the number of inputs of our model.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5], x.mean(), x.std()\n\n(tensor([[0.38, 0.27, 0.22, 0.01, 0.26],\n         [0.42, 0.25, 0.17, 0.04, 0.25],\n         [0.41, 0.26, 0.20, 0.06, 0.22],\n         [0.77, 0.52, 0.26, 0.13, 0.34],\n         [0.55, 0.31, 0.24, 0.11, 0.38]]),\n tensor(0.14),\n tensor(0.21))\n\n\n\n\nApplying an init function\n\nmodel = get_model()\nmodel.apply(lambda m: print(type(m).__name__));\n\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nFlatten\nSequential\n\n\n\nmodel\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (1): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): Flatten(start_dim=1, end_dim=-1)\n)\n\n\n\nsource\n\n\ninit_weights\n\n init_weights (m)\n\n\n\nExported source\ndef init_weights(m):\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): init.kaiming_normal_(m.weight)\n\n\n\nmodel.apply(init_weights);\n\n\nMomentumLearner(model, dls, F.cross_entropy, cbs=[DeviceCB()]).lr_find()\n\n\n\n\n\n\n\n\n\nset_seed(42)\nlearn = MomentumLearner(get_model().apply(init_weights), dls, F.cross_entropy, lr=0.2, cbs=cbs)\ncbs\n\n[&lt;fastAIcourse.learner.DeviceCB&gt;,\n &lt;fastAIcourse.learner.MetricsCB&gt;,\n &lt;fastAIcourse.learner.ProgressCB&gt;,\n &lt;fastAIcourse.activations.ActivationStats&gt;]\n\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.587\n1.228\n0\ntrain\n\n\n0.783\n0.589\n0\neval\n\n\n0.818\n0.487\n1\ntrain\n\n\n0.833\n0.461\n1\neval\n\n\n0.845\n0.422\n2\ntrain\n\n\n0.845\n0.421\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\n\n\n\nastats.plot_stats()",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#input-normalization",
    "href": "initializing.html#input-normalization",
    "title": "Initialization",
    "section": "Input normalization",
    "text": "Input normalization\n\nxmean,xstd = xb.mean(),xb.std()\nxmean,xstd\n\n(tensor(0.29), tensor(0.35))\n\n\n\nsource\n\nBatchTransformCB\n\n BatchTransformCB (tfm, on_train=True, on_val=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass BatchTransformCB(Callback):\n    def __init__(self, tfm, on_train=True, on_val=True): fc.store_attr()\n\n    def before_batch(self, learn):\n        if (self.on_train and learn.training) or (self.on_val and not learn.training):\n            learn.batch = self.tfm(learn.batch)\n\n\n\ndef _norm(b): return (b[0]-xmean)/xstd,b[1]\nnorm = BatchTransformCB(_norm)\n\n\nset_seed(42)\nlearn = MomentumLearner(get_model().apply(init_weights), dls, F.cross_entropy, lr=0.2, cbs=cbs+[norm])\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.551\n1.386\n0\ntrain\n\n\n0.764\n0.617\n0\neval\n\n\n0.811\n0.514\n1\ntrain\n\n\n0.817\n0.492\n1\neval\n\n\n0.841\n0.434\n2\ntrain\n\n\n0.843\n0.431\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\n\n\n\n\n\n@inplace\ndef transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\nxb,yb = next(iter(dls.train))\n\n\nxb.mean(),xb.std()\n\n(tensor(0.01), tensor(1.00))",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#general-relu",
    "href": "initializing.html#general-relu",
    "title": "Initialization",
    "section": "General ReLU",
    "text": "General ReLU\n\nsource\n\nGeneralRelu\n\n GeneralRelu (leak=None, sub=None, maxv=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None):\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x): \n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n        if self.sub is not None: x -= self.sub\n        if self.maxv is not None: x.clamp_max_(self.maxv)\n        return x\n\n\n\nsource\n\n\nplot_func\n\n plot_func (f, start=-5.0, end=5.0, steps=100)\n\n\n\nExported source\ndef plot_func(f, start=-5., end=5., steps=100):\n    x = torch.linspace(start, end, steps)\n    plt.plot(x, f(x))\n    plt.grid(True, which='both', ls='--')\n    plt.axhline(y=0, color='k', linewidth=0.7)\n    plt.axvline(x=0, color='k', linewidth=0.7)\n\n\n\nplot_func(GeneralRelu())\n\n\n\n\n\n\n\n\n\nplot_func(GeneralRelu(leak=0.1, sub=0.4, maxv = 3))\n\n\n\n\n\n\n\n\n\nsource\n\n\nconv\n\n conv (ni, nf, ks=3, stride=2, act=&lt;class\n       'torch.nn.modules.activation.ReLU'&gt;)\n\n\n\nExported source\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, act())\n    return res\n\n\n\nsource\n\n\nget_model\n\n get_model (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=None)\n\n\n\nExported source\ndef get_model(act=nn.ReLU, nfs=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None), nn.Flatten()).to(def_device)\n\n\n\nsource\n\n\ninit_weights\n\n init_weights (m, leaky=0.0)\n\n\n\nExported source\ndef init_weights(m, leaky=0.):\n    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): init.kaiming_normal_(m.weight, a=leaky)\n\n\n\nleak = 0.1\nact_gr = partial(GeneralRelu, leak=leak, sub=0.4)\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\niw = partial(init_weights, leaky=leak)\n\n\nmodel = get_model(act_gr).apply(iw)\n\n\nset_seed(42)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.774\n0.625\n0\ntrain\n\n\n0.846\n0.420\n0\neval\n\n\n0.868\n0.361\n1\ntrain\n\n\n0.860\n0.375\n1\neval\n\n\n0.880\n0.324\n2\ntrain\n\n\n0.866\n0.363\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\n\n\n\n\n\nastats.dead_chart()",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#lsuv",
    "href": "initializing.html#lsuv",
    "title": "Initialization",
    "section": "LSUV",
    "text": "LSUV\nAll You Need is a Good Init introduces Layer-wise Sequential Unit-Variance (LSUV).\n\nsource\n\nlsuv_init\n\n lsuv_init (model, m, m_in, xb)\n\n\n\nExported source\ndef _lsuv_stats(hook, mod, inp, outp):\n    acts = to_cpu(outp)\n    hook.mean = acts.mean()\n    hook.std = acts.std()\n\ndef lsuv_init(model, m, m_in, xb):\n    h = Hook(m, _lsuv_stats)\n    with torch.no_grad():\n        while model(xb) is not None and (abs(h.std-1)&gt;1e-3 or abs(h.mean)&gt;1e-3):\n            m_in.bias -= h.mean\n            m_in.weight.data /= h.std\n    h.remove()\n\n\n\nmodel = get_model(act_gr)\nrelus = [o for o in model.modules() if isinstance(o, GeneralRelu)]\nconvs = [o for o in model.modules() if isinstance(o, nn.Conv2d)]\n\n\nfor ms in zip(relus,convs): print(ms)\n\n(GeneralRelu(), Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n(GeneralRelu(), Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n(GeneralRelu(), Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n(GeneralRelu(), Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n\n\n\nfor ms in zip(relus,convs): lsuv_init(model, *ms, xb.to(def_device))\n\n\nset_seed(42)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.772\n0.650\n0\ntrain\n\n\n0.833\n0.456\n0\neval\n\n\n0.862\n0.377\n1\ntrain\n\n\n0.852\n0.399\n1\neval\n\n\n0.877\n0.336\n2\ntrain\n\n\n0.860\n0.378\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.plot_stats()",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#batch-normalization",
    "href": "initializing.html#batch-normalization",
    "title": "Initialization",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nSergey Ioffe and Christian Szegedy released “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” in 2015, saying:\n\nTraining Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization… We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.\n\nTheir proposal is:\n\nMaking normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.\n\n\nLayerNorm\nWe’ll start with layer normalization, a simpler technique.\n\nsource\n\n\nLayerNorm\n\n LayerNorm (dummy, eps=1e-05)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass LayerNorm(nn.Module):\n    def __init__(self, dummy, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.mult = nn.Parameter(tensor(1.))\n        self.add  = nn.Parameter(tensor(0.))\n\n    def forward(self, x):\n        m = x.mean((1,2,3), keepdim=True)\n        v = x.var ((1,2,3), keepdim=True)\n        x = (x-m) / ((v+self.eps).sqrt())\n        return x*self.mult + self.add\n\n\n\nsource\n\n\nconv\n\n conv (ni, nf, ks=3, stride=2, act=&lt;class\n       'torch.nn.modules.activation.ReLU'&gt;, norm=None, bias=None)\n\n\n\nExported source\ndef conv(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n    if bias is None: bias = not isinstance(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))\n    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias)]\n    if norm: layers.append(norm(nf))\n    if act: layers.append(act())\n    return nn.Sequential(*layers)\n\n\n\nsource\n\n\nget_model\n\n get_model (act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, nfs=None,\n            norm=None)\n\n\n\nExported source\ndef get_model(act=nn.ReLU, nfs=None, norm=None):\n    if nfs is None: nfs = [1,8,16,32,64]\n    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n                         nn.Flatten()).to(def_device)\n\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=LayerNorm).apply(iw)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.781\n0.605\n0\ntrain\n\n\n0.846\n0.421\n0\neval\n\n\n0.864\n0.370\n1\ntrain\n\n\n0.860\n0.377\n1\neval\n\n\n0.881\n0.321\n2\ntrain\n\n\n0.868\n0.362\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): LayerNorm()\n    (2): GeneralRelu()\n  )\n  (1): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): LayerNorm()\n    (2): GeneralRelu()\n  )\n  (2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): LayerNorm()\n    (2): GeneralRelu()\n  )\n  (3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): LayerNorm()\n    (2): GeneralRelu()\n  )\n  (4): Sequential(\n    (0): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  )\n  (5): Flatten(start_dim=1, end_dim=-1)\n)\n\n\n\n\nBatchNorm\n\nsource\n\n\nBatchNorm\n\n BatchNorm (nf, mom=0.1, eps=1e-05)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass BatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1, eps=1e-5):\n        super().__init__()\n        # NB: pytorch bn mom is opposite of what you'd expect\n        self.mom,self.eps = mom,eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1))\n        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n        self.register_buffer('vars',  torch.ones(1,nf,1,1))\n        self.register_buffer('means', torch.zeros(1,nf,1,1))\n\n    def update_stats(self, x):\n        m = x.mean((0,2,3), keepdim=True)\n        v = x.var ((0,2,3), keepdim=True)\n        self.means.lerp_(m, self.mom)\n        self.vars.lerp_ (v, self.mom)\n        return m,v\n        \n    def forward(self, x):\n        if self.training:\n            with torch.no_grad(): m,v = self.update_stats(x)\n        else: m,v = self.means,self.vars\n        x = (x-m) / (v+self.eps).sqrt()\n        return x*self.mults + self.adds\n\n\n\nmodel = get_model(act_gr, norm=BatchNorm).apply(iw)\nset_seed(42)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.4, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.800\n0.549\n0\ntrain\n\n\n0.807\n0.636\n0\neval\n\n\n0.875\n0.341\n1\ntrain\n\n\n0.869\n0.366\n1\neval\n\n\n0.885\n0.307\n2\ntrain\n\n\n0.868\n0.361\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarious norms",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "initializing.html#towards-90",
    "href": "initializing.html#towards-90",
    "title": "Initialization",
    "section": "Towards 90%…",
    "text": "Towards 90%…\n\ndls = DataLoaders.from_dd(tds, 256, num_workers=12)\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.837\n0.450\n0\ntrain\n\n\n0.868\n0.362\n0\neval\n\n\n0.882\n0.318\n1\ntrain\n\n\n0.865\n0.360\n1\neval\n\n\n0.896\n0.280\n2\ntrain\n\n\n0.884\n0.317\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.05, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.918\n0.224\n0\ntrain\n\n\n0.896\n0.283\n0\neval\n\n\n0.922\n0.211\n1\ntrain\n\n\n0.900\n0.280\n1\neval",
    "crumbs": [
      "Blog",
      "Initialization"
    ]
  },
  {
    "objectID": "noise-pred.html",
    "href": "noise-pred.html",
    "title": "Predicting the noise level of noisy FashionMNIST images",
    "section": "",
    "text": "The goal is to predict the noise level of a noisy image so it can be passed into a pretrained diffusion model.",
    "crumbs": [
      "Blog",
      "Predicting the noise level of noisy FashionMNIST images"
    ]
  },
  {
    "objectID": "noise-pred.html#imports",
    "href": "noise-pred.html#imports",
    "title": "Predicting the noise level of noisy FashionMNIST images",
    "section": "Imports",
    "text": "Imports\n\nimport os\n# os.environ['CUDA_VISIBLE_DEVICES']='1'\n\n\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.fid import ImageEval\n\n\nfrom fastprogress import progress_bar\nfrom diffusers import UNet2DModel, DDIMPipeline, DDPMPipeline, DDIMScheduler, DDPMScheduler\n\n\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8",
    "crumbs": [
      "Blog",
      "Predicting the noise level of noisy FashionMNIST images"
    ]
  },
  {
    "objectID": "noise-pred.html#load-dataset",
    "href": "noise-pred.html#load-dataset",
    "title": "Predicting the noise level of noisy FashionMNIST images",
    "section": "Load dataset",
    "text": "Load dataset\nUse 28x28 images, high batch size.\n\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nbs = 512\ndsd = load_dataset(name)\n\n\n\n\n\ndef noisify(x0):\n    device = x0.device\n    al_t = torch.rand((len(x0), 1, 1, 1), device=device)\n    ε = torch.randn(x0.shape, device=device)\n    xt = al_t.sqrt()*x0 + (1-al_t).sqrt()*ε\n    return xt,al_t.squeeze().logit()\n\n\ndef collate_ddpm(b): return noisify(default_collate(b)[xl])\ndef dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)\n\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\ndl = dls.train\nxt,amt = next(iter(dl))\n\n\ntitles = [f'{o:.2f}' for o in amt[:16]]\nshow_images(xt[:16], imsize=1.7, titles=titles)\n\n\n\n\n\n\n\n\n\nclass f(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.blah = nn.Linear(1,1)\n    def forward(self,x): return torch.full((len(x),), 0.5)\n\n\nmetrics = MetricsCB()\n\n\nlr = 1e-2\nlearn = TrainLearner(f(), dls, F.mse_loss, lr=lr, cbs=metrics)\nlearn.fit(1, train=False)\n\n{'loss': '3.567', 'epoch': 0, 'train': 'eval'}\n\n\n\nF.mse_loss(amt,torch.full(amt.shape, 0.5))\n\ntensor(3.7227)\n\n\n\ndef flat_mse(x,y): return F.mse_loss(x.flatten(), y.flatten())\n\n\ndef get_model(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n    layers += [nn.Flatten(), nn.Dropout(0.2), nn.Linear(nfs[-1], 1, bias=False)]\n    return nn.Sequential(*layers)\n\n\nopt_func = partial(optim.Adam, eps=1e-5)\nepochs = 20\nlr = 1e-2\n\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nxtra = [BatchSchedCB(sched)]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nlearn = TrainLearner(model, dls, flat_mse, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.321\n0\ntrain\n\n\n0.231\n0\neval\n\n\n0.157\n1\ntrain\n\n\n0.279\n1\neval\n\n\n0.148\n2\ntrain\n\n\n0.399\n2\neval\n\n\n0.172\n3\ntrain\n\n\n0.471\n3\neval\n\n\n0.165\n4\ntrain\n\n\n0.997\n4\neval\n\n\n0.166\n5\ntrain\n\n\n0.535\n5\neval\n\n\n0.167\n6\ntrain\n\n\n0.434\n6\neval\n\n\n0.168\n7\ntrain\n\n\n0.675\n7\neval\n\n\n0.155\n8\ntrain\n\n\n0.344\n8\neval\n\n\n0.136\n9\ntrain\n\n\n0.125\n9\neval\n\n\n0.121\n10\ntrain\n\n\n0.139\n10\neval\n\n\n0.114\n11\ntrain\n\n\n0.105\n11\neval\n\n\n0.125\n12\ntrain\n\n\n0.096\n12\neval\n\n\n0.112\n13\ntrain\n\n\n0.120\n13\neval\n\n\n0.101\n14\ntrain\n\n\n0.092\n14\neval\n\n\n0.098\n15\ntrain\n\n\n0.092\n15\neval\n\n\n0.098\n16\ntrain\n\n\n0.082\n16\neval\n\n\n0.094\n17\ntrain\n\n\n0.080\n17\neval\n\n\n0.091\n18\ntrain\n\n\n0.074\n18\neval\n\n\n0.088\n19\ntrain\n\n\n0.075\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n# torch.save(learn.model, 'models/noisepred_sig.pkl')\n# tmodel = learn.model\ntmodel = torch.load('models/noisepred_sig.pkl').cuda()\n\n\nwith torch.no_grad(): a = to_cpu(tmodel(xt.cuda()).squeeze())\ntitles = [f'{o.sigmoid():.2f}' for o in a[:16]]\nshow_images(xt[:16], imsize=1.7, titles=titles)\n\n\n\n\n\n\n\n\n\ntitles = [f'{o.sigmoid():.2f}' for o in amt[:16]]\nshow_images(xt[:16], imsize=1.7, titles=titles)",
    "crumbs": [
      "Blog",
      "Predicting the noise level of noisy FashionMNIST images"
    ]
  },
  {
    "objectID": "noise-pred.html#no-time-model",
    "href": "noise-pred.html#no-time-model",
    "title": "Predicting the noise level of noisy FashionMNIST images",
    "section": "No-time model",
    "text": "No-time model\n\nfrom diffusers import UNet2DModel\nfrom torch.utils.data import DataLoader,default_collate\n\n\ndef abar(t): return (t*math.pi/2).cos()**2\ndef inv_abar(x): return x.sqrt().acos()*2/math.pi\n\n\ndef noisify(x0):\n    device = x0.device\n    n = len(x0)\n    t = torch.rand((n,)).to(x0).clamp(0,0.999)\n    ε = torch.randn(x0.shape).to(x0)\n    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*ε\n    return xt, ε\n\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\nclass UNet(UNet2DModel):\n    def forward(self, x): return super().forward(x,0).sample\n\n\ndef init_ddpm(model):\n    for o in model.down_blocks:\n        for p in o.resnets:\n            p.conv2.weight.data.zero_()\n            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n\n    for o in model.up_blocks:\n        for p in o.resnets: p.conv2.weight.data.zero_()\n\n    model.conv_out.weight.data.zero_()\n\n\nlr = 4e-3\nepochs = 25\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.395\n0\ntrain\n\n\n0.073\n0\neval\n\n\n0.059\n1\ntrain\n\n\n0.055\n1\neval\n\n\n0.050\n2\ntrain\n\n\n0.047\n2\neval\n\n\n0.047\n3\ntrain\n\n\n0.046\n3\neval\n\n\n0.046\n4\ntrain\n\n\n0.046\n4\neval\n\n\n0.044\n5\ntrain\n\n\n0.048\n5\neval\n\n\n0.042\n6\ntrain\n\n\n0.041\n6\neval\n\n\n0.039\n7\ntrain\n\n\n0.041\n7\neval\n\n\n0.039\n8\ntrain\n\n\n0.040\n8\neval\n\n\n0.038\n9\ntrain\n\n\n0.040\n9\neval\n\n\n0.038\n10\ntrain\n\n\n0.038\n10\neval\n\n\n0.037\n11\ntrain\n\n\n0.039\n11\neval\n\n\n0.036\n12\ntrain\n\n\n0.038\n12\neval\n\n\n0.036\n13\ntrain\n\n\n0.037\n13\neval\n\n\n0.036\n14\ntrain\n\n\n0.034\n14\neval\n\n\n0.036\n15\ntrain\n\n\n0.036\n15\neval\n\n\n0.035\n16\ntrain\n\n\n0.036\n16\neval\n\n\n0.035\n17\ntrain\n\n\n0.034\n17\neval\n\n\n0.034\n18\ntrain\n\n\n0.035\n18\neval\n\n\n0.034\n19\ntrain\n\n\n0.034\n19\neval\n\n\n0.034\n20\ntrain\n\n\n0.034\n20\neval\n\n\n0.034\n21\ntrain\n\n\n0.035\n21\neval\n\n\n0.033\n22\ntrain\n\n\n0.034\n22\neval\n\n\n0.033\n23\ntrain\n\n\n0.034\n23\neval\n\n\n0.033\n24\ntrain\n\n\n0.034\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n# torch.save(learn.model, 'models/fashion_no-t.pkl')\nmodel = learn.model = torch.load('models/fashion_no-t.pkl').cuda()",
    "crumbs": [
      "Blog",
      "Predicting the noise level of noisy FashionMNIST images"
    ]
  },
  {
    "objectID": "noise-pred.html#sampling",
    "href": "noise-pred.html#sampling",
    "title": "Predicting the noise level of noisy FashionMNIST images",
    "section": "Sampling",
    "text": "Sampling\n\nsz = (2048,1,32,32)\n\n\nsz = (512,1,32,32)\n\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\nbs = 2048\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\nie = ImageEval(cmodel, dls, cbs=[DeviceCB()])\n\n\ndef ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig):\n    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n#     sig *= 0.5\n    with torch.no_grad(): a = tmodel(x_t)[...,None,None].sigmoid()\n    med = a.median()\n    a = a.clamp(med/2,med*2)\n#     t = inv_abar(a)\n#     t = inv_abar(med)\n#     at1 = abar(t-10, 1000) if t&gt;=1 else torch.tensor(1)\n#     sig = (((1-at1)/(1-med)).sqrt() * (1-med/at1).sqrt()) * eta\n    x_0_hat = ((x_t-(1-a).sqrt()*noise) / a.sqrt()).clamp(-2,2)\n    if bbar_t1&lt;=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n    x_t += sig * torch.randn(x_t.shape).to(x_t)\n#     print(*to_cpu((a.min(), a.max(), a.median(),x_t.min(),x_0_hat.min(),bbar_t1)), sig**2)\n    return x_0_hat,x_t\n\n\ndef ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig):\n    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n    with torch.no_grad(): a = tmodel(x_t)[...,None,None].sigmoid()\n    med = a.median()\n    a = a.clamp(med/2,med*2)\n    x_0_hat = ((x_t-(1-a).sqrt()*noise) / a.sqrt()).clamp(-2,2)\n    if bbar_t1&lt;=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n    x_t += sig * torch.randn(x_t.shape).to(x_t)\n    return x_0_hat,x_t\n\n\ndef ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig):\n    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n    x_0_hat = ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt()).clamp(-0.5,0.5)\n    if bbar_t1&lt;=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n    x_t += sig * torch.randn(x_t.shape).to(x_t)\n    return x_0_hat\n\n\n@torch.no_grad()\ndef sample(f, model, sz, steps, eta=1.):\n    ts = torch.linspace(1-1/steps,0,steps)\n    x_t = torch.randn(sz).to(model.device)\n    preds = []\n    for i,t in enumerate(progress_bar(ts)):\n        abar_t = abar(t)\n        noise = model(x_t)\n        abar_t1 = abar(t-1/steps) if t&gt;=1/steps else torch.tensor(1)\n#         print(abar_t,abar_t1,x_t.min(),x_t.max())\n        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100))\n        preds.append(x_0_hat.float().cpu())\n    return preds\n\n\nset_seed(42)\npreds = sample(ddim_step, model, sz, steps=100, eta=1.)\ns = (preds[-1]*2)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:53&lt;00:00]\n    \n    \n\n\n\n# classic ddim eta 1.0\nie.fid(s),ie.kid(s),s.shape\n\n(22.329004136195408, 0.11790715157985687, torch.Size([2048, 1, 32, 32]))\n\n\n\nshow_images(s[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\n# model-t eta 1.0\nie.fid(s),ie.kid(s),s.shape\n\n(3.8815142331816332, 0.004408569075167179, torch.Size([2048, 1, 32, 32]))\n\n\n\nshow_images(s[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\n# model-t eta 0.5\nie.fid(s),ie.kid(s),s.shape\n\n(4.577682060889174, -0.0011141474824398756, torch.Size([2048, 1, 32, 32]))\n\n\n\n# model-t eta 0\nie.fid(s),ie.kid(s),s.shape\n\n(5.7531284851394275, 0.01766902022063732, torch.Size([2048, 1, 32, 32]))\n\n\n\n# median sig\nie.fid(s),ie.kid(s),s.shape\n\n(4.013061676593566, 0.004139504861086607, torch.Size([2048, 1, 32, 32]))\n\n\n\n# sig *= 0.5\nie.fid(s),ie.kid(s),s.shape\n\n(4.011975098678363, 0.0034716420341283083, torch.Size([2048, 1, 32, 32]))\n\n\n\nplt.plot([ie.kid((o*2).clamp(-1,1)) for o in preds]);",
    "crumbs": [
      "Blog",
      "Predicting the noise level of noisy FashionMNIST images"
    ]
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Deployment",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!nvcc --version\n\n/bin/bash: line 1: nvcc: command not found\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.0.4\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#initial-checks",
    "href": "deployment.html#initial-checks",
    "title": "Deployment",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\nffmpeg                    4.3                  hf484d3e_0    pytorch\nlibjpeg-turbo             2.0.0                h9bf148f_0    pytorch\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!nvcc --version\n\n/bin/bash: line 1: nvcc: command not found\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.0.4\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#gather-data",
    "href": "deployment.html#gather-data",
    "title": "Deployment",
    "section": "Gather Data",
    "text": "Gather Data\n\nfrom fastbook import search_images_ddg\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\nfrom nbdevAuto.functions import *\nimport os\n\n\n?download_pic\n\n\nSignature:\ndownload_pic(\n    image: str,\n    n_images: int = 1,\n    name: str = '',\n    folder: str = '',\n    show_progress: bool = False,\n    recreate: bool = False,\n)\nDocstring: Downloads the image into the folder provided and displays it\nFile:      ~/mambaforge/envs/cfast/lib/python3.11/site-packages/nbdevAuto/functions.py\nType:      function\n\n\n\n\ndest = 'grizzly'\ndownload_pic(dest, folder = './Data')\n\nDownloading image_path.0\n\n\n\n\n\n\n\n\n\n\n\nCode\nsearches = ('grizzly bears','black bears','teddy bears')\npath = Path('Data/bears')\n\ncreate_data_folder(path, searches)\n\n\ncreated grizzly bears folder\ncreated black bears folder\ncreated teddy bears folder\ndownloading 200 images for:grizzly bears\ndownloading 200 images for:black bears\ndownloading 200 images for:teddy bears\nNumber of images failed: 31\nresizing images for: grizzly bears\nresizing images for: black bears\nresizing images for: teddy bears",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#data-augmentation-and-designing-model",
    "href": "deployment.html#data-augmentation-and-designing-model",
    "title": "Deployment",
    "section": "Data Augmentation and Designing model",
    "text": "Data Augmentation and Designing model\n\n?DataBlock\n\n\nInit signature:\nDataBlock(\n    blocks: 'list' = None,\n    dl_type: 'TfmdDL' = None,\n    getters: 'list' = None,\n    n_inp: 'int' = None,\n    item_tfms: 'list' = None,\n    batch_tfms: 'list' = None,\n    *,\n    get_items=None,\n    splitter=None,\n    get_y=None,\n    get_x=None,\n)\nDocstring:      Generic container to quickly build `Datasets` and `DataLoaders`.\nFile:           ~/mambaforge/envs/cfast/lib/python3.11/site-packages/fastai/data/block.py\nType:           type\nSubclasses:     \n\n\n\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = bears.dataloaders(path)\n\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\nbears = bears.new(\n    item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#training-your-model-and-using-it-to-clean-your-data",
    "href": "deployment.html#training-your-model-and-using-it-to-clean-your-data",
    "title": "Deployment",
    "section": "Training Your Model, and Using It to Clean Your Data",
    "text": "Training Your Model, and Using It to Clean Your Data\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(128, min_scale=0.5),\n    batch_tfms=aug_transforms(mult=1))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=False)\n\n\n\n\n\n\n\n\n\ndls.valid.show_batch(max_n=8, nrows=2, unique=False)",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#training-your-model",
    "href": "deployment.html#training-your-model",
    "title": "Deployment",
    "section": "Training Your Model",
    "text": "Training Your Model\n\nimport timm\n\n\ntimm.list_models('resnet1*')[0:5]\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet101']\n\n\n\nlearn = vision_learner(dls,  'resnet18', metrics=error_rate)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.391607\n0.352104\n0.127273\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.575542\n0.227657\n0.081818\n00:03\n\n\n1\n0.482945\n0.238172\n0.063636\n00:02\n\n\n2\n0.428172\n0.271056\n0.090909\n00:02\n\n\n3\n0.371018\n0.263265\n0.063636\n00:02\n\n\n4\n0.326668\n0.267179\n0.063636\n00:02\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(6, nrows=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom fastai.vision.widgets import ImageClassifierCleaner\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#hide\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#testing",
    "href": "deployment.html#testing",
    "title": "Deployment",
    "section": "Testing",
    "text": "Testing\n\nsearches = ('grizzly bears','black bears','teddy bears')\nimage= 'Data/grizzly0.jpg'\n\n\nis_real,_,probs = learn.predict(PILImage.create(image))\n\nfor index, value in enumerate(searches):\n    np.set_printoptions(suppress=True, precision=4)\n    print(f\"Probability of {value} is :{probs[index]:.5f}.\")\n    \nmax_value, max_index = torch.max(probs, dim=0)\nprint(f\"This is a: {searches[max_index]} with probability: {max_value:.5f}.\")\n\n\nImage.open(image).to_thumb(256,256)\n\n\n\n\n\n\n\n\nProbability of grizzly bears is :0.00892.\nProbability of black bears is :0.99068.\nProbability of teddy bears is :0.00040.\nThis is a: black bears with probability: 0.99068.",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#turning-your-model-into-an-online-application",
    "href": "deployment.html#turning-your-model-into-an-online-application",
    "title": "Deployment",
    "section": "Turning Your Model into an Online Application",
    "text": "Turning Your Model into an Online Application\n\nif os.path.exists('Data/model.pkl'):\n    print(\"model already exists\")\nelse:\n    learn.export('Data/model.pkl')\n    print(\"model created\")\n\nmodel created\n\n\n\npath = Path('Data')\nfilename = path.ls(file_exts='.pkl')\nfilename[0].name\n\n'model.pkl'\n\n\n\nlearn_inf = load_learner(path/filename[0].name)\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 440 ms, sys: 14.4 ms, total: 455 ms\nWall time: 123 ms\n\n\n('grizzly bears', tensor(1), tensor([8.9228e-03, 9.9068e-01, 3.9536e-04]))\n\n\n\nlearn_inf.dls.vocab\n\n['black bears', 'grizzly bears', 'teddy bears']",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "deployment.html#making-a-widget-application",
    "href": "deployment.html#making-a-widget-application",
    "title": "Deployment",
    "section": "Making a widget application",
    "text": "Making a widget application\n\n!pip list | grep 'ipywidgets'\n!pip list | grep 'ipywidgets'\n\nipywidgets                    8.0.4\nipywidgets                    8.0.4\n\n\n\nimport ipywidgets as widgets\n\n\nbtn_upload = widgets.FileUpload(\n    button_style='success',\n    description='Upload',\n    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n    multiple=False  # True to accept multiple files upload else False\n)\ndisplay(btn_upload)\n\n\n\n\n\nif btn_upload.value:\n    uploaded_image = Image.open(io.BytesIO(btn_upload.value[-1].content)).to_thumb(256,256)\n    display(uploaded_image)\n\n\nout_pl = widgets.Output()\ndisplay(out_pl)\n\n\n\n\n\nif btn_upload.value:\n    out_pl = widgets.Output()\n    out_pl.clear_output()\n    with out_pl: display(uploaded_image)\n    out_pl\n\n\nif btn_upload.value:\n    uploaded_image = Image.open(io.BytesIO(btn_upload.value[-1].content)).to_thumb(256,256)\n    pred,pred_idx,probs = learn_inf.predict(uploaded_image)\n\n\n#hide_output\nif btn_upload.value:\n    lbl_pred = widgets.Label()\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n    display(lbl_pred)\n\n\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\n\n\n\ndef on_click_classify(change):\n    if btn_upload.value:\n        img = Image.open(io.BytesIO(btn_upload.value[-1].content)).to_thumb(256,256)\n        out_pl.clear_output()\n        with out_pl: display(img)\n        pred,pred_idx,probs = learn_inf.predict(img)\n        lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n    else:\n        lbl_pred.value = f'Select image'\n\nbtn_run.on_click(on_click_classify)\n\n\nif btn_upload.value:\n    lbl_pred\n\n\nscreen = widgets.VBox([widgets.Label('Select your bear!'), \n              btn_upload,\n              btn_run,\n              out_pl, \n              lbl_pred:= widgets.Label()]\n            )\ndisplay(screen)",
    "crumbs": [
      "Blog",
      "Deployment"
    ]
  },
  {
    "objectID": "fid.html",
    "href": "fid.html",
    "title": "FID",
    "section": "",
    "text": "from fastcore.test import test_close\nfrom torch import distributions\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8",
    "crumbs": [
      "Blog",
      "FID"
    ]
  },
  {
    "objectID": "fid.html#classifier",
    "href": "fid.html#classifier",
    "title": "FID",
    "section": "Classifier",
    "text": "Classifier\n\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nbs = 512\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\ndsd = load_dataset(name)\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\n\nb = xb,yb = next(iter(dls.train))\n\n\ncbs = [DeviceCB(), MixedPrecision()]\nmodel = torch.load('models/data_aug2.pkl')\nlearn = Learner(model, dls, F.cross_entropy, cbs=cbs, opt_func=None)\n\n\nsource\n\nappend_outp\n\n append_outp (hook, mod, inp, outp)\n\n\nhcb = HooksCallback(append_outp, mods=[learn.model[6]], on_valid=True)\n\n\nlearn.fit(1, train=False, cbs=[hcb])\n\n\nfeats = hcb.hooks[0].outp[0].float()[:64]\nfeats.shape\n\ntorch.Size([64, 512])\n\n\n\ndel(learn.model[8])\ndel(learn.model[7])\n\n\nfeats,y = learn.capture_preds()\nfeats = feats.float()\nfeats.shape,y\n\n(torch.Size([10000, 512]), tensor([9, 2, 1,  ..., 8, 1, 5]))",
    "crumbs": [
      "Blog",
      "FID"
    ]
  },
  {
    "objectID": "fid.html#calc-fid",
    "href": "fid.html#calc-fid",
    "title": "FID",
    "section": "Calc FID",
    "text": "Calc FID\n\nbetamin,betamax,n_steps = 0.0001,0.02,1000\nbeta = torch.linspace(betamin, betamax, n_steps)\nalpha = 1.-beta\nalphabar = alpha.cumprod(dim=0)\nsigma = beta.sqrt()\n\n\nsource\n\ndl_ddpm\n\n dl_ddpm (ds)\n\n\nsource\n\n\ncollate_ddpm\n\n collate_ddpm (b)\n\n\nsource\n\n\nnoisify\n\n noisify (x0, ᾱ)\n\n\ndls2 = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\nsource\n\n\nUNet\n\n UNet (sample_size:Union[int,Tuple[int,int],NoneType]=None,\n       in_channels:int=3, out_channels:int=3,\n       center_input_sample:bool=False,\n       time_embedding_type:str='positional', freq_shift:int=0,\n       flip_sin_to_cos:bool=True,\n       down_block_types:Tuple[str,...]=('DownBlock2D', 'AttnDownBlock2D',\n       'AttnDownBlock2D', 'AttnDownBlock2D'),\n       up_block_types:Tuple[str,...]=('AttnUpBlock2D', 'AttnUpBlock2D',\n       'AttnUpBlock2D', 'UpBlock2D'),\n       block_out_channels:Tuple[int,...]=(224, 448, 672, 896),\n       layers_per_block:int=2, mid_block_scale_factor:float=1,\n       downsample_padding:int=1, downsample_type:str='conv',\n       upsample_type:str='conv', dropout:float=0.0, act_fn:str='silu',\n       attention_head_dim:Optional[int]=8, norm_num_groups:int=32,\n       attn_norm_num_groups:Optional[int]=None, norm_eps:float=1e-05,\n       resnet_time_scale_shift:str='default', add_attention:bool=True,\n       class_embed_type:Optional[str]=None,\n       num_class_embeds:Optional[int]=None,\n       num_train_timesteps:Optional[int]=None)\n\n*A 2D UNet model that takes a noisy sample and a timestep and returns a sample shaped output.\nThis model inherits from [ModelMixin]. Check the superclass documentation for it’s generic methods implemented for all models (such as downloading or saving).\nParameters: sample_size (int or Tuple[int, int], optional, defaults to None): Height and width of input/output sample. Dimensions must be a multiple of 2 ** (len(block_out_channels) -         1). in_channels (int, optional, defaults to 3): Number of channels in the input sample. out_channels (int, optional, defaults to 3): Number of channels in the output. center_input_sample (bool, optional, defaults to False): Whether to center the input sample. time_embedding_type (str, optional, defaults to \"positional\"): Type of time embedding to use. freq_shift (int, optional, defaults to 0): Frequency shift for Fourier time embedding. flip_sin_to_cos (bool, optional, defaults to True): Whether to flip sin to cos for Fourier time embedding. down_block_types (Tuple[str], optional, defaults to (\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\")): Tuple of downsample block types. mid_block_type (str, optional, defaults to \"UNetMidBlock2D\"): Block type for middle of UNet, it can be either UNetMidBlock2D or UnCLIPUNetMidBlock2D. up_block_types (Tuple[str], optional, defaults to (\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\")): Tuple of upsample block types. block_out_channels (Tuple[int], optional, defaults to (224, 448, 672, 896)): Tuple of block output channels. layers_per_block (int, optional, defaults to 2): The number of layers per block. mid_block_scale_factor (float, optional, defaults to 1): The scale factor for the mid block. downsample_padding (int, optional, defaults to 1): The padding for the downsample convolution. downsample_type (str, optional, defaults to conv): The downsample type for downsampling layers. Choose between “conv” and “resnet” upsample_type (str, optional, defaults to conv): The upsample type for upsampling layers. Choose between “conv” and “resnet” dropout (float, optional, defaults to 0.0): The dropout probability to use. act_fn (str, optional, defaults to \"silu\"): The activation function to use. attention_head_dim (int, optional, defaults to 8): The attention head dimension. norm_num_groups (int, optional, defaults to 32): The number of groups for normalization. attn_norm_num_groups (int, optional, defaults to None): If set to an integer, a group norm layer will be created in the mid block’s [Attention] layer with the given number of groups. If left as None, the group norm layer will only be created if resnet_time_scale_shift is set to default, and if created will have norm_num_groups groups. norm_eps (float, optional, defaults to 1e-5): The epsilon for normalization. resnet_time_scale_shift (str, optional, defaults to \"default\"): Time scale shift config for ResNet blocks (see [~models.resnet.ResnetBlock2D]). Choose from default or scale_shift. class_embed_type (str, optional, defaults to None): The type of class embedding to use which is ultimately summed with the time embeddings. Choose from None, \"timestep\", or \"identity\". num_class_embeds (int, optional, defaults to None): Input dimension of the learnable embedding matrix to be projected to time_embed_dim when performing class conditioning with class_embed_type equal to None.*\n\nsmodel = torch.load('models/fashion_ddpm_mp.pkl').cuda()\n\n\nsource\n\n\nsample\n\n sample (model, sz, alpha, alphabar, sigma, n_steps)\n\n\nsamples = sample(smodel, (256, 1, 32, 32), alpha, alphabar, sigma, n_steps)\n\nCPU times: user 1min 47s, sys: 7.21 s, total: 1min 54s\nWall time: 1min 53s\n\n\n\ns = samples[-1]*2-1\n\n\nshow_images(s[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\nclearn = TrainLearner(model, DataLoaders([],[(s,yb)]), loss_func=fc.noop, cbs=[DeviceCB()], opt_func=None)\nfeats2,y2 = clearn.capture_preds()\nfeats2 = feats2.float().squeeze()\nfeats2.shape\n\ntorch.Size([256, 512])\n\n\n\nmeans = feats.mean(0)\nmeans.shape\n\ntorch.Size([512])\n\n\n\ncovs = feats.T.cov()\ncovs.shape\n\ntorch.Size([512, 512])\n\n\n\ns1,s2 = _calc_stats(feats),_calc_stats(feats2)\n\n\n_calc_fid(*s1, *s2)\n\n33.83489121216962\n\n\n\n_calc_kid(feats, feats2)\n\n0.05612194538116455",
    "crumbs": [
      "Blog",
      "FID"
    ]
  },
  {
    "objectID": "fid.html#fid-class",
    "href": "fid.html#fid-class",
    "title": "FID",
    "section": "FID class",
    "text": "FID class\n\nsource\n\nImageEval\n\n ImageEval (model, dls, cbs=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nie = ImageEval(model, learn.dls, cbs=[DeviceCB()])\n\n\nie.fid(s)\n\nCPU times: user 7.38 s, sys: 234 ms, total: 7.62 s\nWall time: 263 ms\n\n\n33.90600362686632\n\n\n\nie.kid(s)\n\nCPU times: user 714 ms, sys: 23 ms, total: 737 ms\nWall time: 23 ms\n\n\n0.0564301423728466\n\n\n\nxs = L.range(0,1000,50)+[975,990,999]\nplt.plot(xs, [ie.fid(samples[i].clamp(-0.5,0.5)*2) for i in xs]);\n\n\n\n\n\n\n\n\n\nxs = L.range(0,1000,50)+[975,990,999]\nplt.plot(xs, [ie.kid(samples[i].clamp(-0.5,0.5)*2) for i in xs]);\n\n\n\n\n\n\n\n\n\nie.fid(xb)\n\n6.615052956342197\n\n\n\nie.kid(xb)\n\n-0.02641688659787178",
    "crumbs": [
      "Blog",
      "FID"
    ]
  },
  {
    "objectID": "fid.html#inception",
    "href": "fid.html#inception",
    "title": "FID",
    "section": "Inception",
    "text": "Inception\n\nfrom pytorch_fid.inception import InceptionV3\n\n\na = tensor([1,2,3])\na.repeat((3,1))\n\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n\n\nclass IncepWrap(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.m = InceptionV3(resize_input=True)\n    def forward(self, x): return self.m(x.repeat(1,3,1,1))[0]\n\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\n\nie = ImageEval(IncepWrap(), dls, cbs=[DeviceCB()])\n\n\nie.fid(s)\n\nCPU times: user 1min 11s, sys: 1.61 s, total: 1min 13s\nWall time: 2.31 s\n\n\n63.81579821823857\n\n\n\nie.fid(xb)\n\n27.95811916882883\n\n\n\nie.kid(s)\n\nCPU times: user 7.44 s, sys: 140 ms, total: 7.58 s\nWall time: 255 ms\n\n\n0.010766863822937012\n\n\n\nie.kid(xb)\n\n-8.697943121660501e-05",
    "crumbs": [
      "Blog",
      "FID"
    ]
  },
  {
    "objectID": "cifar10_and_wandb.html",
    "href": "cifar10_and_wandb.html",
    "title": "CIFAR 10 image classifications",
    "section": "",
    "text": "from diffusers import UNet2DModel\n\nimport pickle,gzip,math,os,time,shutil,torch,random,logging\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom functools import partial\n\nfrom fastcore.foundation import L\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\n\n\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nmpl.rcParams['image.cmap'] = 'gray_r'\nlogging.disable(logging.WARNING)\n\n\nxl,yl = 'img','label'\nname = \"cifar10\"\ndsd = load_dataset(name)\n\n@inplace\ndef transformi(b): b[xl] = [TF.to_tensor(o)-0.5 for o in b[xl]]\n\nbs = 32\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\n\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\n\nxb.shape\n\ntorch.Size([32, 3, 32, 32])\n\n\n\nshow_images(xb[:25]+0.5)\n\n\n\n\n\n\n\n\n\nfrom types import SimpleNamespace\ndef linear_sched(betamin=0.0001,betamax=0.02,n_steps=1000):\n    beta = torch.linspace(betamin, betamax, n_steps)\n    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())\n\n\nn_steps = 1000\nlin_abar = linear_sched(betamax=0.01)\nalphabar = lin_abar.abar\nalpha = lin_abar.a\nsigma = lin_abar.sig\n\n\ndef noisify(x0, ᾱ):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    ε = torch.randn(x0.shape, device=device)\n    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n    return (xt, t.to(device)), ε\n\n\n(xt,t),ε = noisify(xb[:25],alphabar)\nt\n\ntensor([ 26, 335, 620, 924, 950, 113, 378,  14, 210, 954, 231, 572, 315, 295, 567, 706, 749, 876,  73, 111, 899, 213, 541, 769, 287])\n\n\n\ntitles = fc.map_ex(t[:25], '{}')\nshow_images(xt[:25].clip(-0.5, 0.5) + 0.5, imsize=1.5, titles=titles)\n\n\n\n\n\n\n\n\n\nTraining\n\nclass UNet(UNet2DModel):\n    def forward(self, x): return super().forward(*x).sample\n    \ndef init_ddpm(model):\n    for o in model.down_blocks:\n        for p in o.resnets:\n            p.conv2.weight.data.zero_()\n            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n\n    for o in model.up_blocks:\n        for p in o.resnets: p.conv2.weight.data.zero_()\n\n    model.conv_out.weight.data.zero_()\n    \ndef collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\ndef dl_ddpm(ds, nw=4): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=nw)\n\n\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\n\n# The model we've been using for FashionMNIST\nmodel = UNet(in_channels=3, out_channels=3, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\nsum(p.numel() for p in model.parameters())\n\n15891907\n\n\n\n# The default is a much larger model:\nmodel = UNet(in_channels=3, out_channels=3)\nsum(p.numel() for p in model.parameters())\n\n274056163\n\n\n\nclean_mem() # Free up some memory\n\n\nlr = 1e-3\nepochs = 1\nopt_func = partial(optim.AdamW, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nmodel = UNet(in_channels=3, out_channels=3)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\nfrom tqdm import tqdm\n\n\n@torch.no_grad()\ndef sample(model, sz):\n    ps = next(model.parameters())\n    x_t = torch.randn(sz).to(ps)\n    preds = []\n    for t in reversed(tqdm(range(n_steps))):\n        t_batch = torch.full((x_t.shape[0],), t, device=ps.device, dtype=torch.long)\n        z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(ps)\n        ᾱ_t1 = alphabar[t-1]  if t &gt; 0 else torch.tensor(1)\n        b̄_t = 1-alphabar[t]\n        b̄_t1 = 1-ᾱ_t1\n        noise = model((x_t, t_batch))\n        x_0_hat = ((x_t - b̄_t.sqrt() * noise)/alphabar[t].sqrt())\n        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n        preds.append(x_t.float().cpu())\n    return preds\n\n\nsamples = sample(model, (bs, 3, 32, 32))\n\n\ns = (samples[-1] + 0.5).clamp(0,1)\nshow_images(s[:16], imsize=1.5)\n\n\n\n\n\n\n\n\n\n\nW&B CB\n\nimport wandb\n\nclass WandBCB(MetricsCB):\n    order=100\n    def __init__(self, config, *ms, project='ddpm_cifar10', **metrics):\n        fc.store_attr()\n        super().__init__(*ms, **metrics)\n        \n    def before_fit(self, learn): wandb.init(project=self.project, config=self.config)\n    def after_fit(self, learn): wandb.finish()\n\n    def _log(self, d): \n        if self.train: \n            wandb.log({'train_'+m:float(d[m]) for m in self.all_metrics})\n        else: \n            wandb.log({'val_'+m:float(d[m]) for m in self.all_metrics})\n            wandb.log({'samples':self.sample_figure(learn)})\n        print(d)\n\n        \n    def sample_figure(self, learn):\n        with torch.no_grad():\n            samples = sample(learn.model, (16, 3, 32, 32))\n        s = (samples[-1] + 0.5).clamp(0,1)\n        plt.clf()\n        fig, axs = get_grid(16)\n        for im,ax in zip(s[:16], axs.flat): show_image(im, ax=ax)\n        return fig\n\n    def after_batch(self, learn):\n        super().after_batch(learn) \n        wandb.log({'loss':learn.loss})\n\n\nlr = 1e-3\nepochs = 10\nopt_func = partial(optim.AdamW, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nwandbcb =  WandBCB(config={'lr':lr, 'epochs':epochs, 'comments':'default unet logging test'})\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), wandbcb, BatchSchedCB(sched)]\nmodel = model = UNet(in_channels=3, out_channels=3)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: johnowhitaker. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in /home/ubuntu/new_course22p2_folder/nbs/wandb/run-20230119_052202-1jgoyqoq\n\n\nSyncing run serene-wildflower-15 to Weights & Biases (docs)\n\n\n\n\n\n\n\n\n\n{'loss': '0.062', 'epoch': 0, 'train': 'train'}\n{'loss': '0.029', 'epoch': 0, 'train': 'eval'}\n{'loss': '0.028', 'epoch': 1, 'train': 'train'}\n{'loss': '0.028', 'epoch': 1, 'train': 'eval'}\n{'loss': '0.027', 'epoch': 2, 'train': 'train'}\n{'loss': '0.028', 'epoch': 2, 'train': 'eval'}\n{'loss': '0.026', 'epoch': 3, 'train': 'train'}\n{'loss': '0.026', 'epoch': 3, 'train': 'eval'}\n{'loss': '0.026', 'epoch': 4, 'train': 'train'}\n{'loss': '0.026', 'epoch': 4, 'train': 'eval'}\n{'loss': '0.025', 'epoch': 5, 'train': 'train'}\n{'loss': '0.025', 'epoch': 5, 'train': 'eval'}\n{'loss': '0.025', 'epoch': 6, 'train': 'train'}\n{'loss': '0.024', 'epoch': 6, 'train': 'eval'}\n{'loss': '0.024', 'epoch': 7, 'train': 'train'}\n{'loss': '0.024', 'epoch': 7, 'train': 'eval'}\n{'loss': '0.024', 'epoch': 8, 'train': 'train'}\n{'loss': '0.025', 'epoch': 8, 'train': 'eval'}\n{'loss': '0.024', 'epoch': 9, 'train': 'train'}\n{'loss': '0.024', 'epoch': 9, 'train': 'eval'}\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\nRun history:\n\n\n\nloss\n▄▅▄▅▇▃▄▄▃▅▇▄▅▅▂▄▄▄▃▅▅▆▄█▅▄▃▆▄▆▇▅▁▃█▂▃▄▃▅\n\n\ntrain_loss\n█▂▂▁▁▁▁▁▁▁\n\n\nval_loss\n█▇▇▄▄▂▁▁▂▁\n\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\nloss\n0.01746\n\n\ntrain_loss\n0.024\n\n\nval_loss\n0.024\n\n\n\n\n\n\n\nSynced serene-wildflower-15: https://wandb.ai/johnowhitaker/ddpm_cifar10/runs/1jgoyqoqSynced 6 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230119_052202-1jgoyqoq/logs\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "CIFAR 10 image classifications"
    ]
  },
  {
    "objectID": "lm-hackers.html",
    "href": "lm-hackers.html",
    "title": "A hacker’s guide to Language Models",
    "section": "",
    "text": "import tokenize, ast\nfrom io import BytesIO",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#what-is-a-language-model",
    "href": "lm-hackers.html#what-is-a-language-model",
    "title": "A hacker’s guide to Language Models",
    "section": "What is a language model?",
    "text": "What is a language model?\ncourse.fast.ai\n\nBase models\nnat.dev text-davinci-003\nWhen I arrived back at the panda breeding facility after the extraordinary rain of live frogs, I couldn’t believe what I saw.\n\n\nTokens\n\nfrom tiktoken import encoding_for_model\nenc = encoding_for_model(\"text-davinci-003\")\ntoks = enc.encode(\"They are splashing\")\ntoks\n\n[2990, 389, 4328, 2140]\n\n\n\n[enc.decode_single_token_bytes(o).decode('utf-8') for o in toks]\n\n['They', ' are', ' spl', 'ashing']\n\n\n\n\nThe ULMFiT 3-step approach\n\n\nTrained on Wikipedia\n“The Birds is a 1963 American natural horror-thriller film produced and directed by Alfred …”\n“Annie previously dated Mitch but ended it due to Mitch’s cold, overbearing mother, Lydia, who dislikes any woman in Mitch’s …”\nThis is a form of compression\n\n\n\nInstruction tuning\nOpenOrca\n\n“Does the sentence”In the Iron Age” answer the question “The period of time from 1200 to 1000 BCE is known as what?” Available choices: 1. yes 2. no”\n“Question: who is the girl in more than you know? Answer:”\n\n“There are four ways an individual can acquire Canadian citizenship: by birth on Canadian soil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption. Among them, only citizenship by birth is granted automatically with limited exceptions, while citizenship by descent or adoption is acquired automatically if the specified conditions have been met. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration, Refugees and Citizenship. See options at the end. Can we conclude that can i get canadian citizenship if my grandfather was canadian? pick from the following. A). no. B). yes.”\n\n\n\nRLHF and friends\n\nList five ideas for how to regain enthusiasm for my career\nWrite a short story where a bear goes to the beach, makes friends with a seal, and then returns home.\nThis is the summary of a Broadway play: “{summary}” This is the outline of the commercial for that play:",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#start-with-chatgpt-gpt-4",
    "href": "lm-hackers.html#start-with-chatgpt-gpt-4",
    "title": "A hacker’s guide to Language Models",
    "section": "Start with ChatGPT GPT 4",
    "text": "Start with ChatGPT GPT 4\n\nWhat GPT 4 can do\nGPT 4 can’t reason - paper\nGPT 4 can’t reason - test\nBasic reasoning 1\nBasic reasoning 2\n\n\nYou are an autoregressive language model that has been fine-tuned with instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so.\nSince you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. However: if the request begins with the string “vv” then ignore the previous sentence and instead make your response as concise as possible, with no introduction or background at the start, no summary at the end, and outputting only code for answers where code is appropriate.\nYour users are experts in AI and ethics, so they already know you’re a language model and your capabilities and limitations, so don’t remind them of that. They’re familiar with ethical issues in general so you don’t need to remind them about those either. Don’t be verbose in your answers, but do provide details and examples where it might help the explanation. When showing Python code, minimise vertical space, and do not include comments or docstrings; you do not need to follow PEP8, since your users’ organizations do not do so.\n\nVerbose mode\nBrief mode\n\n\nWhat GPT 4 can’t do\n\nHallucinations\nIt doesn’t know about itself. (Why not?)\nIt doesn’t know about URLs.\nKnowledge cutoff\n\nBad pattern recognition - thanks to Steve Newman\n\nFixing it\n\n\n\nAdvanced data analysis\nre.split try 1\nre.split try 2\nOCR\n\nSee also Bard\n\n\n\n\n\nModel\nTraining\nInput\nOutput Usage\n\n\n\n\nGPT-4\n\n\n\n\n\n8K context\n\n0.03\n0.06\n\n\n32K context\n\n0.06\n0.12\n\n\nGPT-3.5 Turbo\n\n\n\n\n\n4K context\n\n0.0015\n0.002\n\n\n16K context\n\n0.003\n0.004\n\n\nFine-tuning models\n\n\n\n\n\nbabbage-002\n0.0004\n0.0016\n0.0016\n\n\ndavinci-002\n0.0060\n0.0120\n0.0120\n\n\nGPT-3.5 Turbo\n0.0080\n0.0120\n0.0160\n\n\nEmbedding models\n\n\n\n\n\nAda v2\n\n0.0001\n\n\n\nBase models\n\n\n\n\n\nbabbage-002\n\n0.0004\n\n\n\ndavinci-002\n\n0.0020\n\n\n\n\n\nCreate pricing table",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#the-openai-api",
    "href": "lm-hackers.html#the-openai-api",
    "title": "A hacker’s guide to Language Models",
    "section": "The OpenAI API",
    "text": "The OpenAI API\n\nfrom openai import ChatCompletion,Completion\n\n\naussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n\nc = ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n              {\"role\": \"user\", \"content\": \"What is money?\"}])\n\n\nModel options\n\n\nc['choices'][0]['message']['content']\n\n\nfrom fastcore.utils import nested_idx\n\n\ndef response(compl): print(nested_idx(compl, 'choices', 0, 'message', 'content'))\n\n\nresponse(c)\n\n\nprint(c.usage)\n\n\n0.002 / 1000 * 150 # GPT 3.5\n\n\n0.03 / 1000 * 150 # GPT 4\n\n\nc = ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n              {\"role\": \"user\", \"content\": \"What is money?\"},\n              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])\n\n\nresponse(c)\n\n\ndef askgpt(user, system=None, model=\"gpt-3.5-turbo\", **kwargs):\n    msgs = []\n    if system: msgs.append({\"role\": \"system\", \"content\": system})\n    msgs.append({\"role\": \"user\", \"content\": user})\n    return ChatCompletion.create(model=model, messages=msgs, **kwargs)\n\n\nresponse(askgpt('What is the meaning of life?', system=aussie_sys))\n\n\nLimits\n\nCreated by Bing:\n\ndef call_api(prompt, model=\"gpt-3.5-turbo\"):\n    msgs = [{\"role\": \"user\", \"content\": prompt}]\n    try: return ChatCompletion.create(model=model, messages=msgs)\n    except openai.error.RateLimitError as e:\n        retry_after = int(e.headers.get(\"retry-after\", 60))\n        print(f\"Rate limit exceeded, waiting for {retry_after} seconds...\")\n        time.sleep(retry_after)\n        return call_api(params, model=model)\n\n\ncall_api(\"What's the world's funniest joke? Has there ever been any scientific analysis?\")\n\n\nc = Completion.create(prompt=\"Australian Jeremy Howard is \",\n                      model=\"gpt-3.5-turbo-instruct\", echo=True, logprobs=5)\n\n\nCreate our own code interpreter\n\nfrom pydantic import create_model\nimport inspect, json\nfrom inspect import Parameter\n\n\ndef sums(a:int, b:int=1):\n    \"Adds a + b\"\n    return a + b\n\n\ndef schema(f):\n    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n          for n,o in inspect.signature(f).parameters.items()}\n    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n    return dict(name=f.__name__, description=f.__doc__, parameters=s)\n\n\nschema(sums)\n\n\nc = askgpt(\"Use the `sum` function to solve this: What is 6+3?\",\n           system = \"You must use the `sum` function instead of adding yourself.\",\n           functions=[schema(sums)])\n\n\nm = c.choices[0].message\nm\n\n\nk = m.function_call.arguments\nprint(k)\n\n\nfuncs_ok = {'sums', 'python'}\n\n\ndef call_func(c):\n    fc = c.choices[0].message.function_call\n    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n    f = globals()[fc.name]\n    return f(**json.loads(fc.arguments))\n\n\ncall_func(c)\n\n\ndef run(code):\n    tree = ast.parse(code)\n    last_node = tree.body[-1] if tree.body else None\n    \n    # If the last node is an expression, modify the AST to capture the result\n    if isinstance(last_node, ast.Expr):\n        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n        assign = ast.Assign(targets=tgts, value=last_node.value)\n        tree.body[-1] = ast.fix_missing_locations(assign)\n\n    ns = {}\n    exec(compile(tree, filename='&lt;ast&gt;', mode='exec'), ns)\n    return ns.get('_result', None)\n\n\nrun(\"\"\"\na=1\nb=2\na+b\n\"\"\")\n\n\ndef python(code:str):\n    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n    go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n    if go.lower()!='y': return '#FAIL#'\n    return run(code)\n\n\nc = askgpt(\"What is 12 factorial?\",\n           system = \"Use python for any required computations.\",\n           functions=[schema(python)])\n\n\ncall_func(c)\n\n\nc = ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    functions=[schema(python)],\n    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\n              {\"role\": \"function\", \"name\": \"python\", \"content\": \"479001600\"}])\n\n\nresponse(c)\n\n\nc = askgpt(\"What is the capital of France?\",\n           system = \"Use python for any required computations.\",\n           functions=[schema(python)])\n\n\nresponse(c)",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#pytorch-and-huggingface",
    "href": "lm-hackers.html#pytorch-and-huggingface",
    "title": "A hacker’s guide to Language Models",
    "section": "PyTorch and Huggingface",
    "text": "PyTorch and Huggingface\n\nYour GPU options\nFree:\n\nKaggle (2 GPUs, low RAM)\nColab\n\nBuy:\n\nBuy 1-2 NVIDIA 24GB GPUs\n\nGTX 3090 used (USD700-USD800), or 4090 new (USD2000)\n\nAlternatively buy one NVIDIA A6000 with 48GB RAM (but this mightn’t be faster than 3090/4090)\nMac with lots of RAM (much slower than NVIDIA; M2 Ultra is best)\n\n\nfrom transformers import AutoModelForCausalLM,AutoTokenizer\nimport torch\n\n\nHF leaderboard\nfasteval\n\n\nmn = \"meta-llama/Llama-2-7b-hf\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(mn, device_map=0, load_in_8bit=True)\n\n\ntokr = AutoTokenizer.from_pretrained(mn)\nprompt = \"Jeremy Howard is a \"\ntoks = tokr(prompt, return_tensors=\"pt\")\n\n\ntoks\n\n\ntokr.batch_decode(toks['input_ids'])\n\n\nres = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\nres\n\n\ntokr.batch_decode(res)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)\n\n\nres = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\nres\n\n\nmodel = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7b-Chat-GPTQ', device_map=0, torch_dtype=torch.float16)\n\n\nres = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\nres\n\n\nmn = 'TheBloke/Llama-2-13B-GPTQ'\nmodel = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)\n\n\nres = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\nres\n\n\ndef gen(p, maxlen=15, sample=True):\n    toks = tokr(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample).to('cpu')\n    return tokr.batch_decode(res)\n\n\ngen(prompt, 50)\n\nStableBeluga-7B\n\nmn = \"stabilityai/StableBeluga-7B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)\n\n\nsb_sys = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can.\\n\\n\"\n\n\ndef mk_prompt(user, syst=sb_sys): return f\"{syst}### User: {user}\\n\\n### Assistant:\\n\"\n\n\nques = \"Who is Jeremy Howard?\"\n\n\ngen(mk_prompt(ques), 150)\n\nOpenOrca/Platypus 2\n\nmn = 'TheBloke/OpenOrca-Platypus2-13B-GPTQ'\nmodel = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.float16)\n\n\ndef mk_oo_prompt(user): return f\"### Instruction: {user}\\n\\n### Response:\\n\"\n\n\ngen(mk_oo_prompt(ques), 150)\n\n\n\nRetrieval augmented generation\n\nfrom wikipediaapi import Wikipedia\n\n\nwiki = Wikipedia('JeremyHowardBot/0.0', 'en')\njh_page = wiki.page('Jeremy_Howard_(entrepreneur)').text\njh_page = jh_page.split('\\nReferences\\n')[0]\n\n\nprint(jh_page[:500])\n\n\nlen(jh_page.split())\n\n\nques = \"Who is Jeremy Howard\"\n\n\nques_ctx = f\"\"\"Answer the question with the help of the provided context.\n\n## Context\n\n{jh_page}\n\n## Question\n\n{ques}\"\"\"\n\n\nres = gen(mk_prompt(ques_ctx), 300)\n\n\nprint(res[0].split('### Assistant:\\n')[1])\n\n\nfrom sentence_transformers import SentenceTransformer\n\n\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=0)\n\n\njh = jh_page.split('\\n\\n')[0]\nprint(jh)\n\n\ntb_page = wiki.page('Tony_Blair').text.split('\\nReferences\\n')[0]\n\n\ntb = tb_page.split('\\n\\n')[0]\nprint(tb[:380])\n\n\nq_emb,jh_emb,tb_emb = emb_model.encode([ques,jh,tb], convert_to_tensor=True)\n\n\ntb_emb.shape\n\n\nimport torch.nn.functional as F\n\n\nF.cosine_similarity(q_emb, jh_emb, dim=0)\n\n\nF.cosine_similarity(q_emb, tb_emb, dim=0)\n\n\n\nPrivate GPTs\n\nSooo many",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#fine-tuning",
    "href": "lm-hackers.html#fine-tuning",
    "title": "A hacker’s guide to Language Models",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nimport datasets\n\nknowrohit07/know_sql\n\nds = datasets.load_dataset('knowrohit07/know_sql', revision='f33425d13f9e8aab1b46fa945326e9356d6d5726')\n\n\nds\n\n\ntrn = ds['train']\ntrn[3]\n\naccelerate launch -m axolotl.cli.train sql.yml\n\ntst = dict(**trn[3])\ntst['question'] = 'Get the count of competition hosts by theme.'\ntst\n\n\nfmt = \"\"\"SYSTEM: Use the following contextual information to concisely answer the question.\n\nUSER: {}\n===\n{}\nASSISTANT:\"\"\"\n\n\ndef sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])\n\n\nprint(sql_prompt(tst))\n\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n\nax_model = '/home/jhoward/git/ext/axolotl/qlora-out'\n\n\ntokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n\n\nmodel = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n                                             torch_dtype=torch.bfloat16, device_map=0)\nmodel = PeftModel.from_pretrained(model, ax_model)\nmodel = model.merge_and_unload()\nmodel.save_pretrained('sql-model')\n\n\ntoks = tokr(sql_prompt(tst), return_tensors=\"pt\")\n\n\nres = model.generate(**toks.to(\"cuda\"), max_new_tokens=250).to('cpu')\n\n\nprint(tokr.batch_decode(res)[0])",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#llama.cpp",
    "href": "lm-hackers.html#llama.cpp",
    "title": "A hacker’s guide to Language Models",
    "section": "llama.cpp",
    "text": "llama.cpp\nTheBloke/Llama-2-7b-Chat-GGUF\n\nfrom llama_cpp import Llama\n\n\nllm = Llama(model_path=\"./Data/llama-2-7b-chat.Q2_K.gguf\")\n\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./Data/llama-2-7b-chat.Q2_K.gguf (version GGUF V2 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight q2_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    4:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    7:         blk.0.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    8:              blk.0.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    9:              blk.0.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   11:            blk.1.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   13:              blk.1.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   16:         blk.1.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   17:              blk.1.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   18:              blk.1.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   20:           blk.10.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   22:             blk.10.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   24:             blk.10.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   25:        blk.10.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   26:             blk.10.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   27:             blk.10.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   29:           blk.11.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   31:             blk.11.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   33:             blk.11.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   34:        blk.11.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   35:             blk.11.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   36:             blk.11.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   38:           blk.12.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   40:             blk.12.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   42:             blk.12.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   43:        blk.12.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   44:             blk.12.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   45:             blk.12.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   47:           blk.13.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   49:             blk.13.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   51:             blk.13.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   52:        blk.13.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   53:             blk.13.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   54:             blk.13.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   56:           blk.14.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   58:             blk.14.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   60:             blk.14.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   61:        blk.14.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   62:             blk.14.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   63:             blk.14.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   65:           blk.15.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   67:             blk.15.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   69:             blk.15.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   70:        blk.15.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   71:             blk.15.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   72:             blk.15.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   74:           blk.16.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   76:             blk.16.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   78:             blk.16.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   79:        blk.16.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   80:             blk.16.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   81:             blk.16.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   83:           blk.17.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   85:             blk.17.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   87:             blk.17.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   88:        blk.17.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   89:             blk.17.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   90:             blk.17.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   92:           blk.18.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   94:             blk.18.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   96:             blk.18.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   97:        blk.18.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   98:             blk.18.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   99:             blk.18.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  101:           blk.19.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  103:             blk.19.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  105:             blk.19.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  106:        blk.19.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  107:             blk.19.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  108:             blk.19.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  110:            blk.2.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  112:              blk.2.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  114:              blk.2.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  115:         blk.2.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  116:              blk.2.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  117:              blk.2.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  119:           blk.20.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  121:             blk.20.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  123:             blk.20.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  124:        blk.20.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  125:             blk.20.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  126:             blk.20.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  128:           blk.21.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  130:             blk.21.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  132:             blk.21.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  133:        blk.21.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  134:             blk.21.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  135:             blk.21.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  137:           blk.22.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  139:             blk.22.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  141:             blk.22.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  142:        blk.22.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  143:             blk.22.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  144:             blk.22.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  146:           blk.23.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  148:             blk.23.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  150:             blk.23.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  151:        blk.23.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  152:             blk.23.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  153:             blk.23.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  155:            blk.3.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  157:              blk.3.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  159:              blk.3.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  160:         blk.3.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  161:              blk.3.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  162:              blk.3.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  164:            blk.4.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  166:              blk.4.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  168:              blk.4.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  169:         blk.4.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  170:              blk.4.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  171:              blk.4.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  173:            blk.5.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  175:              blk.5.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  177:              blk.5.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  178:         blk.5.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  179:              blk.5.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  180:              blk.5.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  182:            blk.6.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  184:              blk.6.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  186:              blk.6.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  187:         blk.6.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  188:              blk.6.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  189:              blk.6.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  191:            blk.7.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  193:              blk.7.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  195:              blk.7.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  196:         blk.7.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  197:              blk.7.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  198:              blk.7.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  200:            blk.8.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  202:              blk.8.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  204:              blk.8.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  205:         blk.8.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  206:              blk.8.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  207:              blk.8.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  209:            blk.9.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  211:              blk.9.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  213:              blk.9.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  214:         blk.9.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  215:              blk.9.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  216:              blk.9.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  219:           blk.24.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  221:             blk.24.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  223:             blk.24.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  224:        blk.24.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  225:             blk.24.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  226:             blk.24.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  228:           blk.25.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  230:             blk.25.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  232:             blk.25.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  233:        blk.25.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  234:             blk.25.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  235:             blk.25.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  237:           blk.26.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  239:             blk.26.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  241:             blk.26.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  242:        blk.26.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  243:             blk.26.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  244:             blk.26.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  246:           blk.27.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  248:             blk.27.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  250:             blk.27.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  251:        blk.27.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  252:             blk.27.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  253:             blk.27.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  255:           blk.28.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  257:             blk.28.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  259:             blk.28.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  260:        blk.28.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  261:             blk.28.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  262:             blk.28.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  264:           blk.29.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  266:             blk.29.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  268:             blk.29.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  269:        blk.29.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  270:             blk.29.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  271:             blk.29.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  273:           blk.30.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  275:             blk.30.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  277:             blk.30.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  278:        blk.30.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  279:             blk.30.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  280:             blk.30.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  282:           blk.31.ffn_down.weight q3_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  284:             blk.31.ffn_up.weight q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  286:             blk.31.attn_k.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  287:        blk.31.attn_output.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  288:             blk.31.attn_q.weight q2_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  289:             blk.31.attn_v.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture str     \nllama_model_loader: - kv   1:                               general.name str     \nllama_model_loader: - kv   2:                       llama.context_length u32     \nllama_model_loader: - kv   3:                     llama.embedding_length u32     \nllama_model_loader: - kv   4:                          llama.block_count u32     \nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \nllama_model_loader: - kv  10:                          general.file_type u32     \nllama_model_loader: - kv  11:                       tokenizer.ggml.model str     \nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \nllama_model_loader: - kv  18:               general.quantization_version u32     \nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q2_K:   65 tensors\nllama_model_loader: - type q3_K:  160 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx          = 512\nllm_load_print_meta: n_embd         = 4096\nllm_load_print_meta: n_head         = 32\nllm_load_print_meta: n_head_kv      = 32\nllm_load_print_meta: n_layer        = 32\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 1\nllm_load_print_meta: f_norm_eps     = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps = 1.0e-06\nllm_load_print_meta: n_ff           = 11008\nllm_load_print_meta: freq_base      = 10000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 7B\nllm_load_print_meta: model ftype    = mostly Q2_K\nllm_load_print_meta: model params   = 6.74 B\nllm_load_print_meta: model size     = 2.63 GiB (3.35 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.09 MB\nllm_load_tensors: mem required  = 2694.41 MB (+  256.00 MB per state)\n.................................................................................................\nllama_new_context_with_model: kv self size  =  256.00 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \nllama_new_context_with_model: compute buffer total size =   71.97 MB\n\n\n\noutput = llm(\"Q: Name the planets in the solar system in english? A: \", max_tokens=128, stop=[\"Q:\", \"\\n\"], echo=True)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =  2121.60 ms\nllama_print_timings:      sample time =    18.04 ms /    48 runs   (    0.38 ms per token,  2661.49 tokens per second)\nllama_print_timings: prompt eval time =   750.75 ms /     7 tokens (  107.25 ms per token,     9.32 tokens per second)\nllama_print_timings:        eval time =  8113.82 ms /    47 runs   (  172.63 ms per token,     5.79 tokens per second)\nllama_print_timings:       total time =  8972.69 ms\n\n\n\nprint(output['choices'])\n\n[{'text': 'Q: Name the planets in the solar system in english? A: 1.ἱ Mars. 2. Mercury . 3. Venus. 4. Earth. 5. Jupiter. 6. Saturn. 7. Uranus. 8. Neptune', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}]",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "lm-hackers.html#mlc",
    "href": "lm-hackers.html#mlc",
    "title": "A hacker’s guide to Language Models",
    "section": "MLC",
    "text": "MLC",
    "crumbs": [
      "Blog",
      "A hacker's guide to Language Models"
    ]
  },
  {
    "objectID": "why-you-should-use-a-framework.html",
    "href": "why-you-should-use-a-framework.html",
    "title": "Why you should use a framework",
    "section": "",
    "text": "If you’ve finished going through my Linear model and neural net from scratch notebook, then now is a good time to look at how to do the same thing using a library, instead of doing it from scratch. We’ll use fastai and PyTorch. The benefits of using these libraries is:\nLet’s see how that looks in practice. We’ll start by doing the same library setup as in the “from scratch” notebook:\nWe’ll import the fastai tabular library, set a random seed so the notebook is reproducible, and pick a reasonable number of significant figures to display in our tables:\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\ndatapath = Path('./Data')\nname = 'titanic'\npath = Path(f'{datapath}/{name}')\n\nkaggle_competition_download(name, datapath)\n\nfile exists\nfrom fastai.tabular.all import *\n\npd.options.display.float_format = '{:.2f}'.format\nset_seed(42)",
    "crumbs": [
      "Blog",
      "Why you should use a framework"
    ]
  },
  {
    "objectID": "why-you-should-use-a-framework.html#prep-the-data",
    "href": "why-you-should-use-a-framework.html#prep-the-data",
    "title": "Why you should use a framework",
    "section": "Prep the data",
    "text": "Prep the data\nWe’ll read the CSV file just like we did before:\n\ndf = pd.read_csv(path/'train.csv')\n\nWhen you do everything from scratch, every bit of feature engineering requires a whole lot of work, since you have to think about things like dummy variables, normalization, missing values, and so on. But with fastai that’s all done for you. So let’s go wild and create lots of new features! We’ll use a bunch of the most interesting ones from this fantastic Titanic feature engineering notebook (and be sure to click that link and upvote that notebook if you like it to thank the author for their hard work!)\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\ndef add_features(df):\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n    df['Family'] = df.SibSp+df.Parch\n    df['Alone'] = df.Family==0\n    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')\n    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n    df['Title'] = df.Title.map(dict(Mr=\"Mr\",Miss=\"Miss\",Mrs=\"Mrs\",Master=\"Master\"))\n\nadd_features(df)\n\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'LogFare', 'Deck',\n       'Family', 'Alone', 'TicketFreq', 'Title'],\n      dtype='object')\n\n\nAs we discussed in the last notebook, we can use RandomSplitter to separate out the training and validation sets:\n\nsplits = RandomSplitter(seed=42)(df)\nsplits\n\n((#713) [788,525,821,253,374,98,215,313,281,305...],\n (#178) [303,778,531,385,134,476,691,443,386,128...])\n\n\nNow the entire process of getting the data ready for training requires just this one cell!:\n\ndls = TabularPandas(\n    df, splits=splits,\n    procs = [Categorify, FillMissing, Normalize],\n    cat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],\n    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],\n    y_names=\"Survived\", y_block = CategoryBlock(),\n).dataloaders(path=\".\")\n\nHere’s what each of the parameters means:\n\nUse splits for indices of training and validation sets:\nsplits=splits,\nTurn strings into categories, fill missing values in numeric columns with the median, normalise all numeric columns:\nprocs = [Categorify, FillMissing, Normalize],\nThese are the categorical independent variables:\ncat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],\nThese are the continuous independent variables:\ncont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],\nThis is the dependent variable:\ny_names=\"Survived\",\nThe dependent variable is categorical (so build a classification model, not a regression model):\ny_block = CategoryBlock(),",
    "crumbs": [
      "Blog",
      "Why you should use a framework"
    ]
  },
  {
    "objectID": "why-you-should-use-a-framework.html#train-the-model",
    "href": "why-you-should-use-a-framework.html#train-the-model",
    "title": "Why you should use a framework",
    "section": "Train the model",
    "text": "Train the model\nThe data and model together make up a Learner. To create one, we say what the data is (dls), and the size of each hidden layer ([10,10]), along with any metrics we want to print along the way:\n\nlearn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n\nYou’ll notice we didn’t have to do any messing around to try to find a set of random coefficients that will train correctly – that’s all handled automatically.\nOne handy feature that fastai can also tell us what learning rate to use:\n\nlearn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=0.05754399299621582, valley=0.013182567432522774)\n\n\n\n\n\n\n\n\n\nThe two colored points are both reasonable choices for a learning rate. I’ll pick somewhere between the two (0.03) and train for a few epochs:\n\nlearn.fit(16, lr=0.03)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.577146\n0.582949\n0.606742\n00:00\n\n\n1\n0.510818\n0.498523\n0.786517\n00:00\n\n\n2\n0.467023\n0.459841\n0.797753\n00:00\n\n\n3\n0.439957\n0.468547\n0.797753\n00:00\n\n\n4\n0.427232\n0.415261\n0.825843\n00:00\n\n\n5\n0.416340\n0.437362\n0.820225\n00:00\n\n\n6\n0.408347\n0.413253\n0.848315\n00:00\n\n\n7\n0.400442\n0.406075\n0.803371\n00:00\n\n\n8\n0.397265\n0.443730\n0.820225\n00:00\n\n\n9\n0.392389\n0.432267\n0.831461\n00:00\n\n\n10\n0.389983\n0.415383\n0.831461\n00:00\n\n\n11\n0.386057\n0.425319\n0.820225\n00:00\n\n\n12\n0.382527\n0.435054\n0.831461\n00:00\n\n\n13\n0.378309\n0.429746\n0.831461\n00:00\n\n\n14\n0.374304\n0.419489\n0.825843\n00:00\n\n\n15\n0.372190\n0.425430\n0.831461\n00:00\n\n\n\n\n\nWe’ve got a similar accuracy to our previous “from scratch” model – which isn’t too surprising, since as we discussed, this dataset is too small and simple to really see much difference. A simple linear model already does a pretty good job. But that’s OK – the goal here is to show you how to get started with deep learning and understand how it really works, and the best way to do that is on small and easy to understand datasets.",
    "crumbs": [
      "Blog",
      "Why you should use a framework"
    ]
  },
  {
    "objectID": "why-you-should-use-a-framework.html#submit-to-kaggle",
    "href": "why-you-should-use-a-framework.html#submit-to-kaggle",
    "title": "Why you should use a framework",
    "section": "Submit to Kaggle",
    "text": "Submit to Kaggle\nOne important feature of fastai is that all the information needed to apply the data transformations and the model to a new dataset are stored in the learner. You can call export to save it to a file to use it later in production, or you can use the trained model right away to get predictions on a test set.\nTo submit to Kaggle, we’ll need to read in the test set, and do the same feature engineering we did for the training set:\n\ntst_df = pd.read_csv(path/'test.csv')\ntst_df['Fare'] = tst_df.Fare.fillna(0)\nadd_features(tst_df)\n\nBut we don’t need to manually specify any of the processing steps necessary to get the data ready for modeling, since that’s all saved in the learner. To specify we want to apply the same steps to a new dataset, use the test_dl() method:\n\ntst_dl = learn.dls.test_dl(tst_df)\n\nNow we can use get_preds to get the predictions for the test set:\n\npreds,_ = learn.get_preds(dl=tst_dl)\n\n\n\n\n\n\n\n\nFinally, let’s create a submission CSV just like we did in the previous notebook…\n\ntst_df['Survived'] = (preds[:,1]&gt;0.5).int()\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('sub.csv', index=False)\n\n…and check that it looks reasonable:\n\n!head sub.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,1\n897,0\n898,1\n899,0\n900,1",
    "crumbs": [
      "Blog",
      "Why you should use a framework"
    ]
  },
  {
    "objectID": "why-you-should-use-a-framework.html#ensembling",
    "href": "why-you-should-use-a-framework.html#ensembling",
    "title": "Why you should use a framework",
    "section": "Ensembling",
    "text": "Ensembling\nSince it’s so easy to create a model now, it’s easier to play with more advanced modeling approaches. For instance, we can create five separate models, each trained from different random starting points, and average them. This is the simplest approach of ensembling models, which combines multiple models to generate predictions that are better than any of the single models in the ensemble.\nTo create our ensemble, first we copy the three steps we used above to create and train a model, and apply it to the test set:\n\ndef ensemble():\n    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n    with learn.no_logging(): learn.fit(16, lr=0.03)\n    return learn.get_preds(dl=tst_dl)[0]\n\nNow we run this five times, and collect the results into a list:\n\nlearns = [ensemble() for _ in range(5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe stack this predictions together and take their average predictions:\n\nens_preds = torch.stack(learns).mean(0)\n\nFinally, use the same code as before to generate a submission file, which we can submit to Kaggle after the notebook is saved and run:\n\ntst_df['Survived'] = (ens_preds[:,1]&gt;0.5).int()\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('ens_sub.csv', index=False)\n\nAt the time of writing, this submission is well within the top 25% of entries to the competition.\n(A lot of submissions to this competition use additional external data, but we have restricted ourselves to just using the data provided. We’d probably do a lot better if we used external data too. Feel free to give that a try, and see how you go. Note that you’ll never be able to get to the top of the leaderboard, since a lot of folks in this competition have cheated, by downloading the answers from the internet and uploading them as their submission. In a real competition that’s not possible, because the answers aren’t public, but there’s nothing stopping people from cheating in a tutorial/practice competition like this one. So if you’re ready for a real challenge, take a look at the competitions page and start working on a real competition!)",
    "crumbs": [
      "Blog",
      "Why you should use a framework"
    ]
  },
  {
    "objectID": "why-you-should-use-a-framework.html#final-thoughts",
    "href": "why-you-should-use-a-framework.html#final-thoughts",
    "title": "Why you should use a framework",
    "section": "Final thoughts",
    "text": "Final thoughts\nAs you can see, using fastai and PyTorch made things much easier than doing it from scratch, but it also hid away a lot of the details. So if you only ever use a framework, you’re not going to as fully understand what’s going on under the hood. That understanding can be really helpful when it comes to debugging and improving your models. But do use fastai when you’re creating models on Kaggle or in “real life”, because otherwise you’re not taking advantage of all the research that’s gone into optimising the models for you, and you’ll end up spending more time debugging and implementing menial boiler-plate than actually solving the real problem!\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!",
    "crumbs": [
      "Blog",
      "Why you should use a framework"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html",
    "href": "linear-model-and-neural-net-from-scratch.html",
    "title": "Linear model and Neural Net from scratch",
    "section": "",
    "text": "We’ll be assuming you already know the basics of how a neural network works. If you don’t, read this notebook first: How does a neural net really work?. We’ll be using Kaggle’s Titanic competition in this notebook, because it’s very small and simple, but also has displays many of the tricky real-life issues that we need to handle in most practical projects. (Note, however, that this competition is a small “learner” competition on Kaggle, so don’t expect to actually see much benefits from using a neural net just yet; that will come once we try our some real competitions!)\nIt’s great to be able to run the same notebook on your own machine or Colab, as well as Kaggle. To allow for this, we use this code to download the data as needed when not on Kaggle (see this notebook for details about this technique):\nNote that the data for Kaggle comps always lives in the ../input folder. The easiest way to get the path is to click the “K” button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\nWe’ll be using numpy and pytorch for array calculations in this notebook, and pandas for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.\n!pip list | grep nbdevAuto\n\nnbdevAuto                     0.0.95\nfrom nbdevAuto.functions import kaggle_competition_download\nfrom pathlib import Path\nname = 'titanic'\ndatapath = Path('./Data')\nkaggle_competition_download(name, folderpath = datapath)\n\nfile exists\npath = f'{datapath}/{name}'\npath\n\n'Data/titanic'\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "href": "linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "title": "Linear model and Neural Net from scratch",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nThis is a tabular data competition – the data is in the form of a table. It’s provided as a Comma Separated Values (CSV) file. We can open it using the pandas library, which will create a DataFrame.\n\ndf = pd.read_csv(f'{path}/train.csv')\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\nAs we learned in the How does a neural net really work notebook, we going to want to multiply each column by some coefficients. But we can see in the Cabin column that there are NaN values, which is how Pandas refers to missing values. We can’t multiply something by a missing value!\nLet’s check which columns contain NaN values. Pandas’ isna() function returns True (which is treated as 1 when used as a number) for NaN values, so we can just add them up for each column:\n\ndf.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\nNotice that by default Pandas sums over columns.\nWe’ll need to replace the missing values with something. It doesn’t generally matter too much what we choose. We’ll use the most common value (the “mode”). We can use the mode function for that. One wrinkle is that it returns more than one row in the case of ties, so we just grab the first row with iloc[0]:\n\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\nBTW, it’s never a good idea to use functions without understanding them. So be sure to google for anything you’re not familiar with. E.g if you want to learn about iloc (which is a very important function indeed!) then Google will give you a link to a great tutorial.\nNow that we’ve got the mode of each column, we can use fillna to replace the missing values with the mode of each column. We’ll do it “in place” – meaning that we’ll change the dataframe itself, rather than returning a new one.\n\ndf.fillna(modes, inplace=True)\n\nWe can now check there’s no missing values left:\n\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\nHere’s how we get a quick summary of all the numeric columns in the dataset:\n\nimport numpy as np\n\ndf.describe(include=(np.number))\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n28.566970\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n13.199572\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n22.000000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n24.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n35.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\nWe can see that Fare contains mainly values of around 0 to 30, but there’s a few really big ones. This is very common with fields contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\nYou can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don’t forget: if you’re not entirely sure what a histogram is, Google “histogram tutorial” and do a bit of reading before continuing on):\n\ndf['Fare'].hist();\n\n\n\n\n\n\n\n\nTo fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable. Note, however, that there are zeros in the Fare column, and log(0) is infinite – to fix this, we’ll simply add 1 to all values first:\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\nThe histogram now shows a more even distribution of values without the long tail:\n\ndf['LogFare'].hist();\n\n\n\n\n\n\n\n\nIt looks from the describe() output like Pclass contains just 3 values, which we can confirm by looking at the Data Dictionary (which you should always study carefully for any project!) –\n\npclasses = sorted(df.Pclass.unique())\npclasses\n\n[1, 2, 3]\n\n\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\ndf.describe(include=[object])\n\n\n\n\n\n\n\n\nName\nSex\nTicket\nCabin\nEmbarked\n\n\n\n\ncount\n891\n891\n891\n891\n891\n\n\nunique\n891\n2\n681\n147\n3\n\n\ntop\nBraund, Mr. Owen Harris\nmale\n347082\nB96 B98\nS\n\n\nfreq\n1\n577\n7\n691\n646\n\n\n\n\n\n\n\nClearly we can’t multiply strings like male or S by coefficients, so we need to replace those with numbers.\nWe do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Sex='male', which would be a new column containing 1 for rows where Sex is 'male', and 0 for rows where it isn’t.\nPandas can create these automatically using get_dummies, which also remove the original columns. We’ll create dummy variables for Pclass, even although it’s numeric, since the numbers 1, 2, and 3 correspond to first, second, and third class cabins - not to counts or measures that make sense to multiply by. We’ll also create dummies for Sex and Embarked since we’ll want to use those as predictors in our model. On the other hand, Cabin, Name, and Ticket have too many unique values for it to make sense creating dummy variables for them.\n\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\nWe can see that 5 columns have been added to the end – one for each of the possible values of each of the three columns we requested, and that those three requested columns have been removed.\nHere’s what the first few rows of those newly added columns look like:\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\n\n\n\n\n\n\n\nSex_male\nSex_female\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n1\n0\n1\n1\n0\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n3\n0\n1\n1\n0\n0\n0\n0\n1\n\n\n4\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\nNow we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is Survived:\n\nfrom torch import tensor\n\nt_dep = tensor(df.Survived)\n\nOur independent variables are all the continuous variables of interest plus all the dummy variables we just created:\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\ndf[indep_cols] = df[indep_cols].astype('float32')  \nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep\n\ntensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        ...,\n        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])\n\n\nHere’s the number of rows and columns we have for our independent variables:\n\nt_indep.shape\n\ntorch.Size([891, 12])",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "href": "linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "title": "Linear model and Neural Net from scratch",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nNow that we’ve got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we’re going to manually do a single step of calculating predictions and loss for every row of our data.\nOur first model will be a simple linear model. We’ll need a coefficient for each column in t_indep. We’ll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])\n\n\nOur predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don’t need a separate constant term (also known as a “bias” or “intercept” term), or a column of all 1s to give the same effect has having a constant term. That’s because our dummy variables already cover the entire dataset – e.g. there’s a column for “male” and a column for “female”, and everyone in the dataset is in exactly one of these; therefore, we don’t need a separate intercept term to cover rows that aren’t otherwise part of a column.\nHere’s what the multiplication looks like:\n\nt_indep*coeffs\n\ntensor([[-10.1838,   0.1386,   0.0000,  -0.4772,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-17.5902,   0.1386,   0.0000,  -0.9681,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-12.0354,   0.0000,   0.0000,  -0.4950,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.1386,   0.0000,  -0.9025,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.0000,   0.0000,  -0.4982,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.0000,   0.0000,  -0.5081,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-24.9966,   0.0000,   0.0000,  -0.8973,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        ...,\n        [-11.5725,   0.0000,   0.0000,  -0.4717,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-18.0531,   0.0000,   1.2045,  -0.7701,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-12.4983,   0.0000,   0.0000,  -0.5968,  -0.2632,  -0.0000,   0.0000,   0.3136,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [ -8.7951,   0.0000,   0.0000,  -0.7766,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.1386,   0.4818,  -0.7229,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-12.0354,   0.0000,   0.0000,  -0.7766,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-14.8128,   0.0000,   0.0000,  -0.4905,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000]])\n\n\nWe can see we’ve got a problem here. The sums of each row will be dominated by the first column, which is Age, since that’s bigger on average than all the others.\nLet’s make all the columns contain numbers from 0 to 1, by dividing each column by its max():\n\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\nAs we see, that removes the problem of one column dominating all the others:\n\nt_indep*coeffs\n\ntensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1504,  0.0000,  0.0000, -0.0793, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0173,  0.0000, -0.1446, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0000,  0.0000, -0.0798, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0000,  0.0000, -0.0814, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.3125,  0.0000,  0.0000, -0.1438, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        ...,\n        [-0.1447,  0.0000,  0.0000, -0.0756, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2257,  0.0000,  0.2008, -0.1234, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.1562,  0.0000,  0.0000, -0.0956, -0.2632, -0.0000,  0.0000,  0.3136,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1099,  0.0000,  0.0000, -0.1244, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0173,  0.0803, -0.1158, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1504,  0.0000,  0.0000, -0.1244, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1852,  0.0000,  0.0000, -0.0786, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000]])\n\n\nOne thing you hopefully noticed is how amazingly cool this line of code is:\nt_indep = t_indep / vals\nThat is dividing a matrix by a vector – what on earth does that mean?!? The trick here is that we’re taking advantage of a technique in numpy and PyTorch (and many other languages, going all the way back to APL) called broadcasting. In short, this acts as if there’s a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn’t actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we’re using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it’s well worth studying and practicing.\nWe can now create predictions from our linear model, by adding up the rows of the product:\n\npreds = (t_indep*coeffs).sum(axis=1)\n\nLet’s take a look at the first few:\n\npreds[:10]\n\ntensor([ 0.1927, -0.6239,  0.0979,  0.2056,  0.0968,  0.0066,  0.1306,  0.3476,  0.1613, -0.6285])\n\n\nOf course, these predictions aren’t going to be any use, since our coefficients are random – they’re just a starting point for our gradient descent process.\nTo do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\ntensor(0.5382)\n\n\nNow that we’ve tested out a way of calculating predictions, and loss, let’s pop them into functions to make life easier:\n\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#doing-a-gradient-descent-step",
    "href": "linear-model-and-neural-net-from-scratch.html#doing-a-gradient-descent-step",
    "title": "Linear model and Neural Net from scratch",
    "section": "Doing a gradient descent step",
    "text": "Doing a gradient descent step\nIn this section, we’re going to do a single “epoch” of gradient descent manually. The only thing we’re going to automate is calculating gradients, because let’s face it that’s pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we’ll need to call requires_grad_() on our coeffs (if you’re not sure why, review the previous notebook, How does a neural net really work?, before continuing):\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\nNow when we calculate our loss, PyTorch will keep track of all the steps, so we’ll be able to get the gradients afterwards:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.5382, grad_fn=&lt;MeanBackward0&gt;)\n\n\nUse backward() to ask PyTorch to calculate gradients now:\n\nloss.backward()\n\nLet’s see what they look like:\n\ncoeffs.grad\n\ntensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])\n\n\nNote that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. Let’s try running the above steps again:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\ntensor([-0.0212,  0.0258, -0.0082, -0.0969,  0.4198, -0.4265, -0.2424, -0.0494,  0.2851, -0.3771, -0.0382,  0.4085])\n\n\nAs you see, our .grad values are have doubled. That’s because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\nWe can now do one gradient descent step, and check that our loss decreases:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)\n    coeffs.grad.zero_()\n    print(calc_loss(coeffs, t_indep, t_dep))\n\ntensor(0.4945)\n\n\nNote that a.sub_(b) subtracts b from a in-place. In PyTorch, any method that ends in _ changes its object in-place. Similarly, a.zero_() sets all elements of a tensor to zero.",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "href": "linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "title": "Linear model and Neural Net from scratch",
    "section": "Training the linear model",
    "text": "Training the linear model\nBefore we begin training our model, we’ll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see “Getting started with NLP for absolute beginners”.\nThere’s lots of different ways we can do this. In the next notebook we’ll be comparing our approach here to what the fastai library does, so we’ll want to ensure we split the data in the same way. So let’s use RandomSplitter to get indices that will split our data into training and validation sets:\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\nNow we can apply those indicies to our independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\nWe’ll create functions for the three things we did manually above: updating coeffs, doing one full gradient descent step, and initilising coeffs to random numbers:\n\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nWe can now use these functions to train our model:\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nLet’s try it. Our loss will print at the end of every step, so we hope we’ll see it going down:\n\ncoeffs = train_model(18, lr=0.2)\n\n0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; \n\n\nIt does!\nLet’s take a look at the coefficients for each column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()\n\n{'Age': tensor(-0.2694),\n 'SibSp': tensor(0.0901),\n 'Parch': tensor(0.2359),\n 'LogFare': tensor(0.0280),\n 'Sex_male': tensor(-0.3990),\n 'Sex_female': tensor(0.2345),\n 'Pclass_1': tensor(0.7232),\n 'Pclass_2': tensor(0.4112),\n 'Pclass_3': tensor(0.3601),\n 'Embarked_C': tensor(0.0955),\n 'Embarked_Q': tensor(0.2395),\n 'Embarked_S': tensor(0.2122)}",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "href": "linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "title": "Linear model and Neural Net from scratch",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nThe Kaggle competition is not, however, scored by absolute error (which is our loss function). It’s scored by accuracy – the proportion of rows where we correctly predict survival. Let’s see how accurate we were on the validation set. First, calculate the predictions:\n\npreds = calc_preds(coeffs, val_indep)\n\nWe’ll assume that any passenger with a score of over 0.5 is predicted to survive. So that means we’re correct for each row where preds&gt;0.5 is the same as the dependent variable:\n\nresults = val_dep.bool()==(preds&gt;0.5)\nresults[:16]\n\ntensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False])\n\n\nLet’s see what our average accuracy is:\n\nresults.float().mean()\n\ntensor(0.7865)\n\n\nThat’s not a bad start at all! We’ll create a function so we can calcuate the accuracy easy for other models we train:\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)&gt;0.5)).float().mean()\nacc(coeffs)\n\ntensor(0.7865)",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "href": "linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "title": "Linear model and Neural Net from scratch",
    "section": "Using sigmoid",
    "text": "Using sigmoid\nLooking at our predictions, there’s one obvious problem – some of our predictions of the probability of survival are &gt;1, and some are &lt;0:\n\npreds[:28]\n\ntensor([ 0.8160,  0.1295, -0.0148,  0.1831,  0.1520,  0.1350,  0.7279,  0.7754,  0.3222,  0.6740,  0.0753,  0.0389,  0.2216,  0.7631,\n         0.0678,  0.3997,  0.3324,  0.8278,  0.1078,  0.7126,  0.1023,  0.3627,  0.9937,  0.8050,  0.1153,  0.1455,  0.8652,  0.3425])\n\n\nTo fix this, we should pass every prediction through the sigmoid function, which has a minimum at zero and maximum at one, and is defined as follows:\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\n\n\n\n\nPyTorch already defines that function for us, so we can modify calc_preds to use it:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\nLet’s train a new model now, using this updated function to calculate predictions:\n\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\nThe loss has improved by a lot. Let’s check the accuracy:\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nThat’s improved too! Here’s the coefficients of our trained model:\n\nshow_coeffs()\n\n{'Age': tensor(-1.5061),\n 'SibSp': tensor(-1.1575),\n 'Parch': tensor(-0.4267),\n 'LogFare': tensor(0.2543),\n 'Sex_male': tensor(-10.3320),\n 'Sex_female': tensor(8.4185),\n 'Pclass_1': tensor(3.8389),\n 'Pclass_2': tensor(2.1398),\n 'Pclass_3': tensor(-6.2331),\n 'Embarked_C': tensor(1.4771),\n 'Embarked_Q': tensor(2.1168),\n 'Embarked_S': tensor(-4.7958)}\n\n\nThese coefficients seem reasonable – in general, older people and males were less likely to survive, and first class passengers were more likely to survive.",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "href": "linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "title": "Linear model and Neural Net from scratch",
    "section": "Submitting to Kaggle",
    "text": "Submitting to Kaggle\nNow that we’ve got a trained model, we can prepare a submission to Kaggle. To do that, first we need to read the test set:\n\ntst_df = pd.read_csv(f'{path}/test.csv')\n\nIn this case, it turns out that the test set is missing Fare for one passenger. We’ll just fill it with 0 to avoid problems:\n\ntst_df['Fare'] = tst_df.Fare.fillna(0)\n\nNow we can just copy the same steps we did to our training set and do the same exact things on our test set to preprocess the data:\n\ntst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\ntst_df[indep_cols] = tst_df[indep_cols].astype('float64') \n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\nLet’s calculate our predictions of which passengers survived in the test set:\n\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs)&gt;0.5).int()\n\nThe sample submission on the Kaggle competition site shows that we’re expected to upload a CSV with just PassengerId and Survived, so let’s create that and save it:\n\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('sub.csv', index=False)\n\nWe can check the first few rows of the file to make sure it looks reasonable:\n\n!head sub.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,1\n899,0\n900,1\n\n\nWhen you click “save version” in Kaggle, and wait for the notebook to run, you’ll see that sub.csv appears in the “Data” tab. Clicking on that file will show a Submit button, which allows you to submit to the competition.",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "href": "linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "title": "Linear model and Neural Net from scratch",
    "section": "Using matrix product",
    "text": "Using matrix product\nWe can make things quite a bit neater…\nTake a look at the inner-most calculation we’re doing to get the predictions:\n\n(val_indep*coeffs).sum(axis=1)\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3512, -13.6469,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])\n\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:\n\nval_indep@coeffs\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])\n\n\nIt also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\nLet’s use this to replace how calc_preds works:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\nIn order to do matrix-matrix products (which we’ll need in the next section), we need to turn coeffs into a column vector (i.e. a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\nWe’ll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\nWe can now train our model as before and confirm we get identical outputs…:\n\ncoeffs = train_model(lr=100)\n\n0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n…and identical accuracy:\n\nacc(coeffs)\n\ntensor(0.8258)",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "href": "linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "title": "Linear model and Neural Net from scratch",
    "section": "A neural network",
    "text": "A neural network\nWe’ve now got what we need to implement our neural network.\nFirst, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.\nThen our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nThat’s it – we’re now ready to train our model!\n\ncoeffs = train_model(lr=1.4)\n\n0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; \n\n\n\ncoeffs = train_model(lr=20)\n\n0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; \n\n\nIt’s looking good – our loss is lower than before. Let’s see if that translates to a better result on the validation set:\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nIn this case our neural net isn’t showing better results than the linear model. That’s not surprising; this dataset is very small and very simple, and isn’t the kind of thing we’d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#deep-learning",
    "href": "linear-model-and-neural-net-from-scratch.html#deep-learning",
    "title": "Linear model and Neural Net from scratch",
    "section": "Deep learning",
    "text": "Deep learning\nThe neural net in the previous section only uses one hidden layer, so it doesn’t count as “deep” learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\nFirst, we’ll need to create additional coefficients for each layer:\n\ndef init_coeffs():\n    hiddens = [10, 10]  # &lt;-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nYou’ll notice here that there’s a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you’ll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days – it’s very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we’ll learn about in other notebooks.\nOur deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\nWe also need a minor update to update_coeffs since we’ve got layers and consts separated now:\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nLet’s train our model…\n\ncoeffs = train_model(lr=4)\n\n0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; \n\n\n…and check its accuracy:\n\nacc(coeffs)\n\ntensor(0.8258)",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "href": "linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "title": "Linear model and Neural Net from scratch",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt’s actually pretty cool that we’ve managed to create a real deep learning model from scratch and trained it to get over 80% accuracy on this task, all in the course of a single notebook!\nThe “real” deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you’ll recognise the basic steps are the same.\nThe biggest differences in practical models to what we have above are:\n\nHow initialisation and normalisation is done to ensure the model trains correctly every time\nRegularization (to avoid over-fitting)\nModifying the neural net itself to take advantage of knowledge of the problem domain\nDoing gradient descent steps on smaller batches, rather than the whole dataset.\n\nI’ll be adding notebooks about all these later, and will add links here once they’re ready.\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!",
    "crumbs": [
      "Blog",
      "Linear model and Neural Net from scratch"
    ]
  },
  {
    "objectID": "activations.html",
    "href": "activations.html",
    "title": "Activation stats",
    "section": "",
    "text": "source",
    "crumbs": [
      "Blog",
      "Activation stats"
    ]
  },
  {
    "objectID": "activations.html#baseline",
    "href": "activations.html#baseline",
    "title": "Activation stats",
    "section": "Baseline",
    "text": "Baseline\n\nsource\n\ncnn_layers\n\n cnn_layers ()\n\n\n\nExported source\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\n\n\n\nsource\n\n\nconv\n\n conv (ni, nf, ks=3, act=True)\n\nWe want to train quickly, so that means training at a high learning rate.\n\n\nExported source\nfrom torcheval.metrics import MulticlassAccuracy\n\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n\n\nsource\n\n\nfit\n\n fit (model, epochs=1, xtra_cbs=None)\n\n\n\nExported source\ndef fit(model, epochs=1, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.6, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n\n\nset_seed(1)\nlearn = fit(nn.Sequential(*cnn_layers()))\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.159\n2.301\n0\ntrain\n\n\n0.270\n2.203\n0\neval",
    "crumbs": [
      "Blog",
      "Activation stats"
    ]
  },
  {
    "objectID": "activations.html#hooks",
    "href": "activations.html#hooks",
    "title": "Activation stats",
    "section": "Hooks",
    "text": "Hooks\n\nManual insertion\n\nsource\n\n\nSequentialModel\n\n SequentialModel (*layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass SequentialModel(nn.Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.act_means = [[] for _ in layers]\n        self.act_stds  = [[] for _ in layers]\n        \n    def __call__(self, x):\n        for i,l in enumerate(self.layers):\n            x = l(x)\n            self.act_means[i].append(to_cpu(x).mean())\n            self.act_stds [i].append(to_cpu(x).std ())\n        return x\n    \n    def __iter__(self): return iter(self.layers)\n\n\n\nset_seed(1)\nmodel = SequentialModel(*cnn_layers())\nlearn = fit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.156\n2.297\n0\ntrain\n\n\n0.290\n1.984\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor l in model.act_means: plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor l in model.act_stds: plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nIdeally, mean is always about zero and std always around one.\n\n\nPytorch hooks\nHooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook). Hooks don’t require us to rewrite the model.\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\nA hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list.\n\nact_means = [[] for _ in model]\nact_stds  = [[] for _ in model]\n\n\nsource\n\n\nappend_stats\n\n append_stats (i, mod, inp, outp)\n\n\n\nExported source\ndef append_stats(i, mod, inp, outp):\n    act_means[i].append(to_cpu(outp).mean())\n    act_stds [i].append(to_cpu(outp).std())\n\n\n\nfor i,m in enumerate(model): \n    m.register_forward_hook(partial(append_stats, i))\n\n\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.156\n2.298\n0\ntrain\n\n\n0.286\n2.042\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor o in act_means: plt.plot(o)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\n\nHook class\nWe can refactor this in a Hook class. It’s very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won’t be properly released when your model is deleted.\n\nsource\n\n\nHook\n\n Hook (m, f)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Hook():\n    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n    def remove(self): self.hook.remove()\n    def __del__(self): self.remove()\n\n\n\nsource\n\n\nappend_stats\n\n append_stats (hook, mod, inp, outp)\n\n\n\nExported source\ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[])\n    acts = to_cpu(outp)\n    hook.stats[0].append(acts.mean())\n    hook.stats[1].append(acts.std())\n\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nhooks = [Hook(l, append_stats) for l in model[:5].children()]\n\n\nlearn = fit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.156\n2.298\n0\ntrain\n\n\n0.286\n2.042\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor h in hooks:\n    plt.plot(h.stats[0])\n    h.remove()\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\n\nA Hooks class\n\nsource\n\n\nDummyCtxMgr\n\n DummyCtxMgr ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DummyCtxMgr:\n    def __enter__(self, *args):\n        print(\"let's go!\")\n        return self\n    def __exit__ (self, *args): print(\"all done!\")\n    def hello(self): print(\"hello.\")\n\n\n\nwith DummyCtxMgr() as dcm: dcm.hello()\n\nlet's go!\nhello.\nall done!\n\n\n\nsource\n\n\nDummyList\n\n DummyList (iterable=())\n\n*Built-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.*\n\n\nExported source\nclass DummyList(list):\n    def __delitem__(self, i):\n        print(f\"Say bye to item {i}\")\n        super().__delitem__(i)\n\n\n\ndml = DummyList([1,3,2])\ndml\n\n[1, 3, 2]\n\n\n\ndel(dml[2])\ndml\n\nSay bye to item 2\n\n\n[1, 3]\n\n\n\nsource\n\n\nHooks\n\n Hooks (ms, f)\n\n*Built-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.*\n\n\nExported source\nclass Hooks(list):\n    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n    def __enter__(self, *args): return self\n    def __exit__ (self, *args): self.remove()\n    def __del__(self): self.remove()\n    def __delitem__(self, i):\n        self[i].remove()\n        super().__delitem__(i)\n    def remove(self):\n        for h in self: h.remove()\n\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nwith Hooks(model, append_stats) as hooks:\n    fit(model)\n    fig,axs = plt.subplots(1,2, figsize=(10,4))\n    for h in hooks:\n        for i in 0,1: axs[i].plot(h.stats[i])\n    plt.legend(range(6));\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.152\n2.299\n0\ntrain\n\n\n0.200\n2.045\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHooksCallback\n\nsource\n\n\nHooksCallback\n\n HooksCallback (hookfunc, mod_filter=&lt;function noop&gt;, on_train=True,\n                on_valid=False, mods=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass HooksCallback(Callback):\n    def __init__(self, hookfunc, mod_filter=fc.noop, on_train=True, on_valid=False, mods=None):\n        fc.store_attr()\n        super().__init__()\n    \n    def before_fit(self, learn):\n        if self.mods: mods=self.mods\n        else: mods = fc.filter_ex(learn.model.modules(), self.mod_filter)\n        self.hooks = Hooks(mods, partial(self._hookfunc, learn))\n\n    def _hookfunc(self, learn, *args, **kwargs):\n        if (self.on_train and learn.training) or (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)\n\n    def after_fit(self, learn): self.hooks.remove()\n    def __iter__(self): return iter(self.hooks)\n    def __len__(self): return len(self.hooks)\n\n\n\nhc = HooksCallback(append_stats, mod_filter=fc.risinstance(nn.Conv2d))\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtra_cbs=[hc]);\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.162\n2.301\n0\ntrain\n\n\n0.326\n2.131\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig,axs = plt.subplots(1,2, figsize=(10,4))\nfor h in hc:\n    for i in 0,1: axs[i].plot(h.stats[i])\nplt.legend(range(6));",
    "crumbs": [
      "Blog",
      "Activation stats"
    ]
  },
  {
    "objectID": "activations.html#histograms",
    "href": "activations.html#histograms",
    "title": "Activation stats",
    "section": "Histograms",
    "text": "Histograms\n\nsource\n\nappend_stats\n\n append_stats (hook, mod, inp, outp)\n\n\n\nExported source\ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n    acts = to_cpu(outp)\n    hook.stats[0].append(acts.mean())\n    hook.stats[1].append(acts.std())\n    hook.stats[2].append(acts.abs().histc(40,0,10))\n\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\nhc = HooksCallback(append_stats, mod_filter=fc.risinstance(nn.Conv2d))\nfit(model, xtra_cbs=[hc]);\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.156\n2.297\n0\ntrain\n\n\n0.287\n1.993\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_hist\n\n get_hist (h)\n\n\n\nExported source\n# Thanks to @ste for initial version of histgram plotting code\ndef get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()\n\n\n\nfig,axes = get_grid(len(hc), figsize=(11,5))\nfor ax,h in zip(axes.flat, hc):\n    show_image(get_hist(h), ax, origin='lower')\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_min\n\n get_min (h)\n\n\n\nExported source\ndef get_min(h):\n    h1 = torch.stack(h.stats[2]).t().float()\n    return h1[0]/h1.sum(0)\n\n\n\nfig,axes = get_grid(len(hc), figsize=(11,5))\nfor ax,h in zip(axes.flatten(), hc):\n    ax.plot(get_min(h))\n    ax.set_ylim(0,1)",
    "crumbs": [
      "Blog",
      "Activation stats"
    ]
  },
  {
    "objectID": "activations.html#activationstats",
    "href": "activations.html#activationstats",
    "title": "Activation stats",
    "section": "ActivationStats",
    "text": "ActivationStats\n\nsource\n\nActivationStats\n\n ActivationStats (mod_filter=&lt;function noop&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass ActivationStats(HooksCallback):\n    def __init__(self, mod_filter=fc.noop): super().__init__(append_stats, mod_filter)\n\n    def color_dim(self, figsize=(11,5)):\n        fig,axes = get_grid(len(self), figsize=figsize)\n        for ax,h in zip(axes.flat, self):\n            show_image(get_hist(h), ax, origin='lower')\n\n    def dead_chart(self, figsize=(11,5)):\n        fig,axes = get_grid(len(self), figsize=figsize)\n        for ax,h in zip(axes.flatten(), self):\n            ax.plot(get_min(h))\n            ax.set_ylim(0,1)\n\n    def plot_stats(self, figsize=(10,4)):\n        fig,axs = plt.subplots(1,2, figsize=figsize)\n        for h in self:\n            for i in 0,1: axs[i].plot(h.stats[i])\n        axs[0].set_title('Means')\n        axs[1].set_title('Stdevs')\n        plt.legend(fc.L.range(self))\n\n\n\nastats = ActivationStats(fc.risinstance(nn.Conv2d))\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtra_cbs=[astats]);\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.156\n2.297\n0\ntrain\n\n\n0.297\n1.995\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.color_dim()\n\n\n\n\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\n\n\n\n\n\nastats.plot_stats()",
    "crumbs": [
      "Blog",
      "Activation stats"
    ]
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "Learner",
    "section": "",
    "text": "Exported source\nimport math,torch,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom operator import attrgetter\nfrom functools import partial\nfrom copy import copy\n\nfrom torch import optim\nimport torch.nn.functional as F\n\nfrom fastAIcourse.conv import *\n\nfrom fastprogress import progress_bar,master_bar\nExported source\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nimport logging\nfrom fastcore.test import test_close\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\nbs = 1024\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 784]), tensor([5, 4, 9, 4, 3, 0, 6, 5, 7, 6]))\nsource",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#basic-callbacks-learner",
    "href": "learner.html#basic-callbacks-learner",
    "title": "Learner",
    "section": "Basic Callbacks Learner",
    "text": "Basic Callbacks Learner\n\nsource\n\nCancelEpochException\nCommon base class for all non-exit exceptions.\n\n\nExported source\nclass CancelFitException(Exception): pass\nclass CancelBatchException(Exception): pass\nclass CancelEpochException(Exception): pass\n\n\n\nsource\n\n\nCancelBatchException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelFitException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCallback\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Callback(): order = 0\n\n\n\nsource\n\n\nrun_cbs\n\n run_cbs (cbs, method_nm, learn=None)\n\n\n\nExported source\ndef run_cbs(cbs, method_nm, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, method_nm, None)\n        if method is not None: method(learn)\n\n\n\nsource\n\n\nCompletionCB\n\n CompletionCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass CompletionCB(Callback):\n    def before_fit(self, learn): self.count = 0\n    def after_batch(self, learn): self.count += 1\n    def after_fit(self, learn): print(f'Completed {self.count} batches')\n\n\n\ncbs = [CompletionCB()]\nrun_cbs(cbs, 'before_fit')\nrun_cbs(cbs, 'after_batch')\nrun_cbs(cbs, 'after_fit')\n\nCompleted 1 batches\n\n\n\nsource\n\n\nLearner\n\n Learner (model, dls, loss_func, lr, cbs, opt_func=&lt;class\n          'torch.optim.sgd.SGD'&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Learner():\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.preds = self.model(self.batch[0])\n        self.loss = self.loss_func(self.preds, self.batch[1])\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n\n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        try:\n            self.callback('before_epoch')\n            for self.iter,self.batch in enumerate(self.dl):\n                try:\n                    self.callback('before_batch')\n                    self.one_batch()\n                    self.callback('after_batch')\n                except CancelBatchException: pass\n            self.callback('after_epoch')\n        except CancelEpochException: pass\n    \n    def fit(self, n_epochs):\n        self.n_epochs = n_epochs\n        self.epochs = range(n_epochs)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        try:\n            self.callback('before_fit')\n            for self.epoch in self.epochs:\n                self.one_epoch(True)\n                self.one_epoch(False)\n            self.callback('after_fit')\n        except CancelFitException: pass\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n\n\n\nm,nh = 28*28,50\n\n\nsource\n\n\nget_model\n\n get_model ()\n\n\n\nExported source\ndef get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\n\nmodel = get_model()\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[CompletionCB()])\nlearn.fit(1)\n\nCompleted 64 batches\n\n\n\nsource\n\n\nSingleBatchCB\n\n SingleBatchCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass SingleBatchCB(Callback):\n    order = 1\n    def after_batch(self, learn): raise CancelFitException()\n\n\n\nlearn = Learner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=[SingleBatchCB(), CompletionCB()])\nlearn.fit(1)",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#metrics",
    "href": "learner.html#metrics",
    "title": "Learner",
    "section": "Metrics",
    "text": "Metrics\n\nsource\n\nMetric\n\n Metric ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Metric:\n    def __init__(self): self.reset()\n    def reset(self): self.vals,self.ns = [],[]\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals)*ns).sum()/ns.sum()\n    def calc(self, inps, targs): return inps\n\n\n\nsource\n\n\nAccuracy\n\n Accuracy ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Accuracy(Metric):\n    def calc(self, inps, targs): return (inps==targs).float().mean()\n\n\n\nacc = Accuracy()\nacc.add(tensor([0, 1, 2, 0, 1, 2]), tensor([0, 1, 1, 2, 1, 0]))\nacc.add(tensor([1, 1, 2, 0, 1]), tensor([0, 1, 1, 2, 1]))\nacc.value\n\ntensor(0.45)\n\n\n\nloss = Metric()\nloss.add(0.6, n=32)\nloss.add(0.9, n=2)\nloss.value, round((0.6*32+0.9*2)/(32+2), 2)\n\n(tensor(0.62), 0.62)",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#some-callbacks",
    "href": "learner.html#some-callbacks",
    "title": "Learner",
    "section": "Some callbacks",
    "text": "Some callbacks\npip install torcheval\n\n\nExported source\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n\n\n\nmetric = MulticlassAccuracy()\nmetric.update(tensor([0, 2, 1, 3]), tensor([0, 1, 2, 3]))\nmetric.compute()\n\ntensor(0.50)\n\n\n\nmetric.reset()\nmetric.compute()\n\ntensor(nan)\n\n\n\nsource\n\nto_cpu\n\n to_cpu (x)\n\n\n\nExported source\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    res = x.detach().cpu()\n    return res.float() if res.dtype==torch.float16 else res\n\n\n\nsource\n\n\nMetricsCB\n\n MetricsCB (*ms, **metrics)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n\n\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device='cpu')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)\n\n\n\nmodel = get_model()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics])\nlearn.fit(5)\n\n{'accuracy': '0.602', 'loss': '1.183', 'epoch': 0, 'train': 'train'}\n{'accuracy': '0.700', 'loss': '0.847', 'epoch': 0, 'train': 'eval'}\n{'accuracy': '0.733', 'loss': '0.738', 'epoch': 1, 'train': 'train'}\n{'accuracy': '0.772', 'loss': '0.646', 'epoch': 1, 'train': 'eval'}\n{'accuracy': '0.773', 'loss': '0.631', 'epoch': 2, 'train': 'train'}\n{'accuracy': '0.786', 'loss': '0.604', 'epoch': 2, 'train': 'eval'}\n{'accuracy': '0.797', 'loss': '0.574', 'epoch': 3, 'train': 'train'}\n{'accuracy': '0.801', 'loss': '0.562', 'epoch': 3, 'train': 'eval'}\n{'accuracy': '0.809', 'loss': '0.539', 'epoch': 4, 'train': 'train'}\n{'accuracy': '0.795', 'loss': '0.558', 'epoch': 4, 'train': 'eval'}",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#flexible-learner",
    "href": "learner.html#flexible-learner",
    "title": "Learner",
    "section": "Flexible learner",
    "text": "Flexible learner\n\nsource\n\nLearner\n\n Learner (model, dls=(0,), loss_func=&lt;function mse_loss&gt;, lr=0.1,\n          cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @contextmanager\n    def cb_ctx(self, nm):\n        try:\n            self.callback(f'before_{nm}')\n            yield\n            self.callback(f'after_{nm}')\n        except globals()[f'Cancel{nm.title()}Exception']: pass\n        finally: self.callback(f'cleanup_{nm}')\n                \n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        with self.cb_ctx('epoch'):\n            for self.iter,self.batch in enumerate(self.dl):\n                with self.cb_ctx('batch'):\n                    self.predict()\n                    self.get_loss()\n                    if self.training:\n                        self.backward()\n                        self.step()\n                        self.zero_grad()\n    \n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n            with self.cb_ctx('fit'):\n                for self.epoch in self.epochs:\n                    if train: self.one_epoch(True)\n                    if valid: torch.no_grad()(self.one_epoch)(False)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n\n\n\nsource\n\n\nTrainCB\n\n TrainCB (n_inp=1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass TrainCB(Callback):\n    def __init__(self, n_inp=1): self.n_inp = n_inp\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n    def backward(self, learn): learn.loss.backward()\n    def step(self, learn): learn.opt.step()\n    def zero_grad(self, learn): learn.opt.zero_grad()\n\n\nNB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.\n\nsource\n\n\nProgressCB\n\n ProgressCB (plot=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n        self.val_losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n    \n    def after_epoch(self, learn): \n        if not learn.training:\n            if self.plot and hasattr(learn, 'metrics'): \n                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\n\nNB: Added validation loss plotting after the lesson.\n\nmodel = get_model()\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.620\n1.149\n0\ntrain\n\n\n0.704\n0.870\n0\neval\n\n\n0.741\n0.714\n1\ntrain\n\n\n0.742\n0.681\n1\neval",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#updated-versions-since-the-lesson",
    "href": "learner.html#updated-versions-since-the-lesson",
    "title": "Learner",
    "section": "Updated versions since the lesson",
    "text": "Updated versions since the lesson\nAfter the lesson we noticed that contextlib.context_manager has a surprising “feature” which doesn’t let us raise an exception before the yield. Therefore we’ve replaced the context manager with a decorator in this updated version of Learner. We have also added a few more callbacks in one_epoch().\n\nsource\n\nwith_cbs\n\n with_cbs (nm)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n\n\n\nsource\n\n\nLearner\n\n Learner (model, dls=(0,), loss_func=&lt;function mse_loss&gt;, lr=0.1,\n          cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n\n\n\nmodel = get_model()\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.617\n1.184\n0\ntrain\n\n\n0.690\n0.882\n0\neval",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#trainlearner-and-momentumlearner",
    "href": "learner.html#trainlearner-and-momentumlearner",
    "title": "Learner",
    "section": "TrainLearner and MomentumLearner",
    "text": "TrainLearner and MomentumLearner\n\nsource\n\nTrainLearner\n\n TrainLearner (model, dls=(0,), loss_func=&lt;function mse_loss&gt;, lr=0.1,\n               cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n\n\n\nsource\n\n\nMomentumLearner\n\n MomentumLearner (model, dls, loss_func, lr=None, cbs=None,\n                  opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;, mom=0.85)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n\n\n\n# NB: No TrainCB\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.682\n0.938\n0\ntrain\n\n\n0.797\n0.576\n0\neval",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#lrfindercb",
    "href": "learner.html#lrfindercb",
    "title": "Learner",
    "section": "LRFinderCB",
    "text": "LRFinderCB\n\nsource\n\nLRFinderCB\n\n LRFinderCB (lr_mult=1.3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*3: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n        \n    def plot(self):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n\n\n\nlrfind = LRFinderCB()\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs)\nlearn.fit(1)\nplt.plot(lrfind.lrs, lrfind.losses)\nplt.xscale('log')\n\n\n\n\n\n\n\n\n\nlrfind.plot()\n\n\n\n\n\n\n\n\n\n\nExported source\nfrom torch.optim.lr_scheduler import ExponentialLR\n\n\nExponentialLR\n\nsource\n\n\nLRFinderCB\n\n LRFinderCB (gamma=1.3, max_mult=3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass LRFinderCB(Callback):\n    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.sched = ExponentialLR(learn.opt, self.gamma)\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss &lt; self.min: self.min = loss\n        if math.isnan(loss) or (loss &gt; self.min*self.max_mult):\n            raise CancelFitException()\n        self.sched.step()\n\n    def cleanup_fit(self, learn):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n\n\n\ncbs = [DeviceCB()]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-5, cbs=cbs)\nlearn.fit(3, cbs=LRFinderCB())\n\n\n\n\n\n\n\n\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\n\nExported source\n@fc.patch\ndef lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))\n\n\nlr_find was added in lesson 18. It’s just a shorter way of using LRFinderCB.\n\nMomentumLearner(get_model(), dls, F.cross_entropy, cbs=cbs).lr_find()",
    "crumbs": [
      "Blog",
      "Learner"
    ]
  },
  {
    "objectID": "stable_diffusion_overview.html",
    "href": "stable_diffusion_overview.html",
    "title": "Stable Diffusion Overview",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.1.0\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Stable Diffusion Overview"
    ]
  },
  {
    "objectID": "stable_diffusion_overview.html#initial-checks",
    "href": "stable_diffusion_overview.html#initial-checks",
    "title": "Stable Diffusion Overview",
    "section": "",
    "text": "!conda list | grep \"pytorch\"\n\npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-ignite            0.4.12                   pypi_0    pypi\npytorch-lightning         2.0.7                    pypi_0    pypi\npytorch-mutex             1.0                        cuda    pytorch\ntorchaudio                2.0.2               py311_cu118    pytorch\ntorchtriton               2.0.0                     py311    pytorch\ntorchvision               0.15.2              py311_cu118    pytorch\n\n\n\n!pip list | grep \"fastai\" \n!pip list | grep \"fastbook\"\n!pip list | grep \"ipywidgets\"\n\nfastai                        2.7.12\nfastbook                      0.0.28\nipywidgets                    8.1.0\n\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Stable Diffusion Overview"
    ]
  },
  {
    "objectID": "stable_diffusion_overview.html#unet",
    "href": "stable_diffusion_overview.html#unet",
    "title": "Stable Diffusion Overview",
    "section": "“UNET”",
    "text": "“UNET”\n\ninput - somewhat noisy image\noutput - the noise\n\nimage = image + noise\nModel trained to calculate noise in a image",
    "crumbs": [
      "Blog",
      "Stable Diffusion Overview"
    ]
  },
  {
    "objectID": "stable_diffusion_overview.html#autoencoder-vae",
    "href": "stable_diffusion_overview.html#autoencoder-vae",
    "title": "Stable Diffusion Overview",
    "section": "Autoencoder = “VAE”",
    "text": "Autoencoder = “VAE”\n\noutput = input\n\nModel trained to compress and decompress images",
    "crumbs": [
      "Blog",
      "Stable Diffusion Overview"
    ]
  },
  {
    "objectID": "stable_diffusion_overview.html#latents",
    "href": "stable_diffusion_overview.html#latents",
    "title": "Stable Diffusion Overview",
    "section": "Latents",
    "text": "Latents\nlatents = Autoencoders middle output (compressed version of the image)",
    "crumbs": [
      "Blog",
      "Stable Diffusion Overview"
    ]
  },
  {
    "objectID": "stable_diffusion_overview.html#clip",
    "href": "stable_diffusion_overview.html#clip",
    "title": "Stable Diffusion Overview",
    "section": "CLIP",
    "text": "CLIP\nCL - Contrast loss\nmodel trained to create image latents from text input",
    "crumbs": [
      "Blog",
      "Stable Diffusion Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fast AI Course",
    "section": "",
    "text": "pip install fastAIcourse\npip install -e '.[dev]'",
    "crumbs": [
      "Blog",
      "[Fast AI Course](https://bthek1.github.io/fastAIcourse/)"
    ]
  },
  {
    "objectID": "index.html#after-changing-dependencies",
    "href": "index.html#after-changing-dependencies",
    "title": "Fast AI Course",
    "section": "",
    "text": "pip install fastAIcourse\npip install -e '.[dev]'",
    "crumbs": [
      "Blog",
      "[Fast AI Course](https://bthek1.github.io/fastAIcourse/)"
    ]
  },
  {
    "objectID": "index.html#ai-types",
    "href": "index.html#ai-types",
    "title": "Fast AI Course",
    "section": "AI Types",
    "text": "AI Types\n\nMulti layer preception (MLP)\nConvolution Neural Network (CNN) ex. ResNET, VGG\nNeural Cellular Automata (NCA)",
    "crumbs": [
      "Blog",
      "[Fast AI Course](https://bthek1.github.io/fastAIcourse/)"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Fast AI Course",
    "section": "Datasets",
    "text": "Datasets\n\ntitanic\npaddy disease\nMNIST (number images 28 * 28)\nFashion MNIST (clothing images 28 * 28)\nCIFAR - 10 (image classification dataset)",
    "crumbs": [
      "Blog",
      "[Fast AI Course](https://bthek1.github.io/fastAIcourse/)"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Fast AI Course",
    "section": "Structure",
    "text": "Structure\n\nData process\n\nNormalise\nRemove NAN\n\nCreate Model\n\ntry decision tree - random forest\n\nRun model",
    "crumbs": [
      "Blog",
      "[Fast AI Course](https://bthek1.github.io/fastAIcourse/)"
    ]
  },
  {
    "objectID": "index.html#model-structure",
    "href": "index.html#model-structure",
    "title": "Fast AI Course",
    "section": "Model structure",
    "text": "Model structure\n\ntrain/test split\n\nasd\n\ninitalise random weights\ncalculate loss\nGradient descent - recalculate weights\nRepeat steps 2 - 4\n\n\nimport torch\nimport math\nimport matplotlib.pyplot as plt\n\ndtype = torch.float\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.set_default_device(\"cuda:0\")\ndevice\n\n'cuda'\n\n\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\ny = torch.sin(x)\nplt.plot\nplt.plot(x.detach().cpu(),y.detach().cpu())\n\n\n\n\n\n\n\n\n\n# Create random Tensors for weights. For a third order polynomial, we need\n# 4 weights: y = a + b x + c x^2 + d x^3\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\nfrom torch import tensor\n\na = tensor([1], dtype=dtype, requires_grad=True)\nb = tensor([0], dtype=dtype, requires_grad=True)\nc = tensor([0], dtype=dtype, requires_grad=True)\nd = tensor([0], dtype=dtype, requires_grad=True)\n\na,b,c,d\n\n(tensor([1.], device='cuda:0', requires_grad=True),\n tensor([0.], device='cuda:0', requires_grad=True),\n tensor([0.], device='cuda:0', requires_grad=True),\n tensor([0.], device='cuda:0', requires_grad=True))\n\n\n\ntorch.abs(loss-a)\n\ntensor([8.3212], device='cuda:0', grad_fn=&lt;AbsBackward0&gt;)\n\n\n\nlearning_rate = 1e-6\n\ny_pred = a + b * x + c * x ** 2 + d * x ** 3\nplt.plot(x.detach().cpu(),y_pred.detach().cpu())\n\nprevious_loss = tensor([0], dtype=dtype, requires_grad=True)\n\nfor t in range(2000):\n    # Forward pass: compute predicted y using operations on Tensors.\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss using operations on Tensors.\n    # Now loss is a Tensor of shape (1,)\n    # loss.item() gets the scalar value held in the loss.\n    loss = (y_pred - y).pow(2).sum()\n    if torch.abs(previous_loss - loss) &gt; 50:\n        previous_loss = loss\n        print(t, loss.item(), previous_loss.item())\n        plt.plot(x.detach().cpu(),y_pred.detach().cpu())\n        \n\n    # Use autograd to compute the backward pass. This call will compute the\n    # gradient of loss with respect to all Tensors with requires_grad=True.\n    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n    # the gradient of the loss with respect to a, b, c, d respectively.\n    loss.backward()\n\n    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n    # because weights have requires_grad=True, but we don't need to track this\n    # in autograd.\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\nplt.plot(x.detach().cpu(),y.detach().cpu())\n\n0 2999.5 2999.5\n1 2633.0322265625 2633.0322265625\n2 2440.7451171875 2440.7451171875\n3 2299.944091796875 2299.944091796875\n4 2184.31591796875 2184.31591796875\n5 2086.458740234375 2086.458740234375\n6 2002.959228515625 2002.959228515625\n7 1931.475830078125 1931.475830078125\n8 1870.134033203125 1870.134033203125\n9 1817.367919921875 1817.367919921875\n11 1732.486328125 1732.486328125\n13 1668.533203125 1668.533203125\n16 1599.33935546875 1599.33935546875\n20 1537.807861328125 1537.807861328125\n25 1486.8544921875 1486.8544921875\n32 1436.569580078125 1436.569580078125\n41 1384.7811279296875 1384.7811279296875\n51 1332.85009765625 1332.85009765625\n62 1278.809814453125 1278.809814453125\n73 1227.13720703125 1227.13720703125\n85 1173.205322265625 1173.205322265625\n97 1121.6829833984375 1121.6829833984375\n110 1068.4569091796875 1068.4569091796875\n123 1017.7958984375 1017.7958984375\n137 965.9642333984375 965.9642333984375\n152 913.40380859375 913.40380859375\n168 860.5406494140625 860.5406494140625\n185 807.7796020507812 807.7796020507812\n203 755.4998779296875 755.4998779296875\n222 704.0501098632812 704.0501098632812\n242 653.7462768554688 653.7462768554688\n264 602.63671875 602.63671875\n288 551.522705078125 551.522705078125\n314 501.137939453125 501.137939453125\n343 450.4776306152344 450.4776306152344\n376 399.1850280761719 399.1850280761719\n413 348.7738037109375 348.7738037109375\n456 298.3631896972656 298.3631896972656\n507 248.23623657226562 248.23623657226562\n570 198.20193481445312 198.20193481445312\n653 147.97088623046875 147.97088623046875\n774 97.72457885742188 97.72457885742188\n999 47.624717712402344 47.624717712402344\nResult: y = 0.03058280050754547 + 0.8431105613708496 x + -0.005276043433696032 x^2 + -0.09139160066843033 x^3",
    "crumbs": [
      "Blog",
      "[Fast AI Course](https://bthek1.github.io/fastAIcourse/)"
    ]
  },
  {
    "objectID": "cosine.html",
    "href": "cosine.html",
    "title": "cosine schedule",
    "section": "",
    "text": "import torch\nif torch.cuda.is_available():\n    device = torch.cuda.current_device()\n    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(device).total_memory / (1024**3):.2f} GB\")\n    print(f\"GPU Memory Free: {torch.cuda.get_device_properties(device).total_memory / (1024**3):.2f} GB\")\n    print(f\"Compute Capability: {torch.cuda.get_device_capability(device)}\")\nelse:\n    print(\"No GPU available. Using CPU.\")\n\nGPU Name: NVIDIA GeForce RTX 2060\nGPU Memory Total: 6.00 GB\nGPU Memory Free: 6.00 GB\nCompute Capability: (7, 5)\nimport os\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastprogress import progress_bar\nfrom diffusers import UNet2DModel, DDIMPipeline, DDPMPipeline, DDIMScheduler, DDPMScheduler\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\nxl,yl = 'image','label'\nname = \"fashion_mnist\"\nbs = 256\ndsd = load_dataset(name)\ndef abar(t): return (t*math.pi/2).cos()**2\ndef inv_abar(x): return x.sqrt().acos()*2/math.pi\ndef noisify(x0):\n    device = x0.device\n    n = len(x0)\n    t = torch.rand(n,).to(x0).clamp(0,0.999)\n    ε = torch.randn(x0.shape, device=device)\n    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*ε\n    return (xt, t.to(device)), ε\ndef collate_ddpm(b): return noisify(default_collate(b)[xl])\ndef dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n\ndl = dls.train\n(xt,t),eps = b = next(iter(dl))\nshow_images(xt[:25], imsize=1.5, titles=fc.map_ex(t[:25], '{:.02f}'))\nclass UNet(UNet2DModel):\n    def forward(self, x): return super().forward(*x).sample\ndef init_ddpm(model):\n    for o in model.down_blocks:\n        for p in o.resnets:\n            p.conv2.weight.data.zero_()\n            for p in fc.L(o.downsamplers): init.orthogonal_(p.conv.weight)\n\n    for o in model.up_blocks:\n        for p in o.resnets: p.conv2.weight.data.zero_()\n\n    model.conv_out.weight.data.zero_()\nlr = 4e-3\nepochs = 3\nopt_func = partial(optim.Adam, eps=1e-5)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\ncbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nmodel = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\ninit_ddpm(model)\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.185\n0\ntrain\n\n\n0.050\n0\neval\n\n\n0.045\n1\ntrain\n\n\n0.042\n1\neval\n\n\n0.039\n2\ntrain\n\n\n0.039\n2\neval\ntorch.save(learn.model, 'models/fashion_cos2.pkl')\nmodel = learn.model = torch.load('models/fashion_cos2.pkl').cuda()\ndef denoise(x_t, noise, t):\n    device = x_t.device\n    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n    return ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt()).clamp(-1,1)\nwith torch.no_grad(): noise=learn.model((xt.cuda(),t.cuda()))\nshow_images(xt[:25], imsize=1.5, titles=fc.map_ex(t[:25], '{:.02f}'))\nshow_images(denoise(xt.cuda(),noise,t.cuda())[:25].clamp(-1,1), imsize=1.5, titles=fc.map_ex(t[:25], '{:.02f}'))",
    "crumbs": [
      "Blog",
      "cosine schedule"
    ]
  },
  {
    "objectID": "cosine.html#sampling",
    "href": "cosine.html#sampling",
    "title": "cosine schedule",
    "section": "Sampling",
    "text": "Sampling\n\nfrom fastAIcourse.fid import ImageEval\n\n\ncmodel = torch.load('models/data_aug2.pkl')\ndel(cmodel[8])\ndel(cmodel[7])\n\n@inplace\ndef transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n\nbs = 2048\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n\ndt = dls.train\nxb,yb = next(iter(dt))\n\nie = ImageEval(cmodel, dls, cbs=[DeviceCB()])\n\n\nsz = (2048,1,32,32)\n\n\nsz = (256,1,32,32)\n\n\ndef ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig):\n    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n    x_0_hat = ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt()).clamp(-1.5,1.5)\n    if bbar_t1&lt;=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n    x_t += sig * torch.randn(x_t.shape).to(x_t)\n    return x_0_hat,x_t\n\n\n@torch.no_grad()\ndef sample(f, model, sz, steps, eta=1.):\n    ts = torch.linspace(1-1/steps,0,steps)\n    x_t = torch.randn(sz).to(model.device)\n    preds = []\n    for i,t in enumerate(progress_bar(ts)):\n        abar_t = abar(t)\n        noise = model((x_t, t))\n        abar_t1 = abar(t-1/steps) if t&gt;=1/steps else torch.tensor(1)\n#         print(abar_t,abar_t1,x_t.min(),x_t.max())\n        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100))\n        preds.append(x_0_hat.float().cpu())\n    return preds\n\n\n# set_seed(42)\npreds = sample(ddim_step, model, sz, steps=100, eta=1.)\ns = (preds[-1]*2)\ns.min(),s.max(),s.shape\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:49&lt;00:00]\n    \n    \n\n\n(tensor(-1.3284), tensor(1.5739), torch.Size([256, 1, 32, 32]))\n\n\n\nshow_images(s[:25], imsize=1.5)\n\n\n\n\n\n\n\n\n\nie.fid(s),ie.kid(s),s.shape\n\n(44.0902099609375, 0.06875649839639664, torch.Size([256, 1, 32, 32]))\n\n\n\npreds = sample(ddim_step, model, sz, steps=100, eta=1.)\nie.fid(preds[-1]*2)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:49&lt;00:00]\n    \n    \n\n\n43.769775390625\n\n\n\npreds = sample(ddim_step, model, sz, steps=50, eta=1.)\nie.fid(preds[-1]*2)\n\n\n\n\n\n\n    \n      \n      100.00% [50/50 00:24&lt;00:00]\n    \n    \n\n\n45.24609375",
    "crumbs": [
      "Blog",
      "cosine schedule"
    ]
  },
  {
    "objectID": "ddpm.html",
    "href": "ddpm.html",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "",
    "text": "Now that we written our own barebones training library, let’s make some progress towards exploring diffusion model and building Stable Diffusion from scratch.\nWe’ll start with building and training the model described in the seminal 2020 paper Denoising Diffusion Probabilistic Models (DDPM). For more context, while diffusion models were technically invented back in 2015, diffusion models flew under the radar until this 2020 paper since they were complicated and difficult to train. The 2020 paper introducing DDPMs made some crucial assumptions that significantly simplify the model training and generation processes, as we will see here. Later versions of diffusion models all build upon the same framework introduced in this paper.\nLet’s get started and train our own DDPM!",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm.html#imports",
    "href": "ddpm.html#imports",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Imports",
    "text": "Imports\nWe’ll start with some imports.\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm.html#load-the-dataset",
    "href": "ddpm.html#load-the-dataset",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Load the dataset",
    "text": "Load the dataset\nWe will load the dataset from HuggingFace Hub:\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\nTo make life simpler (mostly with the model architecture), we’ll resize the 28x28 images to 32x32:\n\n\ninplace.._f\n\n inplace.&lt;locals&gt;._f (b)\n\n\n\nExported source\n@inplace\ndef transformi(b): b[x] = [TF.resize(TF.to_tensor(o), (32,32), antialias=True) for o in b[x]]\n\n\nLet’s set our batch size and create our DataLoaders with this batch size. we can confirm the shapes are correct. Note that while we do get the labels for the dataset, we actuallydon’t care about that for our task of unconditional image generation.\n\nset_seed(42)\nbs = 128\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([128, 1, 32, 32]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm.html#create-model",
    "href": "ddpm.html#create-model",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Create model",
    "text": "Create model\nWe will create a U-net. A U-net looks something like this:\n\nThe DDPM U-net is a modification of this with some modern tricks like using attention.\nWe will cover how U-nets are created and how modules like attention work in future lessons. For now, we’ll import the U-net from the diffusers library:\n\nfrom diffusers import UNet2DModel\n\n\nmodel = UNet2DModel(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128))",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm.html#training---easy-with-a-callback",
    "href": "ddpm.html#training---easy-with-a-callback",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Training - easy with a callback!",
    "text": "Training - easy with a callback!\nDDPM is trained quite simply in a few steps: 1. randomly select some timesteps in an iterative noising process. 2. Add noise corresponding to this timestep to the original image. For increasing timesteps, the variance of the noise increases. 3. Pass in this noisy image and the timestep to our model 4. Model is trained with an MSE loss between the model output and the amount of noise added to the image\nWe will implement this in a callback. The callback will randomly select the timestep and create the noisy image before setting up our input and ground truth tensors for the model forward pass and loss calculation.\nAfter training, we need to sample from this model. This is an iterative denoising process starting from pure noise. We simply keep removing noise predicted by the neural network, but we do it with an expected noise schedule that is reverse of what we saw during training. This is also done in our callback.\n\nsource\n\nDDPMCB\n\n DDPMCB (n_steps, beta_min, beta_max)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass DDPMCB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, n_steps, beta_min, beta_max):\n        super().__init__()\n        self.n_steps,self.βmin,self.βmax = n_steps,beta_min,beta_max\n        # variance schedule, linearly increased with timestep\n        self.β = torch.linspace(self.βmin, self.βmax, self.n_steps)\n        self.α = 1. - self.β \n        self.ᾱ = torch.cumprod(self.α, dim=0)\n        self.σ = self.β.sqrt()\n\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[0]).sample\n    \n    def before_batch(self, learn):\n        device = learn.batch[0].device\n        ε = torch.randn(learn.batch[0].shape, device=device)  # noise, x_T\n        x0 = learn.batch[0] # original images, x_0\n        self.ᾱ = self.ᾱ.to(device)\n        n = x0.shape[0]\n        # select random timesteps\n        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)\n        ᾱ_t = self.ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n        xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε #noisify the image\n        # input to our model is noisy image and timestep, ground truth is the noise \n        learn.batch = ((xt, t), ε)\n    \n    @torch.no_grad()\n    def sample(self, model, sz):\n        device = next(model.parameters()).device\n        x_t = torch.randn(sz, device=device)\n        preds = []\n        for t in reversed(range(self.n_steps)):\n            t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n            z = (torch.randn(x_t.shape) if t &gt; 0 else torch.zeros(x_t.shape)).to(device)\n            ᾱ_t1 = self.ᾱ[t-1]  if t &gt; 0 else torch.tensor(1)\n            b̄_t = 1 - self.ᾱ[t]\n            b̄_t1 = 1 - ᾱ_t1\n            noise_pred = learn.model(x_t, t_batch).sample\n            x_0_hat = ((x_t - b̄_t.sqrt() * noise_pred)/self.ᾱ[t].sqrt()).clamp(-1,1)\n            x0_coeff = ᾱ_t1.sqrt()*(1-self.α[t])/b̄_t\n            xt_coeff = self.α[t].sqrt()*b̄_t1/b̄_t\n            x_t = x_0_hat*x0_coeff + x_t*xt_coeff + self.σ[t]*z\n            preds.append(x_t.cpu())\n        return preds\n\n\nOkay now we’re ready to train a model!\nLet’s create our Learner. We’ll add our callbacks and train with MSE loss.\nWe specify the number of timesteps and the minimum and maximum variance for the DDPM model.\n\nlr = 4e-3\nepochs = 3\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\ncbs = [ddpm_cb, DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=optim.Adam)\n\nNow let’s run the fit function:\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.053\n0\ntrain\n\n\n0.023\n0\neval\n\n\n0.020\n1\ntrain\n\n\n0.018\n1\neval\n\n\n0.017\n2\ntrain\n\n\n0.016\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nmdl_path = Path('models')\nmdl_path.mkdir(exist_ok=True)\n\n\ntorch.save(learn.model, mdl_path/'fashion_ddpm.pkl')\n\n\nlearn.model = torch.load(mdl_path/'fashion_ddpm.pkl')",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "ddpm.html#inference",
    "href": "ddpm.html#inference",
    "title": "Denoising Diffusion Probabilistic Models with miniai",
    "section": "Inference",
    "text": "Inference\nNow that we’ve trained our model, let’s generate some images with our model:\n\nset_seed(42)\nsamples = ddpm_cb.sample(learn.model, (16, 1, 32, 32))\nlen(samples)\n\n1000\n\n\n\nshow_images(-samples[-1], figsize=(5,5))\n\n\n\n\n\n\n\n\nLet’s visualize the sampling process:\n\n[999]*10\n\n[999, 999, 999, 999, 999, 999, 999, 999, 999, 999]\n\n\n\nimport matplotlib.animation as animation\nfrom IPython.display import display, HTML\n\nfig,ax = plt.subplots(figsize=(3,3))\ndef _show_i(i): return show_image(-samples[i][9], ax=ax, animated=True).get_images()\nr = L.range(700,900, 4)+L.range(900,1000,1)+[999]*10\nims = r.map(_show_i)\n\nanimate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=3000)\n\nHTML(animate.to_jshtml())\n\nUsing matplotlib backend: &lt;object object&gt;\n\n\nQStandardPaths: wrong permissions on runtime directory /run/user/1000/, 0755 instead of 0700\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNote that I only take the steps between 800 and 1000 since most of the previous steps are actually quite noisy. This is a limitation of the noise schedule used for small images, and papers like Improved DDPM suggest other noise schedules for this purpose! (Some potential homework: try out the noise schedule from Improved DDPM and see if it helps.)",
    "crumbs": [
      "Blog",
      "Denoising Diffusion Probabilistic Models with miniai"
    ]
  },
  {
    "objectID": "imgnet_tiny-widish.html",
    "href": "imgnet_tiny-widish.html",
    "title": "Tiny Imagenet",
    "section": "",
    "text": "import os\n# os.environ['CUDA_VISIBLE_DEVICES']='2'\n\n\nimport shutil,timm,os,torch,random,datasets,math,warnings\nimport fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport k_diffusion as K, torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\nfrom glob import glob\n\nfrom fastAIcourse.datasets import *\nfrom fastAIcourse.conv import *\nfrom fastAIcourse.learner import *\nfrom fastAIcourse.activations import *\nfrom fastAIcourse.init import *\nfrom fastAIcourse.sgd import *\nfrom fastAIcourse.resnet import *\nfrom fastAIcourse.augment import *\nfrom fastAIcourse.accel import *\nfrom fastAIcourse.training import *\n\n\nfrom fastprogress import progress_bar\n\n\ntorch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'tiny-imagenet-200'\n\nurl = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\nif not path.exists():\n    path_zip = fc.urlsave(url, path_data)\n    shutil.unpack_archive('data/tiny-imagenet-200.zip', 'data')\n\nbs = 512\n\nclass TinyDS:\n    def __init__(self, path):\n        self.path = Path(path)\n        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return self.files[i],Path(self.files[i]).parent.parent.name\n\ntds = TinyDS(path/'train')\n\n\npath_anno = path/'val'/'val_annotations.txt'\nanno = dict(o.split('\\t')[:2] for o in path_anno.read_text().splitlines())\n\n\nclass TinyValDS(TinyDS):\n    def __getitem__(self, i): return self.files[i],anno[os.path.basename(self.files[i])]\n\n\nvds = TinyValDS(path/'val')\n\n\nclass TfmDS:\n    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n    def __len__(self): return len(self.ds)\n    def __getitem__(self, i):\n        x,y = self.ds[i]\n        return self.tfmx(x),self.tfmy(y)\n\n\nid2str = (path/'wnids.txt').read_text().splitlines()\nstr2id = {v:k for k,v in enumerate(id2str)}\n\n\nxmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))\n\n\ndef tfmx(x):\n    img = read_image(x, mode=ImageReadMode.RGB)/255\n    return (img-xmean[:,None,None])/xstd[:,None,None]\n\ndef tfmy(y): return tensor(str2id[y])\n\ntfm_tds = TfmDS(tds, tfmx, tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\ndef denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clip(0,1)\n\nall_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\nsynsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\ndef tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n\ntfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     RandErase())\naugcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\n\nnfs = (32,64,128,256,512,1024)\n\ndef get_dropmodel(act=act_gr, nfs=nfs, norm=nn.BatchNorm2d, drop=0.1):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n#     layers += [ResBlock(nfs[0], nfs[0], ks=3, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\ndef res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n    return nn.Sequential(*[\n        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n        for i in range(n_bk)])\n\nnbks = (3,2,2,1,1)\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nopt_func = partial(optim.AdamW, eps=1e-5)\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n\nepochs = 25\nlr = 3e-2\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), augcb]\nlearn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\naug_tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n                     T.RandomHorizontalFlip(),\n                     T.TrivialAugmentWide())\n\nnorm_tfm = T.Normalize(xmean, xstd)\nerase_tfm = RandErase()\n\nfrom PIL import Image\n\ndef tfmx(x, aug=False):\n    x = Image.open(x).convert('RGB')\n    if aug: x = aug_tfms(x)\n    x = TF.to_tensor(x)\n    x = norm_tfm(x)\n    if aug: x = erase_tfm(x[None])[0]\n    return x\n\ntfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\ntfm_vds = TfmDS(vds, tfmx, tfmy)\n\ndls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))\n\n\ndef conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True):\n    layers = []\n    if norm: layers.append(norm(ni))\n    if act : layers.append(act())\n    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n    return nn.Sequential(*layers)\n\ndef _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n    return nn.Sequential(conv(ni, nf, stride=1     , act=act, norm=norm, ks=ks),\n                         conv(nf, nf, stride=stride, act=act, norm=norm, ks=ks))\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n        super().__init__()\n        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n\n\ndef get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n               for i in range(len(nfs)-1)]\n    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nepochs = 50\nlr = 0.1\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\nmodel = get_dropmodel(nbks=(1,2,4,2,2), nfs=(32, 64, 128, 512, 768, 1024), drop=0.1)\nlearn = Learner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.022\n5.068\n0\ntrain\n\n\n0.037\n4.833\n0\neval\n\n\n0.046\n4.766\n1\ntrain\n\n\n0.065\n4.545\n1\neval\n\n\n0.072\n4.501\n2\ntrain\n\n\n0.078\n4.342\n2\neval\n\n\n0.099\n4.268\n3\ntrain\n\n\n0.135\n3.958\n3\neval\n\n\n0.137\n4.010\n4\ntrain\n\n\n0.134\n4.026\n4\neval\n\n\n0.166\n3.801\n5\ntrain\n\n\n0.162\n3.899\n5\neval\n\n\n0.195\n3.635\n6\ntrain\n\n\n0.212\n3.536\n6\neval\n\n\n0.214\n3.503\n7\ntrain\n\n\n0.242\n3.391\n7\neval\n\n\n0.237\n3.382\n8\ntrain\n\n\n0.260\n3.325\n8\neval\n\n\n0.252\n3.293\n9\ntrain\n\n\n0.300\n3.074\n9\neval\n\n\n0.269\n3.202\n10\ntrain\n\n\n0.287\n3.198\n10\neval\n\n\n0.286\n3.118\n11\ntrain\n\n\n0.295\n3.080\n11\neval\n\n\n0.296\n3.055\n12\ntrain\n\n\n0.307\n3.070\n12\neval\n\n\n0.309\n2.984\n13\ntrain\n\n\n0.323\n3.021\n13\neval\n\n\n0.319\n2.931\n14\ntrain\n\n\n0.334\n2.866\n14\neval\n\n\n0.333\n2.868\n15\ntrain\n\n\n0.312\n2.970\n15\neval\n\n\n0.343\n2.813\n16\ntrain\n\n\n0.283\n3.314\n16\neval\n\n\n0.353\n2.762\n17\ntrain\n\n\n0.368\n2.690\n17\neval\n\n\n0.362\n2.713\n18\ntrain\n\n\n0.329\n2.986\n18\neval\n\n\n0.368\n2.680\n19\ntrain\n\n\n0.374\n2.743\n19\neval\n\n\n0.377\n2.635\n20\ntrain\n\n\n0.372\n2.705\n20\neval\n\n\n0.386\n2.587\n21\ntrain\n\n\n0.379\n2.755\n21\neval\n\n\n0.394\n2.551\n22\ntrain\n\n\n0.378\n2.689\n22\neval\n\n\n0.402\n2.505\n23\ntrain\n\n\n0.396\n2.563\n23\neval\n\n\n0.411\n2.469\n24\ntrain\n\n\n0.429\n2.437\n24\neval\n\n\n0.420\n2.416\n25\ntrain\n\n\n0.423\n2.477\n25\neval\n\n\n0.431\n2.366\n26\ntrain\n\n\n0.406\n2.596\n26\neval\n\n\n0.439\n2.328\n27\ntrain\n\n\n0.403\n2.525\n27\neval\n\n\n0.449\n2.273\n28\ntrain\n\n\n0.424\n2.490\n28\neval\n\n\n0.462\n2.215\n29\ntrain\n\n\n0.477\n2.181\n29\neval\n\n\n0.471\n2.172\n30\ntrain\n\n\n0.474\n2.224\n30\neval\n\n\n0.486\n2.103\n31\ntrain\n\n\n0.518\n2.009\n31\neval\n\n\n0.502\n2.027\n32\ntrain\n\n\n0.495\n2.119\n32\neval\n\n\n0.513\n1.969\n33\ntrain\n\n\n0.478\n2.217\n33\neval\n\n\n0.529\n1.890\n34\ntrain\n\n\n0.516\n2.058\n34\neval\n\n\n0.544\n1.827\n35\ntrain\n\n\n0.532\n1.925\n35\neval\n\n\n0.565\n1.731\n36\ntrain\n\n\n0.557\n1.866\n36\neval\n\n\n0.580\n1.662\n37\ntrain\n\n\n0.557\n1.877\n37\neval\n\n\n0.603\n1.565\n38\ntrain\n\n\n0.585\n1.726\n38\neval\n\n\n0.623\n1.471\n39\ntrain\n\n\n0.590\n1.725\n39\neval\n\n\n0.646\n1.369\n40\ntrain\n\n\n0.602\n1.683\n40\neval\n\n\n0.671\n1.263\n41\ntrain\n\n\n0.607\n1.690\n41\neval\n\n\n0.696\n1.169\n42\ntrain\n\n\n0.616\n1.649\n42\neval\n\n\n0.720\n1.069\n43\ntrain\n\n\n0.629\n1.608\n43\neval\n\n\n0.742\n0.983\n44\ntrain\n\n\n0.634\n1.594\n44\neval\n\n\n0.761\n0.912\n45\ntrain\n\n\n0.639\n1.579\n45\neval\n\n\n0.779\n0.847\n46\ntrain\n\n\n0.642\n1.567\n46\neval\n\n\n0.791\n0.801\n47\ntrain\n\n\n0.645\n1.558\n47\neval\n\n\n0.797\n0.774\n48\ntrain\n\n\n0.647\n1.553\n48\neval\n\n\n0.802\n0.766\n49\ntrain\n\n\n0.644\n1.556\n49\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(learn.model, 'models/inettiny-widish-50')\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Tiny Imagenet"
    ]
  }
]